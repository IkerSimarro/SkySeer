{"file_contents":{"db_models.py":{"content":"import os\nfrom datetime import datetime\nfrom sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Boolean, ForeignKey, Text\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker, relationship\nfrom sqlalchemy.pool import NullPool\n\nBase = declarative_base()\n\nclass AnalysisSession(Base):\n    \"\"\"Represents a video analysis session\"\"\"\n    __tablename__ = 'analysis_sessions'\n    \n    id = Column(Integer, primary_key=True)\n    video_filename = Column(String(500), nullable=False)\n    video_duration = Column(Float)\n    video_fps = Column(Float)\n    video_resolution = Column(String(50))\n    processing_date = Column(DateTime, default=datetime.utcnow)\n    total_detections = Column(Integer, default=0)\n    satellites_count = Column(Integer, default=0)\n    meteors_count = Column(Integer, default=0)\n    planes_count = Column(Integer, default=0)\n    anomalies_count = Column(Integer, default=0)\n    junk_count = Column(Integer, default=0)\n    \n    # Relationships\n    clips = relationship(\"DetectionClip\", back_populates=\"session\", cascade=\"all, delete-orphan\")\n    \n    def __repr__(self):\n        return f\"<AnalysisSession(id={self.id}, video={self.video_filename}, date={self.processing_date})>\"\n\n\nclass DetectionClip(Base):\n    \"\"\"Represents a detected motion clip with ML classification\"\"\"\n    __tablename__ = 'detection_clips'\n    \n    id = Column(Integer, primary_key=True)\n    session_id = Column(Integer, ForeignKey('analysis_sessions.id'), nullable=False)\n    clip_id = Column(Integer, nullable=False)\n    clip_filename = Column(String(500))\n    \n    # Classification results\n    classification = Column(String(50), nullable=False)\n    confidence = Column(Float)\n    anomaly_score = Column(Float)\n    is_anomaly = Column(Boolean, default=False)\n    \n    # Movement features\n    duration = Column(Float)\n    avg_speed = Column(Float)\n    max_speed = Column(Float)\n    speed_consistency = Column(Float)\n    linearity = Column(Float)\n    direction_changes = Column(Integer)\n    \n    # Object characteristics\n    avg_area = Column(Float)\n    max_area = Column(Float)\n    size_consistency = Column(Float)\n    avg_aspect_ratio = Column(Float)\n    \n    # Detection metadata\n    detection_count = Column(Integer)\n    first_frame = Column(Integer)\n    last_frame = Column(Integer)\n    \n    # Timestamps\n    detected_at = Column(DateTime, default=datetime.utcnow)\n    \n    # Relationships\n    session = relationship(\"AnalysisSession\", back_populates=\"clips\")\n    detections = relationship(\"ObjectDetection\", back_populates=\"clip\", cascade=\"all, delete-orphan\")\n    \n    def __repr__(self):\n        return f\"<DetectionClip(id={self.id}, classification={self.classification}, confidence={self.confidence})>\"\n\n\nclass ObjectDetection(Base):\n    \"\"\"Represents individual object detections within a clip\"\"\"\n    __tablename__ = 'object_detections'\n    \n    id = Column(Integer, primary_key=True)\n    clip_id = Column(Integer, ForeignKey('detection_clips.id'), nullable=False)\n    \n    frame_number = Column(Integer, nullable=False)\n    centroid_x = Column(Float)\n    centroid_y = Column(Float)\n    bbox_x = Column(Integer)\n    bbox_y = Column(Integer)\n    bbox_width = Column(Integer)\n    bbox_height = Column(Integer)\n    area = Column(Float)\n    aspect_ratio = Column(Float)\n    \n    # Relationships\n    clip = relationship(\"DetectionClip\", back_populates=\"detections\")\n    \n    def __repr__(self):\n        return f\"<ObjectDetection(id={self.id}, frame={self.frame_number}, pos=({self.centroid_x}, {self.centroid_y}))>\"\n\n\n# Database setup functions\ndef get_database_url():\n    \"\"\"Get database URL from environment variables\"\"\"\n    return os.environ.get('DATABASE_URL')\n\n\ndef init_database():\n    \"\"\"Initialize database connection and create tables\"\"\"\n    database_url = get_database_url()\n    if not database_url:\n        raise ValueError(\"DATABASE_URL environment variable not set\")\n    \n    engine = create_engine(database_url, poolclass=NullPool)\n    Base.metadata.create_all(engine)\n    Session = sessionmaker(bind=engine)\n    return Session(), engine\n\n\ndef get_session():\n    \"\"\"Get a database session\"\"\"\n    database_url = get_database_url()\n    if not database_url:\n        raise ValueError(\"DATABASE_URL environment variable not set\")\n    \n    engine = create_engine(database_url, poolclass=NullPool)\n    Session = sessionmaker(bind=engine)\n    return Session()\n","size_bytes":4424},"app.py":{"content":"import streamlit as st\nimport os\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom io import BytesIO\nimport zipfile\nimport shutil\nfrom datetime import datetime\nimport base64\n\n# Import custom modules\nfrom video_processor import VideoProcessor\nfrom feature_extractor import FeatureExtractor\nfrom ml_classifier import MLClassifier\nfrom utils import create_download_zip, format_duration, get_video_info, recommend_settings\nfrom db_service import DatabaseService\nfrom trajectory_visualizer import TrajectoryVisualizer\nfrom trajectory_analysis import (\n    analyze_all_trajectories, \n    create_trajectory_comparison_plot,\n    create_trajectory_error_plot,\n    get_trajectory_summary_stats\n)\nfrom reportlab.lib.pagesizes import letter\nfrom reportlab.lib import colors\nfrom reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, PageBreak\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.lib.units import inch\n\n# Configure page\nst.set_page_config(\n    page_title=\"SkySeer AI\",\n    page_icon=\"üåå\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\"\n)\n\n\n# Initialize session state\nif 'processing_complete' not in st.session_state:\n    st.session_state.processing_complete = False\nif 'results_data' not in st.session_state:\n    st.session_state.results_data = None\nif 'processed_clips' not in st.session_state:\n    st.session_state.processed_clips = []\nif 'db_service' not in st.session_state:\n    st.session_state.db_service = DatabaseService()\nif 'metadata' not in st.session_state:\n    st.session_state.metadata = []\nif 'video_info' not in st.session_state:\n    st.session_state.video_info = {}\nif 'recommendations' not in st.session_state:\n    st.session_state.recommendations = None\nif 'uploaded_video_path' not in st.session_state:\n    st.session_state.uploaded_video_path = None\n\ndef main():\n    st.title(\"üåå SkySeer AI\")\n    st.markdown(\"**Advanced Sky Object Detection & Classification System**\")\n    \n    # How It Works section\n    st.success(\"\"\"\n    **üîç How Does It Work?**\n    \n    SkySeer uses a three-step process to find satellites and meteors in your videos:\n    \n    1. **Motion Detection** - Computer vision scans your video frame-by-frame to spot \n       objects moving across the sky, filtering out background stars and noise.\n    \n    2. **Feature Extraction** - Each detected object is analyzed to measure its speed, \n       trajectory, brightness, and consistency, creating a unique \"flight signature.\"\n    \n    3. **Smart Classification** - Machine learning algorithms categorize objects as \n       satellites, meteors, or noise based on their flight patterns.\n    \n    The system is designed to only catch **very obvious movement**, minimizing false \n    positives and giving you clean, reliable results!\n    \"\"\")\n    \n    st.markdown(\"---\")\n    \n    # Sidebar configuration\n    with st.sidebar:\n        st.header(\"‚öôÔ∏è Configuration\")\n        \n        # Motion detection sensitivity\n        sensitivity = st.slider(\n            \"Motion Detection Sensitivity\",\n            min_value=1,\n            max_value=10,\n            value=5,\n            help=\"Controls how small of an object can be detected. Higher = detects smaller/fainter satellites. Lower = only bright, obvious objects. Increase if missing small satellites.\"\n        )\n        \n        # Minimum clip duration\n        min_duration = st.slider(\n            \"Minimum Clip Duration (seconds)\",\n            min_value=0.3,\n            max_value=5.0,\n            value=1.5,\n            step=0.1,\n            help=\"Filters out quick flashes and noise. Objects must be visible for at least this long to be saved. Increase to reduce false positives from camera noise or birds.\"\n        )\n        \n        # Maximum clip duration (always enabled)\n        max_duration = st.slider(\n            \"Maximum Clip Duration (seconds)\",\n            min_value=5.0,\n            max_value=120.0,\n            value=80.0,\n            step=1.0,\n            help=\"Filters out stationary objects like stars (which appear to move due to Earth's rotation). Objects visible longer than this are discarded. Slow-moving satellites can take 60-100+ seconds to cross the frame.\"\n        )\n        \n        # Frame skip rate for performance\n        frame_skip = st.slider(\n            \"Frame Skip Rate\",\n            min_value=1,\n            max_value=10,\n            value=3,\n            help=\"Processes every Nth frame to speed up analysis. Higher = faster processing but may miss very fast meteors. Use 1 for high accuracy, 3-5 for balanced, 7+ for long videos.\"\n        )\n        \n        st.markdown(\"---\")\n        \n        # Settings recommendations (always visible, updates after video upload)\n        st.markdown(\"### üí° Recommended Settings\")\n        \n        # Use stored recommendations or defaults\n        if st.session_state.recommendations:\n            rec = st.session_state.recommendations\n            # Format values for display\n            sensitivity_val = rec['sensitivity']\n            min_dur_val = f\"{rec['min_duration']}s\"\n            max_dur_val = f\"{rec['max_duration']}s\"\n            frame_skip_val = rec['frame_skip']\n        else:\n            # Default recommendations before video upload - show dashes\n            rec = {\n                'explanations': ['Upload a video to see customized recommendations']\n            }\n            sensitivity_val = \"-\"\n            min_dur_val = \"-\"\n            max_dur_val = \"-\"\n            frame_skip_val = \"-\"\n        \n        # Create a clean grid layout for recommended values\n        rec_col1, rec_col2 = st.columns(2)\n        \n        with rec_col1:\n            st.metric(label=\"Sensitivity\", value=sensitivity_val)\n            st.metric(label=\"Min Duration\", value=min_dur_val)\n        \n        with rec_col2:\n            st.metric(label=\"Max Duration\", value=max_dur_val)\n            st.metric(label=\"Frame Skip\", value=frame_skip_val)\n        \n        # Show explanations in an expander for a cleaner look\n        if rec['explanations']:\n            with st.expander(\"‚ÑπÔ∏è Why these settings?\"):\n                for explanation in rec['explanations']:\n                    st.markdown(f\"‚Ä¢ {explanation}\")\n\n    # Main content area\n    col1, col2 = st.columns([2, 1])\n    \n    with col1:\n        st.header(\"üìÅ Video Upload\")\n        \n        # File uploader with drag and drop\n        uploaded_file = st.file_uploader(\n            \"Drop your night sky video here or click to browse\",\n            type=['mp4', 'avi', 'mov', 'mkv'],\n            help=\"Supported formats: MP4, AVI, MOV, MKV. Best results with stable, tripod-mounted footage.\"\n        )\n        \n        st.warning(\"‚ö†Ô∏è **Important for videos >3 minutes:** Use the recommended Frame Skip settings shown in the sidebar after uploading. Higher frame skip (5-6) prevents connection timeouts during processing.\")\n        \n        if uploaded_file is not None:\n            # Display video info and generate recommendations\n            with st.expander(\"üìä Video Information\", expanded=True):\n                video_info = get_video_info(uploaded_file)\n                \n                # Generate recommendations based on video properties and content analysis\n                if 'error' not in video_info:\n                    # Check if recommendations need to be generated\n                    previous_recommendations = st.session_state.recommendations\n                    st.session_state.recommendations = recommend_settings(video_info, uploaded_file)\n                    \n                    # Trigger rerun if recommendations just changed from None to actual values\n                    if previous_recommendations is None and st.session_state.recommendations is not None:\n                        st.rerun()\n                \n                col_a, col_b, col_c = st.columns(3)\n                \n                with col_a:\n                    st.metric(\"Duration\", video_info.get('duration', 'Unknown'))\n                    st.metric(\"FPS\", video_info.get('fps', 'Unknown'))\n                \n                with col_b:\n                    st.metric(\"Resolution\", video_info.get('resolution', 'Unknown'))\n                    st.metric(\"File Size\", video_info.get('file_size', 'Unknown'))\n                \n                with col_c:\n                    st.metric(\"Format\", video_info.get('format', 'Unknown'))\n                    st.metric(\"Bitrate\", video_info.get('bitrate', 'Unknown'))\n            \n            # Process video button\n            if st.button(\"üöÄ Start Analysis\", type=\"primary\", use_container_width=True):\n                process_video(uploaded_file, sensitivity, min_duration, max_duration, frame_skip)\n    \n    with col2:\n        st.header(\"üìà Processing Status\")\n        \n        if not st.session_state.processing_complete:\n            st.info(\"Upload a video to begin analysis\")\n            \n            # Processing pipeline visualization\n            st.markdown(\"**Analysis Pipeline:**\")\n            st.markdown(\"1. üé• Motion Detection\")\n            st.markdown(\"2. üìä Feature Extraction\") \n            st.markdown(\"3. üß† ML Classification\")\n            st.markdown(\"4. üìã Results Generation\")\n        else:\n            st.success(\"‚úÖ Processing Complete!\")\n            \n            if st.button(\"üîÑ Process New Video\", use_container_width=True):\n                reset_session()\n                st.rerun()\n    \n    # FAQ / Troubleshooting Section - Under video upload\n    st.markdown(\"---\")\n    st.info(\"**‚ùì Quick Troubleshooting Tips**\")\n    \n    col_faq1, col_faq2 = st.columns(2)\n    \n    with col_faq1:\n        with st.expander(\"üîç Too many false detections?\"):\n            st.markdown(\"\"\"\n            - Lower sensitivity to 2-4\n            - Increase minimum duration to 2-3s\n            - Check footage quality\n            \"\"\")\n        \n        with st.expander(\"‚è±Ô∏è Processing too slow?\"):\n            st.markdown(\"\"\"\n            - Increase frame skip to 5-6\n            - Use recommended settings\n            \"\"\")\n    \n    with col_faq2:\n        with st.expander(\"üòï Missing satellites?\"):\n            st.markdown(\"\"\"\n            **If satellites are not being detected:**\n            - **Increase max duration to 80-100s** (slow satellites can take 60+ seconds to cross!)\n            - **Increase sensitivity to 6-7** (detects smaller, dimmer objects)\n            - **Decrease min duration to 1.0-1.5s** (catches brief passes)\n            - **Lower frame skip to 2-3** (processes more frames)\n            \n            **Note:** Many satellites move very slowly and need 60-100 seconds to cross the frame. Max duration is the most important setting!\n            \"\"\")\n        \n        with st.expander(\"üé® Color codes?\"):\n            st.markdown(\"\"\"\n            - üî¥ **RED** = Satellites\n            - üü° **YELLOW** = Meteors\n            \"\"\")\n\n    # Results section\n    if st.session_state.processing_complete and st.session_state.results_data is not None:\n        display_results()\n\ndef process_video(uploaded_file, sensitivity, min_duration, max_duration, frame_skip):\n    \"\"\"Process the uploaded video through the complete pipeline\"\"\"\n    \n    # Create temporary file\n    temp_path = f\"temp_uploads/{uploaded_file.name}\"\n    os.makedirs(\"temp_uploads\", exist_ok=True)\n    os.makedirs(\"processed_clips\", exist_ok=True)\n    os.makedirs(\"results\", exist_ok=True)\n    \n    with open(temp_path, \"wb\") as f:\n        f.write(uploaded_file.getbuffer())\n    \n    # Get video info for database\n    video_info = get_video_info(uploaded_file)\n    \n    # Progress tracking\n    progress_bar = st.progress(0)\n    status_text = st.empty()\n    \n    try:\n        # Stage 1: Motion Detection\n        status_text.text(\"üé• Stage 1/4: Detecting motion in video...\")\n        progress_bar.progress(10)\n        \n        processor = VideoProcessor(\n            sensitivity=sensitivity,\n            min_duration=min_duration,\n            max_duration=max_duration,\n            frame_skip=frame_skip\n        )\n        \n        # Progress callback to keep connection alive during long processing\n        callback_count = [0]\n        \n        def video_progress_callback(current_frame, total_frames):\n            callback_count[0] += 1\n            \n            # Calculate actual progress percentage\n            frame_percent = (current_frame / max(total_frames, 1)) * 100\n            stage_progress = 10 + min(int(frame_percent * 0.2), 20)  # 10-30%\n            \n            # Update progress bar\n            progress_bar.progress(stage_progress)\n            \n            # CRITICAL: Update status text with unique content to prevent timeout\n            # This keeps WebSocket alive even if progress bar value repeats\n            status_text.text(f\"üé• Stage 1/4: Detecting motion... ({callback_count[0]} updates, frame {current_frame})\")\n        \n        motion_clips, metadata = processor.process_video(temp_path, progress_callback=video_progress_callback)\n        progress_bar.progress(30)\n        \n        if not motion_clips:\n            st.error(\"No motion detected in video. Try lowering sensitivity or check video quality.\")\n            return\n        \n        if not metadata:\n            max_filter_msg = f\"\\n- Objects appear for more than {max_duration} seconds (maximum duration filter enabled)\" if max_duration else \"\"\n            max_solution_msg = \"\\n- Disable or increase the maximum duration filter\" if max_duration else \"\"\n            \n            st.error(f\"\"\"‚ö†Ô∏è Motion detected but all objects were filtered out!\n            \n**Possible causes:**\n- Objects appear for less than {min_duration} seconds (current minimum duration){max_filter_msg}\n- Objects are intermittent or flickering\n\n**Solutions:**\n- Lower the minimum duration threshold (try 0.3 or 0.5 seconds)\n- Increase sensitivity to capture more motion{max_solution_msg}\n- Check if video has stable objects that move continuously\n            \"\"\")\n            return\n        \n        st.success(f\"‚úÖ Detected {len(motion_clips)} motion events with {len(set([m['clip_id'] for m in metadata]))} tracked objects\")\n        \n        # Stage 2: Feature Extraction\n        status_text.text(\"üìä Stage 2/4: Extracting movement features... (0/3 steps)\")\n        progress_bar.progress(40)\n        \n        extractor = FeatureExtractor()\n        \n        # Keep connection alive during feature extraction\n        status_text.text(\"üìä Stage 2/4: Extracting movement features... (1/3 steps)\")\n        progress_bar.progress(50)\n        \n        features_df = extractor.extract_features(metadata)\n        \n        status_text.text(\"üìä Stage 2/4: Extracting movement features... (2/3 steps)\")\n        progress_bar.progress(60)\n        \n        # Extra keepalive\n        status_text.text(\"üìä Stage 2/4: Extracting movement features... (3/3 steps)\")\n        progress_bar.progress(70)\n        \n        # Stage 3: ML Classification\n        status_text.text(\"üß† Stage 3/4: Classifying objects with AI... (processing)\")\n        progress_bar.progress(75)\n        \n        classifier = MLClassifier()\n        \n        # Keep connection alive during classification\n        status_text.text(\"üß† Stage 3/4: Classifying objects with AI... (analyzing patterns)\")\n        progress_bar.progress(82)\n        \n        results_df = classifier.classify_objects(features_df)\n        \n        status_text.text(\"üß† Stage 3/4: Classifying objects with AI... (finalizing)\")\n        progress_bar.progress(90)\n        \n        # Stage 4: Add color-coded rectangles and generate results\n        status_text.text(\"üìã Stage 4/4: Adding color-coded rectangles... (starting)\")\n        progress_bar.progress(92)\n        \n        # Add colored rectangles based on classification\n        from utils import add_colored_rectangles_to_clips\n        \n        # Progress callback for rectangle drawing to prevent timeout\n        rectangle_callback_count = [0]\n        \n        def rectangle_progress_callback(current_clip, total_clips):\n            rectangle_callback_count[0] += 1\n            percent = int((current_clip / max(total_clips, 1)) * 100)\n            \n            # Update progress bar (92-97% range for this stage)\n            stage_progress = 92 + min(int(percent * 0.05), 5)\n            progress_bar.progress(stage_progress)\n            \n            # CRITICAL: Update status with unique text to prevent WebSocket timeout\n            status_text.text(f\"üìã Stage 4/4: Adding rectangles... (clip {current_clip}/{total_clips}, {percent}%)\")\n        \n        motion_clips = add_colored_rectangles_to_clips(\n            motion_clips, metadata, results_df, \n            progress_callback=rectangle_progress_callback\n        )\n        \n        status_text.text(\"üìã Stage 4/4: Rectangles complete, finalizing...\")\n        progress_bar.progress(97)\n        \n        status_text.text(\"üìã Stage 4/4: Finalizing results...\")\n        \n        # Store results in session state\n        st.session_state.results_data = results_df\n        st.session_state.processed_clips = motion_clips\n        st.session_state.metadata = metadata\n        st.session_state.video_info = video_info\n        st.session_state.processing_complete = True\n        \n        # Save to database\n        try:\n            session_id = st.session_state.db_service.save_analysis_session(\n                video_info, results_df, motion_clips\n            )\n            if session_id:\n                st.session_state.current_session_id = session_id\n        except Exception as e:\n            st.warning(f\"Could not save to database: {e}\")\n        \n        progress_bar.progress(100)\n        status_text.text(\"‚úÖ Analysis Complete!\")\n        \n        # Store video path for clip extraction (will be cleaned up on reset)\n        st.session_state.uploaded_video_path = temp_path\n        \n        st.rerun()\n        \n    except Exception as e:\n        st.error(f\"Error processing video: {str(e)}\")\n        if os.path.exists(temp_path):\n            os.remove(temp_path)\n\ndef display_results():\n    \"\"\"Display the analysis results with interactive dashboard\"\"\"\n    \n    st.header(\"üéØ Detection Results\")\n    \n    results_df = st.session_state.results_data\n    \n    # Summary metrics\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        total_detections = len(results_df)\n        st.metric(\"Total Detections\", total_detections)\n    \n    with col2:\n        satellites = len(results_df[results_df['classification'] == 'Satellite'])\n        st.metric(\"üõ∞Ô∏è Satellites\", satellites)\n    \n    with col3:\n        meteors = len(results_df[results_df['classification'] == 'Meteor'])\n        st.metric(\"‚òÑÔ∏è Meteors\", meteors)\n    \n    # Classification distribution chart\n    st.subheader(\"üìä Classification Distribution\")\n    \n    classification_counts = results_df['classification'].value_counts()\n    \n    fig_pie = px.pie(\n        values=classification_counts.values,\n        names=classification_counts.index,\n        title=\"Object Classification Breakdown\",\n        color_discrete_map={\n            'Star': '#ffd700',\n            'Satellite': '#1f77b4',\n            'Meteor': '#ff7f0e', \n            'Junk': '#d62728'\n        }\n    )\n    \n    st.plotly_chart(fig_pie, use_container_width=True)\n    \n    # Feature analysis\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.subheader(\"üèÉ Speed Analysis\")\n        fig_speed = px.histogram(\n            results_df,\n            x='avg_speed',\n            color='classification',\n            title=\"Speed Distribution by Classification\",\n            labels={'avg_speed': 'Average Speed (pixels/frame)'}\n        )\n        st.plotly_chart(fig_speed, use_container_width=True)\n    \n    with col2:\n        st.subheader(\"üìè Duration Analysis\") \n        fig_duration = px.histogram(\n            results_df,\n            x='duration',\n            color='classification',\n            title=\"Duration Distribution by Classification\",\n            labels={'duration': 'Duration (seconds)'}\n        )\n        st.plotly_chart(fig_duration, use_container_width=True)\n    \n    # Trajectory Visualization\n    if st.session_state.metadata:\n        st.subheader(\"üéØ Trajectory Analysis\")\n        \n        # Create visualizer\n        video_info = st.session_state.video_info\n        width = video_info.get('width', 1920)\n        height = video_info.get('height', 1080)\n        fps = video_info.get('fps', 30)\n        \n        visualizer = TrajectoryVisualizer(frame_width=width, frame_height=height)\n        \n        # Create tabs for different visualizations\n        tab1, tab2, tab3, tab4 = st.tabs([\"üìç Trajectories\", \"üå°Ô∏è Speed Heatmap\", \"üß≠ Directions\", \"‚è±Ô∏è Timeline\"])\n        \n        with tab1:\n            traj_fig = visualizer.create_trajectory_plot(st.session_state.metadata)\n            st.plotly_chart(traj_fig, use_container_width=True)\n            st.caption(\"Interactive trajectory plot showing object paths. Green stars = start, Red X = end\")\n        \n        with tab2:\n            heatmap_fig = visualizer.create_speed_heatmap(st.session_state.metadata)\n            st.plotly_chart(heatmap_fig, use_container_width=True)\n            st.caption(\"Heatmap showing average object speed across different regions of the frame\")\n        \n        with tab3:\n            direction_fig = visualizer.create_direction_plot(st.session_state.metadata)\n            st.plotly_chart(direction_fig, use_container_width=True)\n            st.caption(\"Polar plot showing the distribution of object movement directions\")\n        \n        with tab4:\n            timeline_fig = visualizer.create_timeline_plot(st.session_state.metadata, fps=fps)\n            st.plotly_chart(timeline_fig, use_container_width=True)\n            st.caption(\"Timeline showing when objects were detected throughout the video\")\n    \n    # Detailed results table\n    st.subheader(\"üìã Detailed Results\")\n    \n    # Filter options\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        selected_classifications = st.multiselect(\n            \"Filter by Classification\",\n            options=results_df['classification'].unique(),\n            default=results_df['classification'].unique()\n        )\n    \n    with col2:\n        min_confidence = st.slider(\n            \"Minimum Confidence\",\n            min_value=0.0,\n            max_value=1.0,\n            value=0.0,\n            step=0.1\n        )\n    \n    with col3:\n        sort_by = st.selectbox(\n            \"Sort by\",\n            options=['confidence', 'avg_speed', 'duration'],\n            index=0\n        )\n    \n    # Apply filters\n    filtered_df = results_df[\n        (results_df['classification'].isin(selected_classifications)) &\n        (results_df['confidence'] >= min_confidence)\n    ].sort_values(sort_by, ascending=False)\n    \n    # Display filtered table\n    st.dataframe(\n        filtered_df,\n        use_container_width=True,\n        column_config={\n            'clip_id': 'Clip ID',\n            'classification': 'Classification',\n            'confidence': st.column_config.ProgressColumn(\n                'Confidence',\n                min_value=0,\n                max_value=1,\n                format='%.2f'\n            ),\n            'avg_speed': 'Avg Speed',\n            'speed_consistency': 'Speed Consistency', \n            'duration': 'Duration (s)'\n        }\n    )\n    \n    # Download section\n    st.subheader(\"üì• Download Results\")\n    \n    # CSV download (always available)\n    csv_buffer = BytesIO()\n    results_df.to_csv(csv_buffer, index=False)\n    csv_buffer.seek(0)\n    \n    st.download_button(\n        label=\"üìä Download CSV Report\",\n        data=csv_buffer,\n        file_name=f\"skyseer_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\",\n        mime=\"text/csv\",\n        use_container_width=True\n    )\n    \n    # Classification-specific clip downloads\n    st.markdown(\"**Download Video Clips by Category:**\")\n    \n    # Get unique classifications with counts (only show Satellite and Meteor)\n    classification_counts = results_df['classification'].value_counts()\n    # Filter to only show Satellite and Meteor in downloads\n    classification_counts = classification_counts[classification_counts.index.isin(['Satellite', 'Meteor'])]\n    \n    # Create emoji mapping (only Satellite and Meteor)\n    emoji_map = {\n        'Satellite': 'üõ∞Ô∏è',\n        'Meteor': '‚òÑÔ∏è'\n    }\n    \n    # Initialize session state for prepared downloads\n    if 'prepared_zips' not in st.session_state:\n        st.session_state.prepared_zips = {}\n    \n    # Create download buttons for Satellite and Meteor only\n    cols = st.columns(min(len(classification_counts), 2)) if len(classification_counts) > 0 else st.columns(1)\n    \n    for idx, (classification, count) in enumerate(classification_counts.items()):\n        with cols[idx % len(cols)]:\n            emoji = emoji_map.get(classification, 'üì¶')\n            \n            # Check if this classification's ZIP is already prepared\n            if classification in st.session_state.prepared_zips:\n                # Show download button\n                st.download_button(\n                    label=f\"üì¶ Download {classification} ZIP\",\n                    data=st.session_state.prepared_zips[classification],\n                    file_name=f\"skyseer_{classification.lower()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip\",\n                    mime=\"application/zip\",\n                    use_container_width=True,\n                    key=f\"download_btn_{classification}\"\n                )\n            else:\n                # Show prepare button\n                if st.button(\n                    f\"{emoji} {classification} ({count})\",\n                    key=f\"prepare_{classification}\",\n                    use_container_width=True\n                ):\n                    # Prepare the ZIP with simple loading\n                    with st.spinner(\"\"):\n                        zip_buffer = create_download_zip(\n                            st.session_state.processed_clips, \n                            results_df,\n                            classification_filter=classification,\n                            metadata=st.session_state.metadata\n                        )\n                        # Convert BytesIO to bytes for session state storage\n                        st.session_state.prepared_zips[classification] = zip_buffer.getvalue()\n                        st.rerun()\n    \n    # Object Clip Extractor Section\n    st.markdown(\"---\")\n    st.subheader(\"üé¨ Object Clip Extractor\")\n    st.markdown(\"Extract normal-speed clips of specific detected objects from the original video.\")\n    \n    # Check if we have the uploaded video available\n    if st.session_state.uploaded_video_path and os.path.exists(st.session_state.uploaded_video_path):\n        # Quick reference table\n        st.markdown(\"**Available Objects:**\")\n        \n        # Create reference table with key info\n        ref_data = []\n        for _, row in results_df.iterrows():\n            ref_data.append({\n                'ID': int(row['clip_id']),\n                'Classification': row['classification'],\n                'Confidence': f\"{row['confidence']*100:.0f}%\",\n                'Duration': f\"{row['duration']:.1f}s\",\n                'Avg Speed': f\"{row['avg_speed']:.1f} px/f\"\n            })\n        \n        ref_df = pd.DataFrame(ref_data)\n        st.dataframe(ref_df, use_container_width=True, hide_index=True)\n        \n        # Input field for object IDs\n        st.markdown(\"**Enter Object ID(s):**\")\n        col_input, col_button = st.columns([3, 1])\n        \n        with col_input:\n            object_input = st.text_input(\n                \"Enter single ID (e.g., '3') or multiple IDs separated by commas (e.g., '3, 7, 12')\",\n                key=\"object_id_input\",\n                label_visibility=\"collapsed\"\n            )\n        \n        with col_button:\n            extract_button = st.button(\"Extract Clip\", use_container_width=True, type=\"primary\")\n        \n        # Process extraction when button is clicked\n        if extract_button and object_input:\n            try:\n                # Parse input IDs\n                object_ids = [int(x.strip()) for x in object_input.split(',')]\n                \n                # Validate IDs\n                valid_ids = results_df['clip_id'].unique().tolist()\n                invalid_ids = [oid for oid in object_ids if oid not in valid_ids]\n                \n                if invalid_ids:\n                    st.error(f\"‚ùå Invalid Object ID(s): {', '.join(map(str, invalid_ids))}. Valid range: {min(valid_ids)}-{max(valid_ids)}\")\n                else:\n                    with st.spinner(f\"Extracting clip for Object(s): {', '.join(map(str, object_ids))}...\"):\n                        # Extract the clip\n                        clip_path = extract_object_clip(\n                            object_ids,\n                            st.session_state.uploaded_video_path,\n                            st.session_state.metadata,\n                            results_df\n                        )\n                        \n                        if clip_path and os.path.exists(clip_path):\n                            # Read the clip file\n                            with open(clip_path, 'rb') as f:\n                                clip_data = f.read()\n                            \n                            # Generate filename\n                            if len(object_ids) == 1:\n                                filename = f\"Object_{object_ids[0]}_clip.mp4\"\n                            else:\n                                filename = f\"Objects_{'_'.join(map(str, object_ids))}_clip.mp4\"\n                            \n                            # Display success message\n                            st.success(f\"‚úÖ Clip extracted successfully for {len(object_ids)} object(s)!\")\n                            \n                            # Show object details\n                            for obj_id in object_ids:\n                                obj_row = results_df[results_df['clip_id'] == obj_id].iloc[0]\n                                st.info(\n                                    f\"**Object {obj_id}:** {obj_row['classification']} | \"\n                                    f\"Confidence: {obj_row['confidence']*100:.0f}% | \"\n                                    f\"Duration: {obj_row['duration']:.1f}s | \"\n                                    f\"Speed: {obj_row['avg_speed']:.1f} px/frame\"\n                                )\n                            \n                            # Download button\n                            st.download_button(\n                                label=f\"üì• Download {filename}\",\n                                data=clip_data,\n                                file_name=filename,\n                                mime=\"video/mp4\",\n                                use_container_width=True\n                            )\n                            \n                            # Clean up temp file\n                            try:\n                                os.remove(clip_path)\n                            except:\n                                pass\n                        else:\n                            st.error(\"‚ùå Failed to extract clip. Please try again.\")\n                            \n            except ValueError:\n                st.error(\"‚ùå Invalid input format. Please enter numeric IDs separated by commas (e.g., '3, 7, 12')\")\n            except Exception as e:\n                st.error(f\"‚ùå Error extracting clip: {str(e)}\")\n    else:\n        st.warning(\"‚ö†Ô∏è Original video file not available. Please process a new video to use this feature.\")\n    \n    # Trajectory Prediction Analysis Section\n    st.markdown(\"---\")\n    st.subheader(\"üéØ Trajectory Prediction Analysis\")\n    st.markdown(\"Demonstrates predictive modeling capabilities - comparing actual object paths vs. predicted trajectories using linear regression.\")\n    \n    # Initialize trajectory_results outside the expander\n    trajectory_results = None\n    \n    with st.expander(\"üìä View Trajectory Predictions\", expanded=False):\n        st.info(\"Click 'Analyze Trajectories' below to run predictive modeling analysis. This may take a moment for videos with many detections.\")\n        \n        if st.button(\"üöÄ Analyze Trajectories\", use_container_width=True):\n            with st.spinner(\"Analyzing trajectories...\"):\n                # Prepare detection data for trajectory analysis\n                detections_list = []\n                for meta in st.session_state.metadata:\n                    detections_list.append({\n                        'object_id': meta['clip_id'],\n                        'frame_number': meta['frame_number'],\n                        'center_x': meta['centroid_x'],\n                        'center_y': meta['centroid_y']\n                    })\n                \n                if detections_list:\n                    detections_df = pd.DataFrame(detections_list)\n                    \n                    # Analyze all trajectories\n                    trajectory_results = analyze_all_trajectories(detections_df, method='linear')\n            \n                    if trajectory_results:\n                        # Show summary statistics\n                        summary = get_trajectory_summary_stats(trajectory_results)\n                        \n                        col1, col2, col3, col4 = st.columns(4)\n                        with col1:\n                            st.metric(\"Objects Analyzed\", summary['total_objects'])\n                        with col2:\n                            st.metric(\"Avg Prediction Error\", f\"{summary['avg_mean_error']:.2f} px\")\n                        with col3:\n                            st.metric(\"Avg R¬≤ Score\", f\"{summary['avg_r2_x']:.3f}\")\n                        with col4:\n                            st.metric(\"Highly Predictable\", f\"{summary['highly_predictable']}/{summary['total_objects']}\")\n                        \n                        st.caption(\"**Interpretation:** R¬≤ > 0.95 indicates highly linear motion (typical for satellites). Lower R¬≤ suggests curved/irregular paths (meteors, aircraft maneuvers).\")\n                        \n                        # Show error plot\n                        error_fig = create_trajectory_error_plot(trajectory_results)\n                        if error_fig:\n                            st.plotly_chart(error_fig, use_container_width=True)\n                        \n                        # Allow user to select specific objects for detailed view\n                        st.markdown(\"**Detailed Trajectory Comparison:**\")\n                        selected_obj = st.selectbox(\n                            \"Select object to view prediction details\",\n                            options=[r['object_id'] for r in trajectory_results],\n                            format_func=lambda x: f\"Object #{x}\"\n                        )\n                        \n                        if selected_obj:\n                            selected_result = next(r for r in trajectory_results if r['object_id'] == selected_obj)\n                            \n                            # Get classification for this object\n                            obj_classification = results_df[results_df['clip_id'] == selected_obj]['classification'].iloc[0]\n                            \n                            # Show detailed comparison plot\n                            comparison_fig = create_trajectory_comparison_plot(selected_result, obj_classification)\n                            st.plotly_chart(comparison_fig, use_container_width=True)\n                            \n                            # Show prediction metrics\n                            col1, col2, col3 = st.columns(3)\n                            with col1:\n                                st.metric(\"Mean Error\", f\"{selected_result['mean_error']:.2f} px\")\n                            with col2:\n                                st.metric(\"Max Error\", f\"{selected_result['max_error']:.2f} px\")\n                            with col3:\n                                st.metric(\"RMSE\", f\"{selected_result['rmse_total']:.2f} px\")\n                            \n                            st.info(f\"‚ú® **Technical Insight:** This object's trajectory has an R¬≤ score of {selected_result['r2_x']:.3f}, \"\n                                   f\"indicating {'highly' if selected_result['r2_x'] > 0.95 else 'moderately'} predictable linear motion. \"\n                                   f\"{'This is characteristic of satellite passes.' if selected_result['r2_x'] > 0.95 else 'This suggests non-linear or irregular movement patterns.'}\")\n                    else:\n                        st.info(\"No trajectory data available for analysis.\")\n    \n    # Technical Details Section\n    st.markdown(\"---\")\n    st.subheader(\"üìö Technical Details\")\n    st.markdown(\"Deep dive into the machine learning pipeline and computer vision techniques used in SkySeer.\")\n    \n    with st.expander(\"üî¨ View ML Pipeline Architecture\", expanded=False):\n        st.markdown(\"\"\"\n        ### **System Architecture Overview**\n        \n        SkySeer employs a sophisticated multi-stage pipeline combining computer vision and unsupervised machine learning:\n        \n        ---\n        \n        #### **Stage 1: Motion Detection**\n        - **Algorithm:** MOG2 (Mixture of Gaussians) Background Subtraction\n        - **Purpose:** Identify moving objects against the static night sky\n        - **Parameters:** Adaptive variance threshold (45), history frames (500)\n        - **Output:** Motion blobs with bounding boxes and frame-by-frame tracking\n        \n        #### **Stage 2: Feature Extraction (11-Dimensional Feature Space)**\n        \n        Each detected object is transformed into a numerical \"flight signature\" consisting of:\n        \n        1. **avg_speed** - Average velocity in pixels/frame\n        2. **speed_consistency** - Standard deviation of speed (0-1, lower = more consistent)\n        3. **duration** - Total observation time in seconds\n        4. **linearity** - How straight the path is (0-1, higher = straighter)\n        5. **direction_changes** - Number of significant heading changes\n        6. **size_consistency** - Variation in object size across frames\n        7. **acceleration** - Rate of speed change\n        8. **blinking_score** - Detection of periodic brightness changes (aircraft lights)\n        9. **satellite_score** - Weighted score favoring satellite characteristics\n        10. **meteor_score** - Weighted score favoring meteor characteristics\n        11. **avg_brightness** - Average pixel intensity\n        \n        **Why not visual features?** In low-light conditions, kinematic patterns are more reliable than pixel-level features.\n        \n        ---\n        \n        #### **Stage 3: ML Classification (K-Means Clustering)**\n        \n        - **Algorithm:** K-Means Unsupervised Clustering\n        - **Number of Clusters:** 2 (Satellite, Meteor)\n        - **Preprocessing:** StandardScaler normalization on all 11 features\n        - **Classification Logic:**\n          - Cluster centers are analyzed for characteristic patterns\n          - **Satellites:** Moderate speed (0.6-35 px/frame), high linearity, long duration (3-25s)\n          - **Meteors:** High speed (>10 px/frame), short duration (1-4s), high linearity\n          - Objects not matching either pattern are classified as Junk\n        \n        **Why K-Means?**\n        - No labeled training data required\n        - Adapts automatically to new motion patterns\n        - Computationally efficient for real-time analysis\n        \n        ---\n        \n        #### **Stage 4: Aggressive Filtering**\n        \n        - **Speed Validation:** Objects <0.3 px/frame ‚Üí Junk (stationary noise)\n        - **Target:** <10 detections per video (precision over recall)\n        - **Confidence Scoring:** Distance from cluster center determines confidence (0-100%)\n        \n        ---\n        \n        ### **Performance Characteristics**\n        \n        **Strengths:**\n        - Excellent at detecting obvious satellite passes and meteor streaks\n        - Minimal false positives due to aggressive filtering\n        - No training data dependency\n        \n        **Limitations:**\n        - May miss very faint or very slow-moving objects\n        - Optimized for tripod-mounted, stable footage\n        - Best results with clear, dark skies\n        \n        ---\n        \n        ### **Video Processing**\n        \n        - **Frame Skip:** Adaptive (3-6) based on video length and FPS\n        - **Output Speed:** 10x speedup for efficient review\n        - **Color Coding:** RED = Satellites, YELLOW = Meteors\n        \n        ---\n        \n        ### **Technical Stack**\n        \n        - **Computer Vision:** OpenCV (MOG2, contour detection)\n        - **Machine Learning:** scikit-learn (K-Means, StandardScaler)\n        - **Visualization:** Plotly (interactive charts), Streamlit (web interface)\n        - **Data Processing:** NumPy, Pandas\n        - **Database:** PostgreSQL (multi-night tracking capability)\n        \"\"\")\n        \n        # Show actual feature distribution\n        st.markdown(\"### **Feature Distribution in Current Dataset**\")\n        st.markdown(\"These box plots show how different features separate Satellites from Meteors in your video:\")\n        \n        feature_info = {\n            'avg_speed': {\n                'title': 'Average Speed (pixels/frame)',\n                'explanation': '**What to look for:** Meteors typically move much faster (>10 px/frame) than satellites (0.6-35 px/frame). The higher the box, the faster the objects moved.'\n            },\n            'speed_consistency': {\n                'title': 'Speed Consistency (0-1, lower = more consistent)',\n                'explanation': '**What to look for:** Both satellites and meteors should have LOW values (boxes near 0), meaning constant speed. Higher values indicate erratic movement (likely junk).'\n            },\n            'duration': {\n                'title': 'Duration (seconds)',\n                'explanation': '**What to look for:** Satellites typically appear for 3-25 seconds (longer boxes), while meteors are brief flashes lasting 1-4 seconds (boxes near bottom).'\n            },\n            'linearity': {\n                'title': 'Path Linearity (0-1, higher = straighter)',\n                'explanation': '**What to look for:** Both satellites and meteors should have HIGH values (boxes near 1), meaning straight paths. Curved or zigzag paths suggest planes or noise.'\n            }\n        }\n        \n        available_features = [col for col in feature_info.keys() if col in results_df.columns]\n        \n        if available_features:\n            for feature in available_features:\n                info = feature_info[feature]\n                \n                fig = px.box(\n                    results_df,\n                    x='classification',\n                    y=feature,\n                    color='classification',\n                    title=info['title'],\n                    labels={feature: info['title']}\n                )\n                fig.update_layout(\n                    plot_bgcolor='#0e1117',\n                    paper_bgcolor='#0e1117',\n                    font=dict(color='white'),\n                    height=350\n                )\n                st.plotly_chart(fig, use_container_width=True)\n                st.caption(info['explanation'])\n    \n    # PDF Mission Report Download\n    st.markdown(\"---\")\n    st.subheader(\"üìÑ Export Mission Report\")\n    st.markdown(\"Generate a professional PDF report documenting all detections, technical parameters, and analysis results.\")\n    \n    if st.button(\"üì• Generate PDF Mission Report\", use_container_width=True):\n        with st.spinner(\"Generating professional mission report...\"):\n            pdf_buffer = generate_mission_report_pdf(\n                results_df,\n                st.session_state.metadata,\n                st.session_state.video_info,\n                trajectory_results\n            )\n            \n            if pdf_buffer:\n                st.success(\"‚úÖ Mission report generated successfully!\")\n                st.download_button(\n                    label=\"üì• Download PDF Report\",\n                    data=pdf_buffer,\n                    file_name=f\"SkySeer_Mission_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\",\n                    mime=\"application/pdf\",\n                    use_container_width=True\n                )\n            else:\n                st.error(\"‚ùå Failed to generate PDF report.\")\n\ndef generate_mission_report_pdf(results_df, metadata, video_info, trajectory_results=None):\n    \"\"\"\n    Generate a professional PDF mission report\n    \n    Args:\n        results_df: DataFrame with detection results\n        metadata: List of detection metadata\n        video_info: Dict with video information\n        trajectory_results: Optional trajectory prediction results\n        \n    Returns:\n        BytesIO buffer with PDF content\n    \"\"\"\n    try:\n        buffer = BytesIO()\n        doc = SimpleDocTemplate(buffer, pagesize=letter,\n                              rightMargin=72, leftMargin=72,\n                              topMargin=72, bottomMargin=18)\n        \n        story = []\n        styles = getSampleStyleSheet()\n        \n        title_style = ParagraphStyle(\n            'CustomTitle',\n            parent=styles['Heading1'],\n            fontSize=24,\n            textColor=colors.HexColor('#1f77b4'),\n            spaceAfter=30,\n            alignment=1\n        )\n        \n        heading_style = ParagraphStyle(\n            'CustomHeading',\n            parent=styles['Heading2'],\n            fontSize=14,\n            textColor=colors.HexColor('#2ca02c'),\n            spaceAfter=12,\n            spaceBefore=12\n        )\n        \n        title = Paragraph(\"SkySeer AI - Mission Report\", title_style)\n        story.append(title)\n        \n        timestamp = Paragraph(f\"<para align=center>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</para>\", \n                            styles[\"Normal\"])\n        story.append(timestamp)\n        story.append(Spacer(1, 0.3*inch))\n        \n        story.append(Paragraph(\"Video Information\", heading_style))\n        total_frames = video_info.get('total_frames', 0)\n        video_data = [\n            ['Parameter', 'Value'],\n            ['Filename', str(video_info.get('filename', 'N/A'))],\n            ['Duration', video_info.get('duration', 'N/A')],\n            ['Resolution', video_info.get('resolution', 'N/A')],\n            ['FPS', str(video_info.get('fps', 'N/A'))],\n            ['Total Frames', f\"{total_frames:,}\" if isinstance(total_frames, (int, float)) else str(total_frames)]\n        ]\n        \n        video_table = Table(video_data, colWidths=[2*inch, 3.5*inch])\n        video_table.setStyle(TableStyle([\n            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n            ('FONTSIZE', (0, 0), (-1, 0), 12),\n            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n            ('GRID', (0, 0), (-1, -1), 1, colors.black)\n        ]))\n        story.append(video_table)\n        story.append(Spacer(1, 0.3*inch))\n        \n        story.append(Paragraph(\"Detection Summary\", heading_style))\n        total_detections = len(results_df)\n        satellites = len(results_df[results_df['classification'] == 'Satellite'])\n        meteors = len(results_df[results_df['classification'] == 'Meteor'])\n        \n        summary_data = [\n            ['Metric', 'Count'],\n            ['Total Detections', str(total_detections)],\n            ['Satellites', str(satellites)],\n            ['Meteors', str(meteors)],\n        ]\n        \n        summary_table = Table(summary_data, colWidths=[2.5*inch, 1.5*inch])\n        summary_table.setStyle(TableStyle([\n            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n            ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n            ('FONTSIZE', (0, 0), (-1, 0), 12),\n            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n            ('BACKGROUND', (0, 1), (-1, -1), colors.lightblue),\n            ('GRID', (0, 0), (-1, -1), 1, colors.black)\n        ]))\n        story.append(summary_table)\n        story.append(Spacer(1, 0.3*inch))\n        \n        if trajectory_results:\n            story.append(Paragraph(\"Trajectory Analysis Summary\", heading_style))\n            summary = get_trajectory_summary_stats(trajectory_results)\n            \n            traj_data = [\n                ['Metric', 'Value'],\n                ['Objects Analyzed', str(summary['total_objects'])],\n                ['Avg Prediction Error', f\"{summary['avg_mean_error']:.2f} pixels\"],\n                ['Avg R¬≤ Score (X-axis)', f\"{summary['avg_r2_x']:.3f}\"],\n                ['Highly Predictable Objects', f\"{summary['highly_predictable']}/{summary['total_objects']}\"],\n            ]\n            \n            traj_table = Table(traj_data, colWidths=[3*inch, 2*inch])\n            traj_table.setStyle(TableStyle([\n                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n                ('FONTSIZE', (0, 0), (-1, 0), 12),\n                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n                ('BACKGROUND', (0, 1), (-1, -1), colors.lightgreen),\n                ('GRID', (0, 0), (-1, -1), 1, colors.black)\n            ]))\n            story.append(traj_table)\n            story.append(Spacer(1, 0.3*inch))\n        \n        story.append(PageBreak())\n        story.append(Paragraph(\"Detailed Detection Results\", heading_style))\n        \n        detection_data = [['ID', 'Classification', 'Confidence', 'Speed', 'Duration']]\n        for _, row in results_df.iterrows():\n            detection_data.append([\n                str(int(row['clip_id'])),\n                row['classification'],\n                f\"{row['confidence']*100:.0f}%\",\n                f\"{row['avg_speed']:.1f} px/f\",\n                f\"{row['duration']:.1f}s\"\n            ])\n        \n        det_table = Table(detection_data, colWidths=[0.6*inch, 1.3*inch, 1.1*inch, 1.1*inch, 1.1*inch])\n        det_table.setStyle(TableStyle([\n            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n            ('FONTSIZE', (0, 0), (-1, -1), 9),\n            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n            ('BACKGROUND', (0, 1), (-1, -1), colors.white),\n            ('GRID', (0, 0), (-1, -1), 1, colors.black),\n            ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.lightgrey])\n        ]))\n        story.append(det_table)\n        story.append(Spacer(1, 0.3*inch))\n        \n        story.append(PageBreak())\n        story.append(Paragraph(\"Technical Documentation\", heading_style))\n        \n        tech_text = \"\"\"\n        <b>Detection Pipeline:</b><br/>\n        1. MOG2 Background Subtraction for motion detection<br/>\n        2. 11-dimensional feature extraction (speed, linearity, duration, etc.)<br/>\n        3. K-Means clustering for unsupervised classification<br/>\n        4. Aggressive filtering to minimize false positives<br/>\n        <br/>\n        <b>Classification Criteria:</b><br/>\n        ‚Ä¢ Satellites: Moderate speed (0.6-35 px/frame), high linearity, 3-25s duration<br/>\n        ‚Ä¢ Meteors: High speed (>10 px/frame), short duration (1-4s), high linearity<br/>\n        ‚Ä¢ Junk: Objects not matching satellite/meteor patterns or speed <0.3 px/frame<br/>\n        <br/>\n        <b>System Characteristics:</b><br/>\n        ‚Ä¢ Target: <10 detections per video (precision over recall)<br/>\n        ‚Ä¢ Optimized for stable, tripod-mounted footage<br/>\n        ‚Ä¢ Best results with clear, dark skies<br/>\n        \"\"\"\n        \n        tech_para = Paragraph(tech_text, styles['Normal'])\n        story.append(tech_para)\n        \n        footer_text = f\"\"\"\n        <para align=center>\n        <br/><br/>\n        ---<br/>\n        <i>Generated by SkySeer AI - Advanced Sky Object Detection System</i><br/>\n        <i>Report Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</i>\n        </para>\n        \"\"\"\n        story.append(Spacer(1, 0.5*inch))\n        story.append(Paragraph(footer_text, styles['Normal']))\n        \n        doc.build(story)\n        buffer.seek(0)\n        return buffer\n        \n    except Exception as e:\n        print(f\"Error generating PDF: {e}\")\n        return None\n\ndef extract_object_clip(object_ids, video_path, metadata, results_df):\n    \"\"\"\n    Extract clips for specified object IDs with overlays and bounding boxes.\n    \n    Args:\n        object_ids: List of object IDs to extract\n        video_path: Path to the original video file\n        metadata: List of detection metadata\n        results_df: DataFrame with classification results\n    \n    Returns:\n        Path to the extracted clip video file\n    \"\"\"\n    import tempfile\n    \n    # Get video properties\n    cap = cv2.VideoCapture(video_path)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    \n    # Calculate buffer frames\n    buffer_before = int(2 * fps)  # 2 seconds before\n    buffer_after = int(1 * fps)   # 1 second after\n    \n    # Collect all frame ranges for requested objects\n    frame_segments = []\n    object_info = {}\n    \n    for obj_id in object_ids:\n        # Get metadata for this object\n        obj_detections = [m for m in metadata if m.get('clip_id') == obj_id]\n        if not obj_detections:\n            continue\n        \n        # Get frame range\n        frame_numbers = [d['frame_number'] for d in obj_detections]\n        start_frame = min(frame_numbers)\n        end_frame = max(frame_numbers)\n        \n        # Add buffers (respect video boundaries)\n        buffered_start = max(0, start_frame - buffer_before)\n        buffered_end = min(total_frames - 1, end_frame + buffer_after)\n        \n        frame_segments.append((buffered_start, buffered_end, obj_id))\n        \n        # Store object info for overlay\n        obj_row = results_df[results_df['clip_id'] == obj_id].iloc[0]\n        object_info[obj_id] = {\n            'classification': obj_row['classification'],\n            'confidence': obj_row['confidence'],\n            'avg_speed': obj_row['avg_speed'],\n            'detections': obj_detections\n        }\n    \n    if not frame_segments:\n        return None\n    \n    # Sort segments by start frame\n    frame_segments.sort(key=lambda x: x[0])\n    \n    # Create temporary output file\n    output_path = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4').name\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    \n    # Process each segment\n    for start_frame, end_frame, obj_id in frame_segments:\n        obj = object_info[obj_id]\n        \n        # Set video to start frame\n        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n        \n        for frame_idx in range(start_frame, end_frame + 1):\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            # Find if there's a detection at this frame for this object\n            detection_at_frame = None\n            for det in obj['detections']:\n                if det['frame_number'] == frame_idx:\n                    detection_at_frame = det\n                    break\n            \n            # Draw bounding box if detection exists at this frame\n            if detection_at_frame:\n                x = detection_at_frame['bbox_x']\n                y = detection_at_frame['bbox_y']\n                w = detection_at_frame['bbox_width']\n                h = detection_at_frame['bbox_height']\n                \n                # Draw rectangle with padding\n                pad = 8\n                cv2.rectangle(frame,\n                            (max(0, x - pad), max(0, y - pad)),\n                            (min(width - 1, x + w + pad), min(height - 1, y + h + pad)),\n                            (0, 255, 0), 2)\n            \n            # Add text overlay in corner\n            overlay_text = f\"Object {obj_id} | {obj['classification']} | {obj['confidence']*100:.0f}% | {obj['avg_speed']:.1f} px/frame\"\n            \n            # Create background for text\n            text_size = cv2.getTextSize(overlay_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n            cv2.rectangle(frame, (10, 10), (20 + text_size[0], 40 + text_size[1]), (0, 0, 0), -1)\n            \n            # Draw text\n            cv2.putText(frame, overlay_text, (15, 35),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n            \n            out.write(frame)\n    \n    cap.release()\n    out.release()\n    \n    return output_path\n\ndef reset_session():\n    \"\"\"Reset session state for new analysis\"\"\"\n    st.session_state.processing_complete = False\n    st.session_state.results_data = None\n    st.session_state.processed_clips = []\n    st.session_state.metadata = []\n    st.session_state.video_info = {}\n    st.session_state.recommendations = None\n    st.session_state.uploaded_video_path = None\n    st.session_state.prepared_zips = {}\n    \n    # Clean up directories\n    for directory in ['temp_uploads', 'processed_clips', 'results']:\n        if os.path.exists(directory):\n            shutil.rmtree(directory)\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":57977},"db_service.py":{"content":"from datetime import datetime\nfrom db_models import AnalysisSession, DetectionClip, ObjectDetection, init_database, get_session\nimport pandas as pd\n\n\nclass DatabaseService:\n    \"\"\"Service layer for database operations\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize database and create tables if they don't exist\"\"\"\n        try:\n            session, engine = init_database()\n            session.close()\n            engine.dispose()\n        except Exception as e:\n            print(f\"Warning: Could not initialize database: {e}\")\n    \n    def save_analysis_session(self, video_info, results_df, clips):\n        \"\"\"\n        Save complete analysis session to database\n        \n        Args:\n            video_info (dict): Video metadata (filename, duration, fps, resolution)\n            results_df (pd.DataFrame): Classification results dataframe\n            clips (list): List of clip file paths\n            \n        Returns:\n            int: Session ID\n        \"\"\"\n        session = None\n        try:\n            session = get_session()\n            \n            # Count classifications\n            classification_counts = results_df['classification'].value_counts().to_dict()\n            \n            # Create analysis session with safe type conversions\n            video_fps = video_info.get('fps_numeric', 30)\n            if isinstance(video_fps, str) or video_fps is None:\n                video_fps = 30  # Default if not available\n            \n            video_duration = video_info.get('duration_seconds', 0)\n            if isinstance(video_duration, str) or video_duration is None:\n                video_duration = 0\n            \n            analysis_session = AnalysisSession(\n                video_filename=video_info.get('filename', 'unknown.mp4'),\n                video_duration=float(video_duration),\n                video_fps=float(video_fps),\n                video_resolution=video_info.get('resolution', 'unknown'),\n                total_detections=len(results_df),\n                satellites_count=classification_counts.get('Satellite', 0),\n                meteors_count=classification_counts.get('Meteor', 0),\n                planes_count=classification_counts.get('Plane', 0),\n                anomalies_count=classification_counts.get('ANOMALY_UAP', 0),\n                junk_count=classification_counts.get('Junk', 0)\n            )\n            \n            session.add(analysis_session)\n            session.flush()  # Get the session ID\n            \n            # Save each detection clip\n            for idx, row in results_df.iterrows():\n                clip_id = row['clip_id']\n                \n                # Find corresponding clip file\n                clip_filename = None\n                for clip_path in clips:\n                    if f\"clip_{clip_id:04d}\" in clip_path:\n                        clip_filename = clip_path\n                        break\n                \n                # Safe conversion helper for handling NaN/None values\n                def safe_float(val, default=0.0):\n                    try:\n                        if pd.isna(val):\n                            return default\n                        return float(val)\n                    except:\n                        return default\n                \n                def safe_int(val, default=0):\n                    try:\n                        if pd.isna(val):\n                            return default\n                        return int(val)\n                    except:\n                        return default\n                \n                detection_clip = DetectionClip(\n                    session_id=analysis_session.id,\n                    clip_id=clip_id,\n                    clip_filename=clip_filename,\n                    classification=str(row.get('classification', 'Unknown')),\n                    confidence=safe_float(row.get('confidence'), 0),\n                    anomaly_score=safe_float(row.get('anomaly_score'), 0),\n                    is_anomaly=bool(row.get('is_anomaly', False)),\n                    duration=safe_float(row.get('duration'), 0),\n                    avg_speed=safe_float(row.get('avg_speed'), 0),\n                    max_speed=safe_float(row.get('max_speed'), 0),\n                    speed_consistency=safe_float(row.get('speed_consistency'), 0),\n                    linearity=safe_float(row.get('linearity'), 0),\n                    direction_changes=safe_int(row.get('direction_changes'), 0),\n                    avg_area=safe_float(row.get('avg_area'), 0),\n                    max_area=safe_float(row.get('max_area'), 0),\n                    size_consistency=safe_float(row.get('size_consistency'), 0),\n                    avg_aspect_ratio=safe_float(row.get('avg_aspect_ratio'), 1.0),\n                    detection_count=safe_int(row.get('detection_count'), 0)\n                )\n                \n                session.add(detection_clip)\n            \n            session.commit()\n            session_id = analysis_session.id\n            session.close()\n            \n            return session_id\n            \n        except Exception as e:\n            print(f\"Error saving analysis session: {e}\")\n            if session:\n                session.rollback()\n                session.close()\n            return None\n    \n    def get_all_sessions(self):\n        \"\"\"Get all analysis sessions ordered by date (newest first)\"\"\"\n        session = None\n        try:\n            session = get_session()\n            sessions = session.query(AnalysisSession).order_by(AnalysisSession.processing_date.desc()).all()\n            session.close()\n            return sessions\n        except Exception as e:\n            print(f\"Error retrieving sessions: {e}\")\n            return []\n    \n    def get_session_by_id(self, session_id):\n        \"\"\"Get specific analysis session with all clips\"\"\"\n        session = None\n        try:\n            session = get_session()\n            analysis_session = session.query(AnalysisSession).filter_by(id=session_id).first()\n            session.close()\n            return analysis_session\n        except Exception as e:\n            print(f\"Error retrieving session: {e}\")\n            return None\n    \n    def get_all_detections(self, classification_filter=None, min_confidence=0.0):\n        \"\"\"\n        Get all detection clips with optional filters\n        \n        Args:\n            classification_filter (list): List of classifications to include (e.g., ['Satellite', 'ANOMALY_UAP'])\n            min_confidence (float): Minimum confidence threshold\n            \n        Returns:\n            list: List of DetectionClip objects\n        \"\"\"\n        session = None\n        try:\n            session = get_session()\n            query = session.query(DetectionClip).filter(DetectionClip.confidence >= min_confidence)\n            \n            if classification_filter:\n                query = query.filter(DetectionClip.classification.in_(classification_filter))\n            \n            clips = query.order_by(DetectionClip.detected_at.desc()).all()\n            session.close()\n            return clips\n        except Exception as e:\n            print(f\"Error retrieving detections: {e}\")\n            return []\n    \n    def get_statistics(self, days=30):\n        \"\"\"\n        Get detection statistics for the last N days\n        \n        Args:\n            days (int): Number of days to look back\n            \n        Returns:\n            dict: Statistics dictionary\n        \"\"\"\n        session = None\n        try:\n            session = get_session()\n            \n            # Get sessions from last N days\n            from datetime import timedelta\n            cutoff_date = datetime.utcnow() - timedelta(days=days)\n            sessions = session.query(AnalysisSession).filter(\n                AnalysisSession.processing_date >= cutoff_date\n            ).all()\n            \n            # Calculate statistics\n            total_sessions = len(sessions)\n            total_detections = sum(s.total_detections for s in sessions)\n            total_satellites = sum(s.satellites_count for s in sessions)\n            total_meteors = sum(s.meteors_count for s in sessions)\n            total_planes = sum(s.planes_count for s in sessions)\n            total_anomalies = sum(s.anomalies_count for s in sessions)\n            total_junk = sum(s.junk_count for s in sessions)\n            \n            stats = {\n                'total_sessions': total_sessions,\n                'total_detections': total_detections,\n                'satellites': total_satellites,\n                'meteors': total_meteors,\n                'planes': total_planes,\n                'anomalies': total_anomalies,\n                'junk': total_junk,\n                'days': days\n            }\n            \n            session.close()\n            return stats\n            \n        except Exception as e:\n            print(f\"Error calculating statistics: {e}\")\n            return {}\n    \n    def delete_session(self, session_id):\n        \"\"\"Delete an analysis session and all its clips\"\"\"\n        session = None\n        try:\n            session = get_session()\n            analysis_session = session.query(AnalysisSession).filter_by(id=session_id).first()\n            \n            if analysis_session:\n                session.delete(analysis_session)\n                session.commit()\n                session.close()\n                return True\n            \n            session.close()\n            return False\n            \n        except Exception as e:\n            print(f\"Error deleting session: {e}\")\n            if session:\n                session.rollback()\n                session.close()\n            return False\n","size_bytes":9642},"trajectory_visualizer.py":{"content":"import plotly.graph_objects as go\nimport plotly.express as px\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport math\n\n\nclass TrajectoryVisualizer:\n    \"\"\"Visualize object trajectories on interactive star charts\"\"\"\n    \n    def __init__(self, frame_width=1920, frame_height=1080):\n        \"\"\"\n        Initialize trajectory visualizer\n        \n        Args:\n            frame_width (int): Video frame width\n            frame_height (int): Video frame height\n        \"\"\"\n        self.frame_width = frame_width\n        self.frame_height = frame_height\n    \n    def create_trajectory_plot(self, metadata, classification='All'):\n        \"\"\"\n        Create interactive trajectory plot showing object paths\n        \n        Args:\n            metadata (list): List of detection metadata dictionaries\n            classification (str): Filter by classification or 'All'\n            \n        Returns:\n            plotly.graph_objects.Figure: Interactive trajectory plot\n        \"\"\"\n        if not metadata:\n            return self._create_empty_plot()\n        \n        # Group by clip_id\n        clips_data = {}\n        for item in metadata:\n            clip_id = item.get('clip_id', -1)\n            if clip_id not in clips_data:\n                clips_data[clip_id] = []\n            clips_data[clip_id].append(item)\n        \n        # Create figure with dark background (star field)\n        fig = go.Figure()\n        \n        # Add starfield background effect\n        fig.update_layout(\n            plot_bgcolor='#0a0e27',\n            paper_bgcolor='#0a0e27',\n            font=dict(color='white'),\n            xaxis=dict(\n                range=[0, self.frame_width],\n                showgrid=False,\n                zeroline=False,\n                title=\"X Position (pixels)\"\n            ),\n            yaxis=dict(\n                range=[self.frame_height, 0],  # Inverted for image coordinates\n                showgrid=False,\n                zeroline=False,\n                title=\"Y Position (pixels)\",\n                scaleanchor=\"x\",\n                scaleratio=1\n            ),\n            title=\"Object Trajectory Visualization\",\n            hovermode='closest',\n            height=600\n        )\n        \n        # Color mapping for classifications\n        color_map = {\n            'Satellite': '#1f77b4',  # Blue\n            'Meteor': '#ff7f0e',     # Orange\n            'Plane': '#2ca02c',      # Green\n            'Junk': '#d62728',       # Red\n            'ANOMALY_UAP': '#9467bd' # Purple\n        }\n        \n        # Plot each trajectory\n        for clip_id, detections in clips_data.items():\n            if len(detections) < 2:\n                continue\n            \n            # Sort by frame number\n            detections.sort(key=lambda x: x.get('frame_number', 0))\n            \n            # Extract positions\n            x_coords = [d.get('centroid_x', 0) for d in detections]\n            y_coords = [d.get('centroid_y', 0) for d in detections]\n            frames = [d.get('frame_number', 0) for d in detections]\n            \n            # Determine color based on classification (if available)\n            clip_class = detections[0].get('classification', 'Unknown')\n            color = color_map.get(clip_class, '#808080')\n            \n            # Calculate trajectory metrics for hover info\n            distances = []\n            for i in range(1, len(x_coords)):\n                dx = x_coords[i] - x_coords[i-1]\n                dy = y_coords[i] - y_coords[i-1]\n                dist = math.sqrt(dx*dx + dy*dy)\n                distances.append(dist)\n            \n            avg_speed = np.mean(distances) if distances else 0\n            \n            # Add trajectory line\n            fig.add_trace(go.Scatter(\n                x=x_coords,\n                y=y_coords,\n                mode='lines+markers',\n                name=f'Clip {clip_id} ({clip_class})',\n                line=dict(color=color, width=2),\n                marker=dict(\n                    size=6,\n                    color=color,\n                    symbol='circle',\n                    line=dict(width=1, color='white')\n                ),\n                hovertemplate=(\n                    f'<b>Clip {clip_id}</b><br>' +\n                    'Position: (%{x:.0f}, %{y:.0f})<br>' +\n                    f'Classification: {clip_class}<br>' +\n                    f'Avg Speed: {avg_speed:.1f} px/frame<br>' +\n                    '<extra></extra>'\n                )\n            ))\n            \n            # Add start and end markers\n            fig.add_trace(go.Scatter(\n                x=[x_coords[0]],\n                y=[y_coords[0]],\n                mode='markers',\n                marker=dict(\n                    size=12,\n                    color='lime',\n                    symbol='star',\n                    line=dict(width=2, color='white')\n                ),\n                name=f'Start {clip_id}',\n                hovertemplate=f'<b>Start</b><br>Frame: {frames[0]}<extra></extra>',\n                showlegend=False\n            ))\n            \n            fig.add_trace(go.Scatter(\n                x=[x_coords[-1]],\n                y=[y_coords[-1]],\n                mode='markers',\n                marker=dict(\n                    size=12,\n                    color='red',\n                    symbol='x',\n                    line=dict(width=2, color='white')\n                ),\n                name=f'End {clip_id}',\n                hovertemplate=f'<b>End</b><br>Frame: {frames[-1]}<extra></extra>',\n                showlegend=False\n            ))\n        \n        return fig\n    \n    def create_speed_heatmap(self, metadata):\n        \"\"\"\n        Create heatmap showing speed distribution across the frame\n        \n        Args:\n            metadata (list): List of detection metadata\n            \n        Returns:\n            plotly.graph_objects.Figure: Speed heatmap\n        \"\"\"\n        if not metadata:\n            return self._create_empty_plot()\n        \n        # Create grid for heatmap\n        grid_size = 50\n        x_bins = np.linspace(0, self.frame_width, grid_size)\n        y_bins = np.linspace(0, self.frame_height, grid_size)\n        \n        speed_grid = np.zeros((grid_size-1, grid_size-1))\n        count_grid = np.zeros((grid_size-1, grid_size-1))\n        \n        # Group by clip and calculate speeds\n        clips_data = {}\n        for item in metadata:\n            clip_id = item.get('clip_id', -1)\n            if clip_id not in clips_data:\n                clips_data[clip_id] = []\n            clips_data[clip_id].append(item)\n        \n        # Calculate speeds and bin them\n        for clip_id, detections in clips_data.items():\n            detections.sort(key=lambda x: x.get('frame_number', 0))\n            \n            for i in range(1, len(detections)):\n                x1, y1 = detections[i-1].get('centroid_x', 0), detections[i-1].get('centroid_y', 0)\n                x2, y2 = detections[i].get('centroid_x', 0), detections[i].get('centroid_y', 0)\n                \n                speed = math.sqrt((x2-x1)**2 + (y2-y1)**2)\n                \n                # Find bin indices\n                x_idx = np.digitize(x2, x_bins) - 1\n                y_idx = np.digitize(y2, y_bins) - 1\n                \n                if 0 <= x_idx < grid_size-1 and 0 <= y_idx < grid_size-1:\n                    speed_grid[y_idx, x_idx] += speed\n                    count_grid[y_idx, x_idx] += 1\n        \n        # Average speeds\n        with np.errstate(divide='ignore', invalid='ignore'):\n            avg_speed_grid = np.where(count_grid > 0, speed_grid / count_grid, 0)\n        \n        fig = go.Figure(data=go.Heatmap(\n            z=avg_speed_grid,\n            x=x_bins[:-1],\n            y=y_bins[:-1],\n            colorscale='Viridis',\n            colorbar=dict(title=\"Avg Speed<br>(px/frame)\")\n        ))\n        \n        fig.update_layout(\n            title=\"Speed Distribution Heatmap\",\n            xaxis_title=\"X Position\",\n            yaxis_title=\"Y Position\",\n            height=500\n        )\n        \n        return fig\n    \n    def create_direction_plot(self, metadata):\n        \"\"\"\n        Create polar plot showing movement directions\n        \n        Args:\n            metadata (list): List of detection metadata\n            \n        Returns:\n            plotly.graph_objects.Figure: Direction distribution plot\n        \"\"\"\n        if not metadata:\n            return self._create_empty_plot()\n        \n        # Group by clip\n        clips_data = {}\n        for item in metadata:\n            clip_id = item.get('clip_id', -1)\n            if clip_id not in clips_data:\n                clips_data[clip_id] = []\n            clips_data[clip_id].append(item)\n        \n        # Calculate directions\n        directions = []\n        speeds = []\n        \n        for clip_id, detections in clips_data.items():\n            detections.sort(key=lambda x: x.get('frame_number', 0))\n            \n            if len(detections) < 2:\n                continue\n            \n            # Overall direction from start to end\n            x1, y1 = detections[0].get('centroid_x', 0), detections[0].get('centroid_y', 0)\n            x2, y2 = detections[-1].get('centroid_x', 0), detections[-1].get('centroid_y', 0)\n            \n            dx, dy = x2 - x1, y2 - y1\n            angle = math.degrees(math.atan2(dy, dx))\n            if angle < 0:\n                angle += 360\n            \n            speed = math.sqrt(dx*dx + dy*dy) / len(detections)\n            \n            directions.append(angle)\n            speeds.append(speed)\n        \n        # Create polar histogram\n        fig = go.Figure()\n        \n        fig.add_trace(go.Scatterpolar(\n            r=speeds,\n            theta=directions,\n            mode='markers',\n            marker=dict(\n                size=10,\n                color=speeds,\n                colorscale='Plasma',\n                showscale=True,\n                colorbar=dict(title=\"Speed\")\n            ),\n            hovertemplate='Direction: %{theta:.0f}¬∞<br>Speed: %{r:.1f}<extra></extra>'\n        ))\n        \n        fig.update_layout(\n            polar=dict(\n                radialaxis=dict(title=\"Speed (px/frame)\"),\n                angularaxis=dict(direction=\"clockwise\")\n            ),\n            title=\"Movement Direction Distribution\",\n            height=500\n        )\n        \n        return fig\n    \n    def create_timeline_plot(self, metadata, fps=30):\n        \"\"\"\n        Create timeline plot showing when objects were detected\n        \n        Args:\n            metadata (list): List of detection metadata\n            fps (float): Video frame rate\n            \n        Returns:\n            plotly.graph_objects.Figure: Timeline plot\n        \"\"\"\n        if not metadata:\n            return self._create_empty_plot()\n        \n        # Convert fps to float if it's a string\n        try:\n            fps = float(fps)\n        except (ValueError, TypeError):\n            fps = 30.0\n        \n        # Group by clip\n        clips_data = {}\n        for item in metadata:\n            clip_id = item.get('clip_id', -1)\n            if clip_id not in clips_data:\n                clips_data[clip_id] = []\n            clips_data[clip_id].append(item)\n        \n        # Prepare timeline data\n        timeline_data = []\n        \n        for clip_id, detections in clips_data.items():\n            detections.sort(key=lambda x: x.get('frame_number', 0))\n            \n            start_frame = detections[0].get('frame_number', 0)\n            end_frame = detections[-1].get('frame_number', 0)\n            \n            start_time = start_frame / fps\n            end_time = end_frame / fps\n            duration = end_time - start_time\n            \n            classification = detections[0].get('classification', 'Unknown')\n            \n            timeline_data.append({\n                'clip_id': clip_id,\n                'start': start_time,\n                'end': end_time,\n                'duration': duration,\n                'classification': classification\n            })\n        \n        df = pd.DataFrame(timeline_data)\n        \n        # Color mapping\n        color_map = {\n            'Satellite': '#1f77b4',\n            'Meteor': '#ff7f0e',\n            'Plane': '#2ca02c',\n            'Junk': '#d62728',\n            'ANOMALY_UAP': '#9467bd'\n        }\n        \n        fig = go.Figure()\n        \n        for _, row in df.iterrows():\n            color = color_map.get(row['classification'], '#808080')\n            \n            fig.add_trace(go.Scatter(\n                x=[row['start'], row['end']],\n                y=[row['clip_id'], row['clip_id']],\n                mode='lines+markers',\n                line=dict(color=color, width=10),\n                marker=dict(size=8, color=color),\n                name=f\"Clip {row['clip_id']}\",\n                hovertemplate=(\n                    f\"<b>Clip {row['clip_id']}</b><br>\" +\n                    f\"Classification: {row['classification']}<br>\" +\n                    f\"Start: {row['start']:.2f}s<br>\" +\n                    f\"Duration: {row['duration']:.2f}s<br>\" +\n                    \"<extra></extra>\"\n                ),\n                showlegend=False\n            ))\n        \n        fig.update_layout(\n            title=\"Detection Timeline\",\n            xaxis_title=\"Time (seconds)\",\n            yaxis_title=\"Clip ID\",\n            height=400,\n            hovermode='closest'\n        )\n        \n        return fig\n    \n    def _create_empty_plot(self):\n        \"\"\"Create empty plot when no data available\"\"\"\n        fig = go.Figure()\n        fig.add_annotation(\n            text=\"No trajectory data available\",\n            xref=\"paper\",\n            yref=\"paper\",\n            x=0.5,\n            y=0.5,\n            showarrow=False,\n            font=dict(size=20)\n        )\n        fig.update_layout(\n            xaxis=dict(visible=False),\n            yaxis=dict(visible=False),\n            height=400\n        )\n        return fig\n","size_bytes":13932},"utils.py":{"content":"import os\nimport cv2\nimport zipfile\nfrom io import BytesIO\nimport pandas as pd\nimport numpy as np\nimport tempfile\nimport shutil\n\ndef get_video_info(uploaded_file):\n    \"\"\"\n    Extract basic information from uploaded video file\n    \n    Args:\n        uploaded_file: Streamlit uploaded file object\n        \n    Returns:\n        dict: Video information dictionary\n    \"\"\"\n    try:\n        # Create temporary file\n        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:\n            tmp_file.write(uploaded_file.getbuffer())\n            tmp_path = tmp_file.name\n        \n        # Open video with OpenCV\n        cap = cv2.VideoCapture(tmp_path)\n        \n        if not cap.isOpened():\n            os.unlink(tmp_path)\n            return {\"error\": \"Could not read video file\"}\n        \n        # Extract video properties\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        \n        # Calculate duration\n        duration_seconds = frame_count / fps if fps > 0 else 0\n        duration_formatted = format_duration(duration_seconds)\n        \n        # Get file size\n        file_size_bytes = len(uploaded_file.getbuffer())\n        file_size_formatted = format_file_size(file_size_bytes)\n        \n        # Calculate bitrate estimate\n        bitrate_kbps = (file_size_bytes * 8) / (duration_seconds * 1000) if duration_seconds > 0 else 0\n        bitrate_formatted = f\"{bitrate_kbps:.1f} kbps\" if bitrate_kbps > 0 else \"Unknown\"\n        \n        cap.release()\n        os.unlink(tmp_path)\n        \n        return {\n            'duration': duration_formatted,\n            'duration_seconds': duration_seconds,\n            'fps': f\"{fps:.1f}\" if fps > 0 else \"Unknown\",\n            'fps_numeric': fps,\n            'resolution': f\"{width}x{height}\",\n            'file_size': file_size_formatted,\n            'format': uploaded_file.name.split('.')[-1].upper(),\n            'bitrate': bitrate_formatted,\n            'filename': uploaded_file.name\n        }\n        \n    except Exception as e:\n        return {\"error\": f\"Error reading video: {str(e)}\"}\n\ndef format_duration(seconds):\n    \"\"\"Format duration in seconds to human readable format\"\"\"\n    if seconds < 60:\n        return f\"{seconds:.1f}s\"\n    elif seconds < 3600:\n        minutes = int(seconds // 60)\n        remaining_seconds = int(seconds % 60)\n        return f\"{minutes}m {remaining_seconds}s\"\n    else:\n        hours = int(seconds // 3600)\n        minutes = int((seconds % 3600) // 60)\n        return f\"{hours}h {minutes}m\"\n\ndef format_file_size(bytes_size):\n    \"\"\"Format file size in bytes to human readable format\"\"\"\n    for unit in ['B', 'KB', 'MB', 'GB']:\n        if bytes_size < 1024.0:\n            return f\"{bytes_size:.1f} {unit}\"\n        bytes_size /= 1024.0\n    return f\"{bytes_size:.1f} TB\"\n\ndef get_object_time_ranges(metadata):\n    \"\"\"\n    Extract time ranges (start/end frames) for each object from metadata\n    \n    Args:\n        metadata (list): List of detection dictionaries with frame_number and clip_id\n        \n    Returns:\n        dict: {clip_id: {'start_frame': int, 'end_frame': int, 'fps': float}}\n    \"\"\"\n    from collections import defaultdict\n    \n    object_frames = defaultdict(list)\n    object_fps = {}\n    \n    # Group frame numbers by clip_id\n    for detection in metadata:\n        clip_id = detection['clip_id']\n        frame_num = detection['frame_number']\n        fps = detection.get('fps', 30)\n        \n        object_frames[clip_id].append(frame_num)\n        object_fps[clip_id] = fps\n    \n    # Calculate start/end frames for each object\n    time_ranges = {}\n    for clip_id, frames in object_frames.items():\n        time_ranges[clip_id] = {\n            'start_frame': min(frames),\n            'end_frame': max(frames),\n            'fps': object_fps[clip_id]\n        }\n    \n    return time_ranges\n\ndef extract_video_segment(source_video_path, start_frame, end_frame, output_path, padding_frames=60):\n    \"\"\"\n    Extract a segment from a video file with padding\n    \n    Args:\n        source_video_path (str): Path to source video\n        start_frame (int): Starting frame number\n        end_frame (int): Ending frame number\n        output_path (str): Path for output video\n        padding_frames (int): Number of frames to add before/after (default 60 = ~2 seconds at 30fps)\n        \n    Returns:\n        bool: True if successful, False otherwise\n    \"\"\"\n    import cv2\n    \n    try:\n        # Open source video\n        cap = cv2.VideoCapture(source_video_path)\n        if not cap.isOpened():\n            print(f\"Warning: Could not open video file: {source_video_path}\")\n            return False\n        \n        # Get video properties\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        \n        # Guard against invalid video properties\n        if fps <= 0 or width <= 0 or height <= 0 or total_frames <= 0:\n            print(f\"Warning: Invalid video properties - fps:{fps}, size:{width}x{height}, frames:{total_frames}\")\n            cap.release()\n            return False\n        \n        # Add padding while staying within video bounds\n        padded_start = max(0, start_frame - padding_frames)\n        padded_end = min(total_frames - 1, end_frame + padding_frames)\n        \n        # Create video writer\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n        \n        if not out.isOpened():\n            print(f\"Warning: Could not create video writer for: {output_path}\")\n            cap.release()\n            return False\n        \n        # Set to padded start frame\n        cap.set(cv2.CAP_PROP_POS_FRAMES, padded_start)\n        \n        # Extract frames\n        current_frame = padded_start\n        frames_written = 0\n        while current_frame <= padded_end:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            out.write(frame)\n            frames_written += 1\n            current_frame += 1\n        \n        # Clean up\n        cap.release()\n        out.release()\n        \n        if frames_written == 0:\n            print(f\"Warning: No frames extracted for segment {start_frame}-{end_frame}\")\n            return False\n        \n        return True\n        \n    except Exception as e:\n        print(f\"Error extracting segment: {e}\")\n        return False\n\ndef create_classification_video(source_video, metadata, results_df, classification_filter, output_path):\n    \"\"\"\n    Create a single video containing only frames with detections of a specific classification\n    \n    Args:\n        source_video (str): Path to source combined video\n        metadata (list): Detection metadata\n        results_df (pd.DataFrame): Results dataframe\n        classification_filter (str): Classification to extract\n        output_path (str): Output video path\n        \n    Returns:\n        bool: True if successful, False otherwise\n    \"\"\"\n    import cv2\n    \n    try:\n        # Filter results and metadata to only include the specified classification\n        filtered_df = results_df[results_df['classification'] == classification_filter]\n        if filtered_df.empty:\n            return False\n        \n        # Get clip IDs for this classification\n        target_clip_ids = set(filtered_df['clip_id'].values)\n        \n        # Get all frame numbers where these objects appear\n        frames_with_detections = set()\n        for detection in metadata:\n            if detection['clip_id'] in target_clip_ids:\n                frames_with_detections.add(detection['frame_number'])\n        \n        if not frames_with_detections:\n            return False\n        \n        # Open source video\n        cap = cv2.VideoCapture(source_video)\n        if not cap.isOpened():\n            return False\n        \n        # Get video properties\n        source_fps = cap.get(cv2.CAP_PROP_FPS)\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        \n        if source_fps <= 0 or width <= 0 or height <= 0:\n            cap.release()\n            return False\n        \n        # Create video writer\n        # Use source FPS to preserve timing - source is already at 10x speed\n        # Extracting frames maintains the same playback rate per frame\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        out = cv2.VideoWriter(output_path, fourcc, source_fps, (width, height))\n        \n        if not out.isOpened():\n            cap.release()\n            return False\n        \n        # Extract frames with detections\n        sorted_frames = sorted(frames_with_detections)\n        current_pos = 0\n        frames_written = 0\n        \n        for frame_num in sorted_frames:\n            # Seek to frame if needed\n            if frame_num != current_pos:\n                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n            \n            ret, frame = cap.read()\n            if ret:\n                out.write(frame)\n                frames_written += 1\n            current_pos = frame_num + 1\n        \n        cap.release()\n        out.release()\n        \n        return frames_written > 0\n        \n    except Exception as e:\n        print(f\"Error creating classification video: {e}\")\n        return False\n\ndef _create_filtered_video(source_video, output_path, results_df, metadata, classification_filter):\n    \"\"\"\n    Create a video showing ONLY rectangles for a specific classification\n    \n    Args:\n        source_video (str): Path to source video (without any rectangles)\n        output_path (str): Path for output video\n        results_df (pd.DataFrame): Filtered results for this classification only\n        metadata (list): All detection metadata\n        classification_filter (str): Classification to show (e.g., 'Satellite')\n    \"\"\"\n    # Color mapping\n    color_map = {\n        'Satellite': (0, 0, 255),       # Red (BGR format)\n        'Meteor': (0, 255, 255),        # Yellow (BGR format)\n        'Plane': (255, 0, 0),           # Blue\n        'Junk': (128, 128, 128)         # Gray\n    }\n    \n    # Get the color for this classification\n    color = color_map.get(classification_filter, (255, 255, 255))\n    \n    # Create lookup of clip_ids for this classification with their actual classification\n    classification_lookup = {}\n    for idx, row in results_df.iterrows():\n        classification_lookup[row['clip_id']] = row['classification']\n    \n    filtered_clip_ids = set(results_df['clip_id'].tolist())\n    \n    # Group metadata by output_frame_number, but ONLY for filtered clip_ids\n    from collections import defaultdict\n    frame_detections = defaultdict(list)\n    for item in metadata:\n        clip_id = item['clip_id']\n        # Double-check: must be in filtered set AND have matching classification\n        if clip_id in filtered_clip_ids and classification_lookup.get(clip_id) == classification_filter:\n            frame_num = item.get('output_frame_number', item['frame_number'])\n            frame_detections[frame_num].append({\n                'clip_id': clip_id,\n                'bbox_x': item['bbox_x'],\n                'bbox_y': item['bbox_y'],\n                'bbox_width': item['bbox_width'],\n                'bbox_height': item['bbox_height']\n            })\n    \n    # Open source video (this is the ORIGINAL without any rectangles drawn yet)\n    cap = cv2.VideoCapture(source_video)\n    if not cap.isOpened():\n        return\n    \n    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    # Create writer\n    writer = cv2.VideoWriter(\n        output_path,\n        cv2.VideoWriter_fourcc(*'mp4v'),\n        fps,\n        (frame_width, frame_height)\n    )\n    \n    frame_num = -1\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        frame_num += 1  # 0-based frame numbering\n        \n        # Draw rectangles ONLY for this classification's objects\n        if frame_num in frame_detections:\n            for detection in frame_detections[frame_num]:\n                clip_id = detection['clip_id']\n                x = detection['bbox_x']\n                y = detection['bbox_y']\n                w = detection['bbox_width']\n                h = detection['bbox_height']\n                \n                pad = 8\n                cv2.rectangle(frame,\n                            (max(0, x-pad), max(0, y-pad)),\n                            (min(frame_width-1, x+w+pad), min(frame_height-1, y+h+pad)),\n                            color, 2)\n                \n                # Add label with object ID and classification\n                label = f\"ID:{clip_id} {classification_filter}\"\n                cv2.putText(frame, label, (x, y-10),\n                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n        \n        writer.write(frame)\n    \n    cap.release()\n    writer.release()\n\ndef create_download_zip(clip_paths, results_df, classification_filter=None, metadata=None):\n    \"\"\"\n    Create a ZIP file containing classified clips\n    \n    Args:\n        clip_paths (list): List of paths to video clips (should be single combined video)\n        results_df (pd.DataFrame): Results dataframe with classifications\n        classification_filter (str, optional): Only include clips of this classification\n        metadata (list, optional): Detection metadata for extracting time ranges\n        \n    Returns:\n        BytesIO: ZIP file buffer\n    \"\"\"\n    import tempfile\n    \n    zip_buffer = BytesIO()\n    \n    # Filter results if classification specified\n    if classification_filter:\n        filtered_results = results_df[results_df['classification'] == classification_filter]\n    else:\n        filtered_results = results_df\n    \n    # Check if we have the combined video and metadata for creating classification video\n    has_combined_video = clip_paths and len(clip_paths) == 1 and os.path.exists(clip_paths[0])\n    \n    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:\n        # Create classification-specific video showing ONLY that category's objects\n        if has_combined_video and metadata:\n            annotated_video = clip_paths[0]\n            # Use clean version (saved before rectangles were added)\n            clean_video = annotated_video.replace('.mp4', '_clean.mp4')\n            \n            # Fallback to annotated if clean doesn't exist (shouldn't happen)\n            source_video = clean_video if os.path.exists(clean_video) else annotated_video\n            \n            safe_classification = classification_filter.replace('/', '_') if classification_filter else \"all\"\n            video_filename = f\"{safe_classification}_detections.mp4\"\n            \n            # Create temporary filtered video\n            temp_video = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')\n            temp_video.close()\n            \n            # Generate video with ONLY the filtered classification's rectangles\n            _create_filtered_video(source_video, temp_video.name, filtered_results, metadata, classification_filter)\n            \n            # Add filtered video to zip\n            zip_file.write(temp_video.name, video_filename)\n            \n            # Clean up temp file\n            try:\n                os.remove(temp_video.name)\n            except:\n                pass\n        \n        # Add CSV report (use filtered results)\n        csv_buffer = BytesIO()\n        filtered_results.to_csv(csv_buffer, index=False)\n        csv_buffer.seek(0)\n        zip_file.writestr(\"analysis_report.csv\", csv_buffer.getvalue())\n        \n        # Generate detection summary\n        if classification_filter:\n            summary_lines = [f\"SkySeer AI - {classification_filter} Detection Summary\", \"=\" * 50, \"\"]\n            summary_lines.append(f\"This archive contains only {classification_filter} detections.\")\n        else:\n            summary_lines = [\"SkySeer AI - Detection Summary\", \"=\" * 50, \"\"]\n        \n        summary_lines.append(\"\")\n        \n        # Count detections by classification (use filtered results)\n        classification_counts = filtered_results['classification'].value_counts().to_dict()\n        summary_lines.append(\"DETECTION COUNTS:\")\n        for classification, count in sorted(classification_counts.items()):\n            summary_lines.append(f\"  {classification}: {count}\")\n        \n        summary_lines.append(\"\")\n        summary_lines.append(\"DETAILED DETECTIONS:\")\n        summary_lines.append(\"-\" * 50)\n        \n        # Add details for each detection (use filtered results)\n        for idx, row in filtered_results.iterrows():\n            clip_id = row['clip_id']\n            classification = row['classification']\n            confidence = row['confidence']\n            duration = row.get('duration', 0)\n            avg_speed = row.get('avg_speed', 0)\n            \n            summary_lines.append(f\"\\nObject ID: {clip_id}\")\n            summary_lines.append(f\"  Classification: {classification}\")\n            summary_lines.append(f\"  Confidence: {confidence:.1%}\")\n            summary_lines.append(f\"  Duration: {duration:.2f}s\")\n            summary_lines.append(f\"  Average Speed: {avg_speed:.1f} px/frame\")\n        \n        summary_lines.append(\"\")\n        summary_lines.append(\"=\" * 50)\n        summary_lines.append(\"Analysis complete. See analysis_report.csv for full details.\")\n        \n        summary_content = \"\\n\".join(summary_lines)\n        zip_file.writestr(\"SUMMARY.txt\", summary_content)\n        \n        # Add README file\n        if classification_filter:\n            readme_content = f\"\"\"SkySeer AI - {classification_filter} Detection Results\n=====================================\n\nThis ZIP file focuses on {classification_filter} detections from your analysis.\n\nFiles Included:\n- {classification_filter}_detections.mp4: Sped-up video (10x speed) showing ONLY {classification_filter} objects\n- analysis_report.csv: Detailed data for {classification_filter} objects only\n- SUMMARY.txt: Quick overview of {classification_filter} detections\n\nVideo Format:\n- Filtered video at 10x speed showing ONLY {classification_filter} detections\n- Color-coded bounding boxes: Red=Satellites, Yellow=Meteors\n- Labels show object ID and classification (e.g., \"ID:3 {classification_filter}\")\n- Video duration = 1/10th of your original upload duration\n- Other classifications are NOT shown in this category-specific video\n\nCSV Report:\nContains detailed metrics for {classification_filter} objects only:\n- clip_id: Unique object identifier (matches ID shown in video)\n- classification: Object type ({classification_filter})\n- confidence: AI confidence score (0.0-1.0)\n- duration: How long the object was visible\n- avg_speed: Average movement speed in pixels/frame\n- And more detailed tracking data\n\nConfidence Scores:\n- 0.9+ : High confidence\n- 0.7-0.9 : Medium-high confidence  \n- 0.5-0.7 : Medium confidence\n- <0.5 : Low confidence\n\nFor questions about this analysis, please refer to the SkySeer documentation.\n\"\"\"\n        else:\n            readme_content = \"\"\"SkySeer AI Pipeline - Analysis Results\n=====================================\n\nThis ZIP file contains the results of your sky object detection analysis.\n\nFiles:\n- SUMMARY.txt: Quick overview of all detections\n- analysis_report.csv: Detailed analysis data with confidence scores\n- Detection videos showing identified objects\n\nConfidence Scores:\n- 0.9+ : High confidence\n- 0.7-0.9 : Medium-high confidence  \n- 0.5-0.7 : Medium confidence\n- <0.5 : Low confidence\n\nFor questions about this analysis, please refer to the SkySeer documentation.\n\"\"\"\n        zip_file.writestr(\"README.txt\", readme_content)\n    \n    zip_buffer.seek(0)\n    return zip_buffer\n\ndef cleanup_temp_files():\n    \"\"\"Clean up temporary files and directories\"\"\"\n    temp_dirs = ['temp_uploads', 'processed_clips', 'results']\n    \n    for directory in temp_dirs:\n        if os.path.exists(directory):\n            try:\n                shutil.rmtree(directory)\n            except Exception as e:\n                print(f\"Warning: Could not clean up {directory}: {e}\")\n\ndef validate_video_file(file_path):\n    \"\"\"\n    Validate if a file is a readable video\n    \n    Args:\n        file_path (str): Path to video file\n        \n    Returns:\n        tuple: (is_valid, error_message)\n    \"\"\"\n    try:\n        cap = cv2.VideoCapture(file_path)\n        if not cap.isOpened():\n            return False, \"Could not open video file\"\n        \n        # Try to read first frame\n        ret, frame = cap.read()\n        if not ret:\n            cap.release()\n            return False, \"Could not read video frames\"\n        \n        # Check basic properties\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n        \n        cap.release()\n        \n        if fps <= 0 or frame_count <= 0:\n            return False, \"Invalid video properties\"\n        \n        return True, \"Video is valid\"\n        \n    except Exception as e:\n        return False, f\"Error validating video: {str(e)}\"\n\ndef estimate_processing_time(file_size_mb, duration_seconds):\n    \"\"\"\n    Estimate processing time based on file size and duration\n    \n    Args:\n        file_size_mb (float): File size in megabytes\n        duration_seconds (float): Video duration in seconds\n        \n    Returns:\n        str: Estimated processing time\n    \"\"\"\n    # Simple estimation based on empirical data\n    # Processing is roughly 0.1x real-time for motion detection\n    # Plus additional time for ML processing\n    \n    base_processing_time = duration_seconds * 0.1  # Motion detection\n    ml_processing_time = min(30, duration_seconds * 0.05)  # ML processing (capped at 30s)\n    \n    total_time = base_processing_time + ml_processing_time\n    \n    return format_duration(total_time)\n\ndef add_colored_rectangles_to_clips(clip_paths, metadata, results_df, progress_callback=None):\n    \"\"\"\n    Add color-coded rectangles to video clips based on classification.\n    Handles combined clips containing multiple objects with different IDs.\n    \n    Args:\n        clip_paths (list): List of paths to video clips\n        metadata (list): Detection metadata with bbox information\n        results_df (pd.DataFrame): Results with classifications\n        progress_callback (callable): Optional callback(current, total) for progress updates\n        \n    Returns:\n        list: Updated clip paths\n    \"\"\"\n    # Color mapping for classifications\n    color_map = {\n        'Satellite': (0, 0, 255),       # Red (BGR format)\n        'Meteor': (0, 255, 255),        # Yellow (BGR format)\n        'Plane': (255, 0, 0),           # Blue\n        'Junk': (128, 128, 128)         # Gray\n    }\n    \n    # Create classification lookup by clip_id\n    classification_lookup = {}\n    for idx, row in results_df.iterrows():\n        classification_lookup[row['clip_id']] = row['classification']\n    \n    # Group metadata by output_frame_number (for video overlay alignment)\n    # This allows us to draw multiple objects per frame\n    from collections import defaultdict\n    frame_detections = defaultdict(list)\n    for item in metadata:\n        # Use output_frame_number for video overlay (matches processed video frames)\n        frame_num = item.get('output_frame_number', item['frame_number'])\n        clip_id = item['clip_id']\n        # Only include objects that made it through filtering\n        if clip_id in classification_lookup:\n            frame_detections[frame_num].append({\n                'clip_id': clip_id,\n                'bbox_x': item['bbox_x'],\n                'bbox_y': item['bbox_y'],\n                'bbox_width': item['bbox_width'],\n                'bbox_height': item['bbox_height'],\n                'classification': classification_lookup[clip_id]\n            })\n    \n    updated_clips = []\n    total_clips = len(clip_paths)\n    \n    for clip_index, clip_path in enumerate(clip_paths):\n        # Open original clip\n        cap = cv2.VideoCapture(clip_path)\n        if not cap.isOpened():\n            updated_clips.append(clip_path)\n            continue\n        \n        fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        \n        # Create temporary output file\n        temp_path = clip_path.replace('.mp4', '_colored.mp4')\n        writer = cv2.VideoWriter(\n            temp_path,\n            cv2.VideoWriter_fourcc(*'mp4v'),\n            fps,\n            (frame_width, frame_height)\n        )\n        \n        frame_num = -1\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            frame_num += 1  # Now 0-based (first frame is 0)\n            \n            # Draw rectangles for ALL detections with color-coded boxes\n            if frame_num in frame_detections:\n                for detection in frame_detections[frame_num]:\n                    clip_id = detection['clip_id']\n                    classification = detection['classification']\n                    \n                    # Get color based on classification\n                    color = color_map.get(classification, (255, 255, 255))  # White fallback\n                    \n                    x = detection['bbox_x']\n                    y = detection['bbox_y']\n                    w = detection['bbox_width']\n                    h = detection['bbox_height']\n                    \n                    pad = 8\n                    cv2.rectangle(frame,\n                                (max(0, x-pad), max(0, y-pad)),\n                                (min(frame_width-1, x+w+pad), min(frame_height-1, y+h+pad)),\n                                color, 2)\n                    \n                    # Add classification label with object ID\n                    label = f\"ID:{clip_id} {classification}\"\n                    cv2.putText(frame, label, (x, y-10),\n                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n            \n            writer.write(frame)\n        \n        cap.release()\n        writer.release()\n        \n        # Replace original with colored version\n        if os.path.exists(temp_path):\n            # Save clean version for category-specific downloads\n            clean_path = clip_path.replace('.mp4', '_clean.mp4')\n            os.rename(clip_path, clean_path)\n            os.rename(temp_path, clip_path)\n        \n        updated_clips.append(clip_path)\n        \n        # Send progress update after each clip to keep connection alive\n        if progress_callback:\n            progress_callback(clip_index + 1, total_clips)\n    \n    return updated_clips\n\ndef analyze_video_content(uploaded_file, video_info):\n    \"\"\"\n    Analyze video content by sampling frames to detect brightness, noise, and motion characteristics\n    \n    Args:\n        uploaded_file: Streamlit uploaded file object\n        video_info (dict): Basic video info from get_video_info()\n        \n    Returns:\n        dict: Content analysis results\n    \"\"\"\n    try:\n        # Create temporary file\n        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:\n            tmp_file.write(uploaded_file.getbuffer())\n            tmp_path = tmp_file.name\n        \n        cap = cv2.VideoCapture(tmp_path)\n        if not cap.isOpened():\n            os.unlink(tmp_path)\n            return {'error': True}\n        \n        fps = video_info.get('fps_numeric', 30)\n        frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n        \n        # Sample 10 frames evenly distributed across the video\n        sample_indices = [int(i * frame_count / 10) for i in range(10)] if frame_count > 10 else [0]\n        \n        brightness_values = []\n        contrast_values = []\n        noise_levels = []\n        \n        for idx in sample_indices:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n            ret, frame = cap.read()\n            if not ret:\n                continue\n            \n            # Convert to grayscale for analysis\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            \n            # Brightness: average pixel intensity\n            brightness = np.mean(gray)\n            brightness_values.append(brightness)\n            \n            # Contrast: standard deviation of pixel intensities\n            contrast = np.std(gray)\n            contrast_values.append(contrast)\n            \n            # Noise level: apply Laplacian to detect high-frequency noise\n            laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n            noise = np.var(laplacian)\n            noise_levels.append(noise)\n        \n        cap.release()\n        os.unlink(tmp_path)\n        \n        return {\n            'avg_brightness': np.mean(brightness_values) if brightness_values else 0,\n            'avg_contrast': np.mean(contrast_values) if contrast_values else 0,\n            'avg_noise': np.mean(noise_levels) if noise_levels else 0,\n            'brightness_std': np.std(brightness_values) if brightness_values else 0,\n            'error': False\n        }\n        \n    except Exception as e:\n        return {'error': True, 'message': str(e)}\n\ndef recommend_settings(video_info, uploaded_file=None):\n    \"\"\"\n    Analyze video and recommend optimal settings for detection\n    \n    Args:\n        video_info (dict): Video information from get_video_info()\n        uploaded_file: Streamlit uploaded file object (optional, for deep analysis)\n        \n    Returns:\n        dict: Recommended settings with explanations\n    \"\"\"\n    recommendations = {\n        'sensitivity': 4,\n        'min_duration': 1.5,\n        'max_duration': 80.0,\n        'max_duration_enabled': True,  # Always enabled now\n        'frame_skip': 3,\n        'explanations': []\n    }\n    \n    duration_seconds = video_info.get('duration_seconds', 0)\n    fps_numeric = video_info.get('fps_numeric', 30)\n    resolution = video_info.get('resolution', '1920x1080')\n    \n    # Parse resolution\n    try:\n        width, height = map(int, resolution.split('x'))\n        total_pixels = width * height\n    except:\n        total_pixels = 1920 * 1080\n    \n    # Deep content analysis if uploaded file provided\n    content_analysis = None\n    if uploaded_file:\n        content_analysis = analyze_video_content(uploaded_file, video_info)\n    \n    # Recommendation 1: Frame skip based on duration and FPS\n    if duration_seconds > 3600:  # > 1 hour\n        recommendations['frame_skip'] = 6\n        recommendations['explanations'].append(\n            \"üìπ Very long video (>1hr) - using frame skip=6 for faster processing\"\n        )\n    elif duration_seconds > 600:  # > 10 minutes\n        recommendations['frame_skip'] = 6\n        recommendations['explanations'].append(\n            \"üìπ Long video (>10min) - using frame skip=6 for faster processing\"\n        )\n    elif duration_seconds > 300:  # > 5 minutes\n        recommendations['frame_skip'] = 5\n        recommendations['explanations'].append(\n            \"üìπ Long video (>5min) - using frame skip=5 for efficient processing\"\n        )\n    elif duration_seconds > 180:  # > 3 minutes\n        recommendations['frame_skip'] = 4\n        recommendations['explanations'].append(\n            \"üìπ Medium video (>3min) - using frame skip=4 for balanced speed/accuracy\"\n        )\n    elif fps_numeric >= 60:\n        recommendations['frame_skip'] = 4\n        recommendations['explanations'].append(\n            \"üìπ High FPS video (60+) - using frame skip=4 to handle extra frames\"\n        )\n    else:\n        recommendations['explanations'].append(\n            \"üìπ Short/standard video - using frame skip=3 for good accuracy\"\n        )\n    \n    # Recommendation 2: Sensitivity based on resolution and brightness\n    if content_analysis and not content_analysis.get('error'):\n        brightness = content_analysis['avg_brightness']\n        noise = content_analysis['avg_noise']\n        \n        # Very dark video (night sky)\n        if brightness < 30:\n            if noise > 100:\n                recommendations['sensitivity'] = 3\n                recommendations['explanations'].append(\n                    \"üåë Very dark & noisy video - using low sensitivity (3) to reduce false positives\"\n                )\n            else:\n                recommendations['sensitivity'] = 5\n                recommendations['explanations'].append(\n                    \"üåë Very dark but clean video - using moderate sensitivity (5)\"\n                )\n        # Dark video (twilight/dusk)\n        elif brightness < 80:\n            recommendations['sensitivity'] = 4\n            recommendations['explanations'].append(\n                \"üåÜ Dark video detected - using sensitivity=4 for twilight conditions\"\n            )\n        # Brighter video\n        else:\n            recommendations['sensitivity'] = 3\n            recommendations['explanations'].append(\n                \"‚òÄÔ∏è Relatively bright video - lowering sensitivity to avoid false detections\"\n            )\n    elif total_pixels > 2073600:  # Fallback to resolution-based\n        recommendations['sensitivity'] = 3\n        recommendations['explanations'].append(\n            \"üéØ High resolution video (>1080p) - using lower sensitivity to reduce noise\"\n        )\n    else:\n        recommendations['explanations'].append(\n            \"üéØ Standard video - using moderate sensitivity (4)\"\n        )\n    \n    # Recommendation 3: Duration settings based on FPS and content\n    if fps_numeric > 0:\n        if fps_numeric >= 60:\n            recommendations['min_duration'] = 1.0\n            recommendations['explanations'].append(\n                \"‚è±Ô∏è High FPS (60+) - can detect shorter events (min 1.0s)\"\n            )\n        elif fps_numeric >= 30:\n            recommendations['min_duration'] = 1.5\n            recommendations['explanations'].append(\n                \"‚è±Ô∏è Standard FPS (30+) - using conservative min duration (1.5s)\"\n            )\n        else:\n            recommendations['min_duration'] = 2.0\n            recommendations['explanations'].append(\n                \"‚è±Ô∏è Low FPS (<30) - using longer min duration (2.0s) for reliability\"\n            )\n    \n    # Recommendation 4: Max duration based on video length\n    if duration_seconds < 300:  # Short videos (<5 min) - use lower max to filter stars\n        recommendations['max_duration'] = 35.0\n        recommendations['explanations'].append(\n            \"‚≠ê Short video (<5min) - max duration 35s to filter out slow-moving stars\"\n        )\n    elif duration_seconds > 600:  # Long videos might have slower satellites\n        recommendations['max_duration'] = 100.0\n        recommendations['explanations'].append(\n            \"‚≠ê Long video (>10min) - max duration 100s (allows slow-moving satellites)\"\n        )\n    else:\n        recommendations['max_duration'] = 80.0\n        recommendations['explanations'].append(\n            \"‚≠ê Max duration 80s to catch slow satellites while filtering stationary stars\"\n        )\n    \n    # Recommendation 5: Noise-specific advice\n    if content_analysis and not content_analysis.get('error'):\n        noise = content_analysis['avg_noise']\n        if noise > 150:\n            recommendations['explanations'].append(\n                \"‚ö†Ô∏è High noise detected - consider increasing min duration to 2.0s\"\n            )\n    \n    # General advice\n    recommendations['explanations'].append(\n        \"üí° Settings optimized for <10 obvious detections per video\"\n    )\n    \n    return recommendations\n","size_bytes":35689},"video_processor.py":{"content":"import cv2\nimport os\nimport numpy as np\nfrom collections import deque, defaultdict\nimport csv\nfrom datetime import datetime\nfrom object_tracker import ObjectTracker\n\nclass VideoProcessor:\n    def __init__(self, sensitivity=5, min_duration=1.0, max_duration=None, frame_skip=3):\n        \"\"\"\n        Initialize video processor with configurable parameters\n        \n        Args:\n            sensitivity (int): Motion detection sensitivity (1-10)\n            min_duration (float): Minimum clip duration in seconds\n            max_duration (float): Maximum clip duration in seconds (None = no limit)\n            frame_skip (int): Process every Nth frame for performance\n        \"\"\"\n        self.sensitivity = sensitivity\n        self.min_duration = min_duration\n        self.max_duration = max_duration\n        self.frame_skip = frame_skip\n        \n        # Convert sensitivity to contour area threshold\n        # Higher sensitivity = lower threshold (detects smaller objects)\n        # Lowered threshold to detect small, distant satellites\n        self.min_contour_area = max(5, 50 - (sensitivity * 8))\n        \n    def process_video(self, video_path, progress_callback=None):\n        \"\"\"\n        Process video to detect and track individual objects\n        \n        Args:\n            video_path (str): Path to input video file\n            progress_callback (callable): Optional callback function(current, total) for progress updates\n            \n        Returns:\n            tuple: (list of clip paths, metadata list)\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            raise ValueError(f\"Could not open video file: {video_path}\")\n        \n        # Get video properties\n        fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0\n        \n        # Setup MOG2 background subtractor for better motion detection\n        # Lower varThreshold to detect small, dim satellites\n        backSub = cv2.createBackgroundSubtractorMOG2(\n            history=500,\n            varThreshold=35,  # Lower threshold - better detection of small, dim satellites\n            detectShadows=False\n        )\n        \n        # Initialize object tracker\n        # Account for frame skipping when calculating max_disappeared\n        # We want 2 seconds of real time, but we only process every frame_skip frames\n        # So: max_disappeared = (fps / frame_skip) * desired_seconds\n        max_disappeared_frames = int((fps / self.frame_skip) * 2.0)  # 2 seconds of tracked frames\n        tracker = ObjectTracker(max_disappeared=max_disappeared_frames, max_distance=150)\n        \n        # Metadata collection per object\n        object_metadata = defaultdict(list)\n        video_fps = fps\n        \n        # Results storage\n        clips_folder = \"processed_clips\"\n        os.makedirs(clips_folder, exist_ok=True)\n        \n        # Maximum contour area to filter out large objects (clouds, etc.)\n        max_contour_area = int(frame_width * frame_height * 0.005)\n        \n        frame_count = 0\n        processed_frame_count = 0  # Track output video frame number\n        motion_active = False\n        clip_writer = None\n        clip_filename = None\n        \n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                # Close video writer if active\n                if clip_writer is not None:\n                    clip_writer.release()\n                break\n            \n            frame_count += 1\n            \n            # Frame skipping for performance\n            if frame_count % self.frame_skip != 0:\n                continue\n            \n            # Send progress updates every 30 PROCESSED frames to keep connection alive\n            # (every 30 * frame_skip actual frames)\n            if progress_callback and frame_count % (30 * self.frame_skip) == 0:\n                if total_frames > 0:\n                    progress_callback(frame_count, total_frames)\n                else:\n                    # If total_frames unknown, estimate growing total for monotonic progress\n                    # Use current_frame * 1.5 so progress gradually increases (66% -> 70% -> 75%...)\n                    estimated_total = int(frame_count * 1.5) + 1000  # +1000 to start slower\n                    progress_callback(frame_count, estimated_total)\n            \n            # Motion detection pipeline\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            gray_blur = cv2.GaussianBlur(gray, (5, 5), 0)\n            \n            # Background subtraction\n            fgMask = backSub.apply(gray_blur)\n            \n            # Morphological operations to clean up the mask\n            kernel = np.ones((3, 3), np.uint8)\n            fgMask = cv2.morphologyEx(fgMask, cv2.MORPH_OPEN, kernel)\n            fgMask = cv2.morphologyEx(fgMask, cv2.MORPH_CLOSE, kernel)\n            \n            # Find contours\n            contours, _ = cv2.findContours(fgMask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            \n            # Detect objects and extract their properties\n            current_frame_objects = []\n            current_frame_centroids = []\n            \n            for contour in contours:\n                area = cv2.contourArea(contour)\n                \n                # Filter by area (sensitivity-based)\n                if self.min_contour_area < area < max_contour_area:\n                    # Extract object properties\n                    x, y, w, h = cv2.boundingRect(contour)\n                    \n                    # Calculate centroid\n                    M = cv2.moments(contour)\n                    if M[\"m00\"] != 0:\n                        centroid_x = int(M[\"m10\"] / M[\"m00\"])\n                        centroid_y = int(M[\"m01\"] / M[\"m00\"])\n                    else:\n                        centroid_x, centroid_y = x + w//2, y + h//2\n                    \n                    # Calculate aspect ratio\n                    aspect_ratio = w / h if h > 0 else 0\n                    \n                    # Calculate brightness (mean intensity in bounding box)\n                    roi = gray[y:y+h, x:x+w]\n                    mean_brightness = np.mean(roi) if roi.size > 0 else 0\n                    max_brightness = np.max(roi) if roi.size > 0 else 0\n                    \n                    # Store object data (without clip_id yet)\n                    object_data = {\n                        'frame_number': frame_count,  # INPUT frame for speed calculations\n                        'output_frame_number': processed_frame_count,  # OUTPUT frame index (0-based, will match video)\n                        'area': area,\n                        'centroid_x': centroid_x,\n                        'centroid_y': centroid_y,\n                        'bbox_x': x,\n                        'bbox_y': y,\n                        'bbox_width': w,\n                        'bbox_height': h,\n                        'aspect_ratio': aspect_ratio,\n                        'mean_brightness': mean_brightness,\n                        'max_brightness': max_brightness,\n                        'fps': video_fps\n                    }\n                    current_frame_objects.append(object_data)\n                    current_frame_centroids.append((centroid_x, centroid_y))\n            \n            # Update tracker with detected centroids\n            tracked_objects = tracker.update(current_frame_centroids)\n            \n            # Assign object_id (as clip_id) to each detection\n            if len(tracked_objects) > 0 and len(current_frame_objects) > 0:\n                # Match detections to tracked objects by centroid\n                for i, (centroid_x, centroid_y) in enumerate(current_frame_centroids):\n                    # Find closest tracked object\n                    min_dist = float('inf')\n                    assigned_id = None\n                    \n                    for obj_id, (tx, ty) in tracked_objects.items():\n                        dist = np.sqrt((centroid_x - tx)**2 + (centroid_y - ty)**2)\n                        if dist < min_dist:\n                            min_dist = dist\n                            assigned_id = obj_id\n                    \n                    if assigned_id is not None:\n                        # Add clip_id and store metadata\n                        current_frame_objects[i]['clip_id'] = assigned_id\n                        object_metadata[assigned_id].append(current_frame_objects[i])\n            \n            # Start video writer if we have detections\n            if len(tracked_objects) > 0 and not motion_active:\n                motion_active = True\n                clip_filename = os.path.join(clips_folder, \"clip_0001.mp4\")\n                # Calculate output FPS to achieve exactly 10x speedup\n                # Output duration = input duration / 10\n                output_fps = 10.0 * fps / self.frame_skip\n                clip_writer = cv2.VideoWriter(\n                    clip_filename,\n                    cv2.VideoWriter_fourcc(*'mp4v'),\n                    output_fps,\n                    (frame_width, frame_height)\n                )\n            \n            # Write frame if writer is active\n            if clip_writer is not None:\n                clip_writer.write(frame)\n                processed_frame_count += 1  # Track output video frame number\n        \n        cap.release()\n        \n        # Filter objects by duration (min and max)\n        # Account for frame skipping: we only get detections every frame_skip frames\n        min_detections = int((fps / self.frame_skip) * self.min_duration)\n        max_detections = int((fps / self.frame_skip) * self.max_duration) if self.max_duration else float('inf')\n        filtered_metadata = []\n        \n        for obj_id, detections in object_metadata.items():\n            num_detections = len(detections)\n            if min_detections <= num_detections <= max_detections:\n                filtered_metadata.extend(detections)\n        \n        # Prepare motion clips list\n        motion_clips = [clip_filename] if clip_filename and os.path.exists(clip_filename) else []\n        \n        return motion_clips, filtered_metadata\n","size_bytes":10288},"replit.md":{"content":"# SkySeer AI\n\n## Overview\n\nSkySeer AI is an advanced computer vision and machine learning system designed for detecting and classifying sky objects such as satellites and meteors in night sky video footage. It processes raw video to identify movements, extracts numerical \"flight signatures,\" and uses K-Means clustering for categorization, prioritizing precision to minimize false positives. The system includes trajectory prediction analysis for aerospace applications, comprehensive technical documentation of its ML pipeline, and professional PDF mission report generation. It aims to provide a robust solution for amateur astronomy and low-light camera setups.\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## Recent Findings\n\n### Satellite Detection Optimization (Oct 2025)\n- **Critical Discovery**: Maximum clip duration was the primary bottleneck for satellite detection\n- **Optimal Settings**: Max duration of 80-100 seconds captures slow-moving satellites that take 60+ seconds to cross the frame\n- **Previous Issue**: Default 15s max duration was filtering out many valid satellites\n- **Detection Parameters**: varThreshold=35 (MOG2 sensitivity), min_contour_area down to 5 pixels at sensitivity=7\n- **Result**: Significantly improved detection rate for small, dim, and slow-moving satellites\n\n## System Architecture\n\n### Frontend Architecture\n\nThe application uses Streamlit to provide a web-based interface. It features a sidebar for configuration, allowing video upload (up to 5GB), real-time processing feedback, and interactive data visualizations (Plotly). The interface supports advanced trajectory visualization and enables the download of processed results.\n\n### Backend Architecture\n\nThe system operates through a multi-stage sequential processing pipeline:\n\n1.  **Video Ingestion**: Employs OpenCV for frame extraction and MOG2 background subtraction to detect motion and generate clips.\n2.  **Feature Extraction**: Converts visual detections into 11-dimensional numerical \"flight signatures\" using kinematic metrics (speed, acceleration, trajectory linearity) and consistency scores.\n3.  **ML Classification**: Utilizes StandardScaler for feature normalization and K-Means clustering (3 clusters) to categorize objects as Satellite, Meteor, or Junk, with confidence scores.\n4.  **Utility Functions**: Manages video metadata, file operations (ZIP creation), and data formatting.\n5.  **Trajectory Visualization**: Generates interactive path visualizations, speed heatmaps, polar plots, and timeline analyses, including predictive modeling for object trajectories.\n\nThe pipeline is designed for maintainability and extensibility, focusing on robustness to visual noise, unsupervised learning, and statistical feature analysis.\n\n### Data Storage Solutions\n\nA hybrid storage approach is used, combining file-based storage for input videos and classified output clips, and a PostgreSQL database for `AnalysisSession`, `DetectionClip`, and `ObjectDetection` data. This setup supports multi-night analysis and historical tracking.\n\n### Machine Learning Model Architecture\n\nThe system employs an unsupervised K-Means Clustering model (3 clusters) for automatic categorization. It operates on an 11-dimensional feature space derived from kinematic and brightness patterns, which are more reliable in low-light conditions. This approach eliminates the need for labeled training data and adapts to new patterns automatically.\n\n## External Dependencies\n\n### Computer Vision Libraries\n\n-   **OpenCV (cv2)**: For video processing, frame extraction, and background subtraction.\n-   **NumPy**: For numerical operations.\n\n### Machine Learning Libraries\n\n-   **scikit-learn**: For K-Means clustering and feature scaling.\n\n### Web Framework & UI\n\n-   **Streamlit**: For the web application framework and UI components.\n-   **Plotly**: For interactive data visualization.\n\n### Data Processing\n\n-   **Pandas**: For DataFrame operations.\n\n### Utility Libraries\n\n-   **ReportLab**: Professional PDF generation for mission reports.\n-   **Python Standard Library**: For file/directory operations, archive creation, in-memory file handling, timestamp generation, efficient data structures, CSV handling, encoding, and temporary files.","size_bytes":4265},"ml_classifier.py":{"content":"import pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nclass MLClassifier:\n    def __init__(self, n_clusters=3, random_state=42):\n        \"\"\"\n        Initialize ML classifier with K-Means clustering\n        \n        Args:\n            n_clusters (int): Number of clusters for K-Means (default: 3 for Satellite/Meteor/Plane)\n            random_state (int): Random seed for reproducibility\n        \"\"\"\n        self.n_clusters = n_clusters\n        self.random_state = random_state\n        \n        # Initialize models\n        self.scaler = StandardScaler()\n        self.kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n        \n        # Feature columns for ML processing\n        self.feature_columns = [\n            'avg_speed', 'speed_consistency', 'duration', 'linearity',\n            'direction_changes', 'size_consistency', 'avg_acceleration',\n            'blinking_score',  # Added for better plane detection\n            'satellite_score', 'meteor_score', 'plane_score'\n        ]\n    \n    def classify_objects(self, features_df):\n        \"\"\"\n        Classify detected objects using unsupervised learning\n        \n        Args:\n            features_df (pd.DataFrame): DataFrame with extracted features\n            \n        Returns:\n            pd.DataFrame: DataFrame with classifications and confidence scores\n        \"\"\"\n        if features_df.empty:\n            return pd.DataFrame()\n        \n        # Step 0: Check for star groups first (highest priority)\n        # Stars are identified by group movement analysis\n        if 'is_star_group' in features_df.columns:\n            star_mask = features_df['is_star_group'] == 1\n            if star_mask.any():\n                # Classify stars separately and return\n                results_df = features_df.copy()\n                results_df['classification'] = ''\n                results_df['confidence'] = 0.0\n                results_df['cluster'] = 0\n                \n                # Classify stars\n                results_df.loc[star_mask, 'classification'] = 'Star'\n                results_df.loc[star_mask, 'confidence'] = 0.9\n                \n                # Classify non-stars using normal pipeline\n                non_star_df = features_df[~star_mask].copy()\n                if not non_star_df.empty and len(non_star_df) >= 2:\n                    non_star_results = self._classify_non_stars(non_star_df)\n                    for col in ['classification', 'confidence', 'cluster']:\n                        if col in non_star_results.columns:\n                            results_df.loc[~star_mask, col] = non_star_results[col].values\n                elif not non_star_df.empty:\n                    # Single non-star object, use rule-based\n                    single_result = self._classify_single_object(non_star_df)\n                    for col in ['classification', 'confidence', 'cluster']:\n                        if col in single_result.columns:\n                            results_df.loc[~star_mask, col] = single_result[col].values\n                \n                return results_df\n        \n        # No stars detected, use normal classification pipeline\n        return self._classify_non_stars(features_df)\n    \n    def _classify_non_stars(self, features_df):\n        \"\"\"Classify non-star objects using the normal ML pipeline\"\"\"\n        if features_df.empty:\n            return pd.DataFrame()\n        \n        # Prepare features for ML\n        X = self._prepare_features(features_df)\n        \n        if X.shape[0] < 2:\n            # Not enough data for clustering\n            return self._classify_single_object(features_df)\n        \n        # Step 1: K-Means Clustering for primary classification\n        # Adjust number of clusters based on available samples\n        n_samples = X.shape[0]\n        n_clusters = min(self.n_clusters, n_samples)\n        \n        X_scaled = self.scaler.fit_transform(X)\n        \n        # Only use KMeans if we have enough samples\n        if n_clusters >= 2:\n            kmeans = KMeans(n_clusters=n_clusters, random_state=self.random_state, n_init=10)\n            cluster_labels = kmeans.fit_predict(X_scaled)\n        else:\n            # Fall back to rule-based for single sample\n            return self._classify_single_object(features_df)\n        \n        # Create results dataframe\n        results_df = features_df.copy()\n        results_df['cluster'] = cluster_labels\n        \n        # Step 2: Interpret clusters and assign classifications\n        results_df = self._interpret_clusters(results_df)\n        \n        # Step 3: Calculate final confidence scores\n        results_df = self._calculate_confidence_scores(results_df)\n        \n        return results_df\n    \n    def _prepare_features(self, features_df):\n        \"\"\"Prepare feature matrix for ML algorithms\"\"\"\n        # Select relevant features and handle missing values\n        X = features_df[self.feature_columns].copy()\n        X = X.fillna(0)  # Fill NaN values with 0\n        \n        # Handle infinite values\n        X = X.replace([np.inf, -np.inf], 0)\n        \n        return X.values\n    \n    def _classify_single_object(self, features_df):\n        \"\"\"Handle classification when only one object is detected\"\"\"\n        results_df = features_df.copy()\n        \n        # Use rule-based classification for single objects\n        for idx, row in results_df.iterrows():\n            classification, confidence = self._rule_based_classification(row)\n            results_df.loc[idx, 'classification'] = classification\n            results_df.loc[idx, 'confidence'] = confidence\n            results_df.loc[idx, 'cluster'] = 0\n        \n        return results_df\n    \n    def _interpret_clusters(self, results_df):\n        \"\"\"Interpret K-Means clusters and assign object classifications\"\"\"\n        cluster_interpretations = {}\n        \n        # Get actual number of clusters from the data\n        n_actual_clusters = results_df['cluster'].nunique()\n        \n        for cluster_id in range(n_actual_clusters):\n            cluster_data = results_df[results_df['cluster'] == cluster_id]\n            \n            if cluster_data.empty:\n                cluster_interpretations[cluster_id] = ('Junk', 0.5)\n                continue\n            \n            # Analyze cluster characteristics\n            avg_speed = cluster_data['avg_speed'].mean()\n            avg_consistency = cluster_data['speed_consistency'].mean()\n            avg_linearity = cluster_data['linearity'].mean()\n            avg_duration = cluster_data['duration'].mean()\n            avg_satellite_score = cluster_data['satellite_score'].mean()\n            avg_meteor_score = cluster_data['meteor_score'].mean()\n            avg_plane_score = cluster_data['plane_score'].mean()\n            \n            # Determine most likely classification for this cluster\n            scores = {\n                'Satellite': avg_satellite_score,\n                'Meteor': avg_meteor_score,\n                'Plane': avg_plane_score,\n                'Junk': 0.1  # Base score for junk category\n            }\n            \n            # Apply minimal heuristics - mostly trust the base ML scores\n            # Only boost when there's VERY strong evidence\n            if avg_speed > 30 and avg_linearity > 0.85 and avg_duration < 1.5:\n                # Extremely fast, linear, brief -> Definitely Meteor\n                scores['Meteor'] *= 1.8\n            elif avg_consistency < 0.3 or avg_linearity < 0.3:\n                # Very poor quality trajectory -> Likely Junk\n                scores['Junk'] *= 1.5\n            \n            # Select best classification\n            best_class = max(scores, key=scores.get)\n            confidence = min(scores[best_class], 0.95)  # Cap confidence at 95%\n            \n            cluster_interpretations[cluster_id] = (best_class, confidence)\n        \n        # Assign classifications based on cluster interpretations\n        results_df['classification'] = ''\n        results_df['confidence'] = 0.0\n        \n        for idx, row in results_df.iterrows():\n            cluster_id = row['cluster']\n            classification, confidence = cluster_interpretations[cluster_id]\n            results_df.loc[idx, 'classification'] = classification\n            results_df.loc[idx, 'confidence'] = confidence\n        \n        return results_df\n    \n    def _rule_based_classification(self, row):\n        \"\"\"Rule-based classification for edge cases - trust the feature scores\"\"\"\n        # Use the pre-computed scores from feature_extractor\n        satellite_score = row.get('satellite_score', 0)\n        meteor_score = row.get('meteor_score', 0)\n        plane_score = row.get('plane_score', 0)\n        avg_speed = row.get('avg_speed', 0)\n        \n        # Determine best classification based on scores\n        scores = {\n            'Satellite': satellite_score,\n            'Meteor': meteor_score,\n            'Plane': plane_score,\n            'Junk': 0.1\n        }\n        \n        # Apply minimal adjustments for extreme cases\n        if avg_speed < 0.5:\n            # Very slow objects (likely stars or stationary noise) - classify as Junk\n            scores['Junk'] *= 5.0\n        elif avg_speed > 30 and row.get('linearity', 0) > 0.85 and row.get('duration', 0) < 1.5:\n            scores['Meteor'] *= 1.8\n        elif row.get('speed_consistency', 0) < 0.3 or row.get('linearity', 0) < 0.3:\n            scores['Junk'] *= 1.5\n        \n        # Select best classification\n        best_class = max(scores, key=scores.get)\n        confidence = min(scores[best_class], 0.85)  # Cap at 85% for single objects\n        \n        return best_class, confidence\n    \n    def _calculate_confidence_scores(self, results_df):\n        \"\"\"Calculate and refine confidence scores based on multiple factors\"\"\"\n        for idx, row in results_df.iterrows():\n            base_confidence = row.get('confidence', 0.5)\n            \n            # Adjust confidence based on detection count (with safe fallback)\n            detection_count = row.get('detection_count', 1)\n            if detection_count < 3:\n                base_confidence *= 0.8  # Lower confidence for few detections\n            elif detection_count > 10:\n                base_confidence = min(base_confidence * 1.1, 0.95)  # Higher confidence for many detections\n            \n            # Adjust confidence based on feature quality\n            linearity = row.get('linearity', 0)\n            speed_consistency = row.get('speed_consistency', 0)\n            \n            if linearity > 0.9 and speed_consistency > 0.8:\n                base_confidence = min(base_confidence * 1.2, 0.95)  # High quality trajectory\n            elif linearity < 0.3 or speed_consistency < 0.3:\n                base_confidence *= 0.7  # Poor quality trajectory\n            \n            results_df.loc[idx, 'confidence'] = round(base_confidence, 3)\n        \n        return results_df\n","size_bytes":11071},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"numpy>=2.3.3\",\n    \"opencv-python>=4.11.0.86\",\n    \"opencv-python-headless>=4.11.0.86\",\n    \"pandas>=2.3.3\",\n    \"plotly>=6.3.1\",\n    \"psycopg2-binary>=2.9.11\",\n    \"reportlab>=4.4.4\",\n    \"scikit-learn>=1.7.2\",\n    \"scipy>=1.16.2\",\n    \"sqlalchemy>=2.0.43\",\n    \"streamlit>=1.50.0\",\n]\n","size_bytes":433},"feature_extractor.py":{"content":"import pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nimport math\nfrom star_detector import StarDetector\n\nclass FeatureExtractor:\n    def __init__(self):\n        \"\"\"Initialize feature extractor for motion analysis\"\"\"\n        self.star_detector = StarDetector()\n    \n    def extract_features(self, metadata):\n        \"\"\"\n        Extract movement features from motion detection metadata\n        \n        Args:\n            metadata (list): List of detection metadata dictionaries\n            \n        Returns:\n            pd.DataFrame: DataFrame with extracted features for each clip\n        \"\"\"\n        if not metadata:\n            return pd.DataFrame()\n        \n        # Detect star groups first\n        star_clip_ids = self.star_detector.detect_star_groups(metadata)\n        \n        # Group metadata by clip_id\n        clips_data = defaultdict(list)\n        for item in metadata:\n            clips_data[item['clip_id']].append(item)\n        \n        features_list = []\n        \n        for clip_id, clip_detections in clips_data.items():\n            if not clip_detections:\n                continue\n            \n            # Sort detections by frame number\n            clip_detections.sort(key=lambda x: x['frame_number'])\n            \n            # Get FPS from first detection (all should have same FPS)\n            video_fps = clip_detections[0].get('fps', 30) if clip_detections else 30\n            \n            # Extract basic trajectory features\n            features = self._extract_clip_features(clip_id, clip_detections, video_fps)\n            \n            # Add star detection feature\n            features['is_star_group'] = 1 if clip_id in star_clip_ids else 0\n            \n            features_list.append(features)\n        \n        return pd.DataFrame(features_list)\n    \n    def _extract_clip_features(self, clip_id, detections, fps=30):\n        \"\"\"\n        Extract comprehensive features for a single clip\n        \n        Args:\n            clip_id (int): Unique identifier for the clip\n            detections (list): List of detection dictionaries for this clip\n            fps (float): Video frame rate for accurate speed/duration calculations\n            \n        Returns:\n            dict: Dictionary of extracted features\n        \"\"\"\n        if len(detections) < 2:\n            return self._create_minimal_features(clip_id, detections, fps)\n        \n        # Extract position data\n        positions = [(d['centroid_x'], d['centroid_y']) for d in detections]\n        frame_numbers = [d['frame_number'] for d in detections]\n        areas = [d['area'] for d in detections]\n        aspect_ratios = [d['aspect_ratio'] for d in detections]\n        \n        # Extract brightness data\n        mean_brightness_values = [d.get('mean_brightness', 0) for d in detections]\n        max_brightness_values = [d.get('max_brightness', 0) for d in detections]\n        \n        # Calculate speeds between consecutive positions\n        speeds = []\n        for i in range(1, len(positions)):\n            dx = positions[i][0] - positions[i-1][0]\n            dy = positions[i][1] - positions[i-1][1]\n            distance = math.sqrt(dx*dx + dy*dy)\n            frame_diff = frame_numbers[i] - frame_numbers[i-1]\n            speed = distance / max(frame_diff, 1)  # pixels per frame\n            speeds.append(speed)\n        \n        # Calculate trajectory features\n        avg_speed = np.mean(speeds) if speeds else 0\n        speed_std = np.std(speeds) if len(speeds) > 1 else 0\n        max_speed = max(speeds) if speeds else 0\n        min_speed = min(speeds) if speeds else 0\n        \n        # Speed consistency (lower std = more consistent)\n        speed_consistency = 1.0 / (1.0 + speed_std) if speed_std > 0 else 1.0\n        \n        # Calculate total path length and duration\n        total_distance = sum([math.sqrt((positions[i][0] - positions[i-1][0])**2 + \n                                      (positions[i][1] - positions[i-1][1])**2) \n                            for i in range(1, len(positions))])\n        \n        duration = (frame_numbers[-1] - frame_numbers[0]) / float(fps)  # Use actual FPS\n        \n        # Trajectory linearity (straight line vs actual path)\n        if len(positions) >= 2:\n            start_pos = positions[0]\n            end_pos = positions[-1]\n            straight_line_distance = math.sqrt((end_pos[0] - start_pos[0])**2 + \n                                             (end_pos[1] - start_pos[1])**2)\n            linearity = straight_line_distance / max(total_distance, 1)\n        else:\n            linearity = 1.0\n        \n        # Direction changes (indicator of erratic movement)\n        direction_changes = 0\n        if len(positions) >= 3:\n            for i in range(1, len(positions) - 1):\n                # Calculate vectors\n                v1 = (positions[i][0] - positions[i-1][0], positions[i][1] - positions[i-1][1])\n                v2 = (positions[i+1][0] - positions[i][0], positions[i+1][1] - positions[i][1])\n                \n                # Calculate angle between vectors\n                dot_product = v1[0]*v2[0] + v1[1]*v2[1]\n                mag1 = math.sqrt(v1[0]**2 + v1[1]**2)\n                mag2 = math.sqrt(v2[0]**2 + v2[1]**2)\n                \n                if mag1 > 0 and mag2 > 0:\n                    cos_angle = dot_product / (mag1 * mag2)\n                    cos_angle = max(-1, min(1, cos_angle))  # Clamp to valid range\n                    angle = math.acos(cos_angle)\n                    \n                    # Count significant direction changes (> 30 degrees)\n                    if angle > math.pi / 6:\n                        direction_changes += 1\n        \n        # Object size statistics\n        avg_area = np.mean(areas)\n        area_std = np.std(areas) if len(areas) > 1 else 0\n        max_area = max(areas)\n        min_area = min(areas)\n        \n        # Size consistency\n        size_consistency = 1.0 / (1.0 + area_std / max(avg_area, 1))\n        \n        # Aspect ratio statistics\n        avg_aspect_ratio = np.mean(aspect_ratios)\n        aspect_ratio_std = np.std(aspect_ratios) if len(aspect_ratios) > 1 else 0\n        \n        # Acceleration analysis\n        accelerations = []\n        if len(speeds) > 1:\n            for i in range(1, len(speeds)):\n                accel = abs(speeds[i] - speeds[i-1])\n                accelerations.append(accel)\n        \n        avg_acceleration = np.mean(accelerations) if accelerations else 0\n        max_acceleration = max(accelerations) if accelerations else 0\n        \n        # Brightness statistics (for distinguishing planes from satellites)\n        avg_brightness = np.mean(mean_brightness_values) if mean_brightness_values else 0\n        brightness_std = np.std(mean_brightness_values) if len(mean_brightness_values) > 1 else 0\n        max_brightness = max(max_brightness_values) if max_brightness_values else 0\n        \n        # Brightness consistency (low variance = constant like satellite, high variance = blinking like plane)\n        brightness_consistency = 1.0 / (1.0 + brightness_std / max(avg_brightness, 1))\n        \n        # Enhanced blinking pattern detection for planes\n        brightness_variation_coeff = (brightness_std / max(avg_brightness, 1)) if avg_brightness > 0 else 0\n        \n        # Detect periodic blinking (on/off pattern typical of plane lights)\n        blinking_pattern_score = 0.0\n        if len(mean_brightness_values) >= 3 and avg_brightness > 1.0:  # Safeguard against zero brightness\n            # Calculate brightness differences between consecutive frames\n            brightness_diffs = [abs(mean_brightness_values[i] - mean_brightness_values[i-1]) \n                               for i in range(1, len(mean_brightness_values))]\n            \n            # High differences indicate blinking\n            avg_diff = np.mean(brightness_diffs) if brightness_diffs else 0\n            if avg_diff > avg_brightness * 0.15:  # 15% change threshold\n                blinking_pattern_score = min(avg_diff / avg_brightness, 1.0)\n        \n        # Combine blinking indicators\n        blinking_score = max(brightness_variation_coeff, blinking_pattern_score)\n        \n        # Movement pattern classification hints  \n        # Use duration as PRIMARY discriminator between satellites and planes\n        \n        # Satellite: consistent, linear, HIGH score for 3-18s duration (typical satellite pass)\n        # Score peaks at 3-18s, gentle decline for longer durations\n        if duration < 3:\n            duration_satellite_factor = 0.6\n        elif duration < 18:\n            duration_satellite_factor = 1.2  # Boost typical satellite range (extended to 18s)\n        elif duration < 25:\n            duration_satellite_factor = 0.85  # Gentle decline\n        else:\n            duration_satellite_factor = 0.65  # Penalize very long durations\n        \n        satellite_consistency = size_consistency * brightness_consistency\n        satellite_score = speed_consistency * linearity * satellite_consistency * duration_satellite_factor\n        \n        # Meteor: high speed, very linear, brief duration, often bright/flashing\n        # Meteors are fast-moving (high speed), very straight (high linearity), \n        # short duration (<3s), and often have brightness spikes\n        speed_factor = min(avg_speed / 30.0, 3.0)  # Cap at 3x for very fast objects\n        duration_factor = (1.0 / max(duration, 0.1)) if duration < 3 else 0.3  # Heavily penalize >3s\n        brightness_factor = 1.0 + min(max_brightness / 255.0, 0.5)  # Bonus for bright objects (up to 1.5x)\n        \n        meteor_score = speed_factor * linearity * duration_factor * brightness_factor\n        \n        # Plane: consistent movement, HIGH score for 15+ sec duration (planes stay visible longer)\n        # Score increases with duration, peaks at 15+ seconds\n        if duration < 10:\n            duration_plane_factor = 0.4  # Low score for short durations\n        elif duration < 15:\n            duration_plane_factor = 0.8\n        else:\n            duration_plane_factor = 1.1  # Boost long durations\n        \n        # Enhanced plane detection with blinking pattern recognition\n        # Planes often have blinking lights (especially red ones)\n        # Penalize planes that have satellite-like consistency (unless they're blinking)\n        satellite_like_consistency = size_consistency * brightness_consistency\n        if satellite_like_consistency > 0.6 and blinking_score < 0.15:\n            consistency_penalty = 0.65  # Penalize satellite-like characteristics\n        else:\n            consistency_penalty = 1.0\n        \n        # Strong bonus for blinking pattern (planes typically blink)\n        blinking_bonus = 1.0 + min(blinking_score * 0.8, 0.8)  # Up to 1.8x bonus for strong blinking\n        plane_score = speed_consistency * linearity * duration_plane_factor * blinking_bonus * consistency_penalty\n        \n        # Anomaly indicators: erratic movement, inconsistent speed/size\n        anomaly_indicators = (\n            direction_changes / max(len(positions), 1) +  # Direction change rate\n            (1.0 - speed_consistency) +  # Speed inconsistency\n            (1.0 - size_consistency) +   # Size inconsistency\n            max_acceleration / max(avg_speed, 1)  # High acceleration relative to speed\n        )\n        \n        return {\n            'clip_id': clip_id,\n            'duration': duration,\n            'avg_speed': avg_speed,\n            'max_speed': max_speed,\n            'min_speed': min_speed,\n            'speed_consistency': speed_consistency,\n            'total_distance': total_distance,\n            'linearity': linearity,\n            'direction_changes': direction_changes,\n            'avg_area': avg_area,\n            'max_area': max_area,\n            'min_area': min_area,\n            'size_consistency': size_consistency,\n            'avg_aspect_ratio': avg_aspect_ratio,\n            'aspect_ratio_std': aspect_ratio_std,\n            'avg_acceleration': avg_acceleration,\n            'max_acceleration': max_acceleration,\n            'avg_brightness': avg_brightness,\n            'brightness_std': brightness_std,\n            'max_brightness': max_brightness,\n            'brightness_consistency': brightness_consistency,\n            'brightness_variation_coeff': brightness_variation_coeff,\n            'blinking_score': blinking_score,  # Enhanced blinking detection\n            'satellite_score': satellite_score,\n            'meteor_score': meteor_score,\n            'plane_score': plane_score,\n            'anomaly_indicators': anomaly_indicators,\n            'detection_count': len(detections)\n        }\n    \n    def _create_minimal_features(self, clip_id, detections, fps=30):\n        \"\"\"Create minimal feature set for clips with insufficient data\"\"\"\n        if not detections:\n            return {\n                'clip_id': clip_id,\n                'duration': 0,\n                'avg_speed': 0,\n                'max_speed': 0,\n                'min_speed': 0,\n                'speed_consistency': 0,\n                'total_distance': 0,\n                'linearity': 0,\n                'direction_changes': 0,\n                'avg_area': 0,\n                'max_area': 0,\n                'min_area': 0,\n                'size_consistency': 0,\n                'avg_aspect_ratio': 1,\n                'aspect_ratio_std': 0,\n                'avg_acceleration': 0,\n                'max_acceleration': 0,\n                'avg_brightness': 0,\n                'brightness_std': 0,\n                'max_brightness': 0,\n                'brightness_consistency': 0,\n                'brightness_variation_coeff': 0,\n                'blinking_score': 0,  # No blinking for insufficient data\n                'satellite_score': 0,\n                'meteor_score': 0,\n                'anomaly_indicators': 1,  # High anomaly score for insufficient data\n                'detection_count': len(detections)\n            }\n        \n        # Single detection case\n        detection = detections[0]\n        return {\n            'clip_id': clip_id,\n            'duration': 1.0 / float(fps),  # Single frame duration using actual FPS\n            'avg_speed': 0,\n            'max_speed': 0,\n            'min_speed': 0,\n            'speed_consistency': 0,\n            'total_distance': 0,\n            'linearity': 0,\n            'direction_changes': 0,\n            'avg_area': detection['area'],\n            'max_area': detection['area'],\n            'min_area': detection['area'],\n            'size_consistency': 1,\n            'avg_aspect_ratio': detection['aspect_ratio'],\n            'aspect_ratio_std': 0,\n            'avg_acceleration': 0,\n            'max_acceleration': 0,\n            'avg_brightness': detection.get('mean_brightness', 0),\n            'brightness_std': 0,\n            'max_brightness': detection.get('max_brightness', 0),\n            'brightness_consistency': 1,\n            'brightness_variation_coeff': 0,\n            'blinking_score': 0,  # No blinking for single detection\n            'satellite_score': 0,\n            'meteor_score': 0,\n            'anomaly_indicators': 0.5,  # Medium anomaly score for single detection\n            'detection_count': 1\n        }\n","size_bytes":15181},"star_detector.py":{"content":"import numpy as np\nimport math\nfrom collections import defaultdict\nfrom typing import List, Dict, Tuple, Set\n\nclass StarDetector:\n    def __init__(self, min_group_size=3, velocity_tolerance=0.2, direction_tolerance=20, \n                 min_temporal_overlap=0.5, max_spatial_distance=500, min_speed=0.01):\n        \"\"\"\n        Initialize star detector to identify stationary stars based on group movement\n        \n        Args:\n            min_group_size (int): Minimum number of objects moving together to classify as stars (default: 3)\n            velocity_tolerance (float): Maximum relative difference in velocity (0-1, default: 0.2 = 20%)\n            direction_tolerance (float): Maximum direction difference in degrees (default: 20)\n            min_temporal_overlap (float): Minimum fraction of frames that must overlap (default: 0.5 = 50%)\n            max_spatial_distance (float): Maximum average distance between group members in pixels (default: 500)\n            min_speed (float): Minimum speed in pixels/frame to consider (default: 0.01, very low to catch stars)\n        \"\"\"\n        self.min_group_size = min_group_size\n        self.velocity_tolerance = velocity_tolerance\n        self.direction_tolerance = direction_tolerance\n        self.min_temporal_overlap = min_temporal_overlap\n        self.max_spatial_distance = max_spatial_distance\n        self.min_speed = min_speed\n    \n    def detect_star_groups(self, metadata: List[Dict]) -> Set[int]:\n        \"\"\"\n        Detect which clip IDs contain stars based on group movement analysis\n        \n        Args:\n            metadata (list): List of detection metadata dictionaries\n            \n        Returns:\n            set: Set of clip IDs that are classified as stars\n        \"\"\"\n        if not metadata:\n            return set()\n        \n        # Group metadata by clip_id\n        clips_data = defaultdict(list)\n        for item in metadata:\n            clips_data[item['clip_id']].append(item)\n        \n        # Build candidate groups by finding mutually overlapping, similarly moving objects\n        candidate_groups = []\n        processed_clips = set()\n        \n        for clip_id in sorted(clips_data.keys()):\n            if clip_id in processed_clips or len(clips_data[clip_id]) < 2:\n                continue\n            \n            clip_velocity = self._calculate_velocity_vector(clips_data[clip_id])\n            if clip_velocity is None:\n                continue\n            \n            clip_frame_range = self._get_frame_range(clips_data[clip_id])\n            \n            # Build a group starting with this clip\n            current_group = [clip_id]\n            \n            # Find all clips that are similar to the current clip\n            for other_clip_id in sorted(clips_data.keys()):\n                if other_clip_id == clip_id or other_clip_id in processed_clips or len(clips_data[other_clip_id]) < 2:\n                    continue\n                \n                other_velocity = self._calculate_velocity_vector(clips_data[other_clip_id])\n                if other_velocity is None:\n                    continue\n                \n                other_frame_range = self._get_frame_range(clips_data[other_clip_id])\n                \n                # Check if this clip is compatible with ALL members of the current group\n                is_compatible = True\n                \n                for group_member_id in current_group:\n                    member_velocity = self._calculate_velocity_vector(clips_data[group_member_id])\n                    member_frame_range = self._get_frame_range(clips_data[group_member_id])\n                    \n                    # Check temporal overlap with this group member\n                    if not self._has_sufficient_temporal_overlap(other_frame_range, member_frame_range):\n                        is_compatible = False\n                        break\n                    \n                    # Check velocity similarity with this group member\n                    if not self._are_velocities_similar(other_velocity, member_velocity):\n                        is_compatible = False\n                        break\n                    \n                    # Check spatial proximity with this group member\n                    if not self._are_spatially_close(clips_data[other_clip_id], clips_data[group_member_id]):\n                        is_compatible = False\n                        break\n                \n                # If compatible with all group members, add to group\n                if is_compatible:\n                    current_group.append(other_clip_id)\n            \n            # Check if group has minimum size and sustained co-movement\n            if len(current_group) >= self.min_group_size:\n                # Verify sustained co-movement by checking absolute overlap duration\n                if self._has_sustained_comovement(current_group, clips_data):\n                    candidate_groups.append(current_group)\n                    processed_clips.update(current_group)\n        \n        # Collect all clip IDs from valid star groups\n        star_clip_ids = set()\n        for group in candidate_groups:\n            star_clip_ids.update(group)\n        \n        return star_clip_ids\n    \n    def _get_frame_range(self, detections: List[Dict]) -> Tuple[int, int]:\n        \"\"\"\n        Get the frame range for a sequence of detections\n        \n        Args:\n            detections (list): List of detection dictionaries\n            \n        Returns:\n            tuple: (min_frame, max_frame)\n        \"\"\"\n        frame_numbers = [d['frame_number'] for d in detections]\n        return (min(frame_numbers), max(frame_numbers))\n    \n    def _has_sufficient_temporal_overlap(self, range1: Tuple[int, int], \n                                        range2: Tuple[int, int]) -> bool:\n        \"\"\"\n        Check if two frame ranges have sufficient temporal overlap\n        \n        Args:\n            range1 (tuple): First frame range (min, max)\n            range2 (tuple): Second frame range (min, max)\n            \n        Returns:\n            bool: True if ranges overlap sufficiently\n        \"\"\"\n        min1, max1 = range1\n        min2, max2 = range2\n        \n        # Calculate overlap\n        overlap_start = max(min1, min2)\n        overlap_end = min(max1, max2)\n        \n        if overlap_start > overlap_end:\n            return False\n        \n        overlap_duration = overlap_end - overlap_start\n        \n        # Calculate minimum required overlap based on shortest range\n        range1_duration = max1 - min1\n        range2_duration = max2 - min2\n        min_duration = min(range1_duration, range2_duration)\n        \n        # Check if overlap is sufficient\n        if min_duration == 0:\n            return False\n        \n        overlap_ratio = overlap_duration / min_duration\n        return overlap_ratio >= self.min_temporal_overlap\n    \n    def _are_spatially_close(self, detections1: List[Dict], \n                            detections2: List[Dict]) -> bool:\n        \"\"\"\n        Check if two sets of detections are spatially close (in the same region)\n        \n        Args:\n            detections1 (list): First set of detections\n            detections2 (list): Second set of detections\n            \n        Returns:\n            bool: True if detections are spatially close\n        \"\"\"\n        # Calculate average positions\n        avg_x1 = np.mean([d['centroid_x'] for d in detections1])\n        avg_y1 = np.mean([d['centroid_y'] for d in detections1])\n        \n        avg_x2 = np.mean([d['centroid_x'] for d in detections2])\n        avg_y2 = np.mean([d['centroid_y'] for d in detections2])\n        \n        # Calculate distance between average positions\n        distance = math.sqrt((avg_x2 - avg_x1)**2 + (avg_y2 - avg_y1)**2)\n        \n        return distance <= self.max_spatial_distance\n    \n    def _has_sustained_comovement(self, group: List[int], clips_data: Dict) -> bool:\n        \"\"\"\n        Verify that a group has sustained co-movement by checking actual overlapping frames\n        \n        Args:\n            group (list): List of clip IDs in the group\n            clips_data (dict): Dictionary mapping clip IDs to their detections\n            \n        Returns:\n            bool: True if group has sustained co-movement\n        \"\"\"\n        if len(group) < 2:\n            return False\n        \n        # Get actual frame sets for each group member\n        member_frame_sets = []\n        for clip_id in group:\n            frames = set(d['frame_number'] for d in clips_data[clip_id])\n            member_frame_sets.append(frames)\n        \n        # Since stars move slowly, detections won't be at exactly the same frames\n        # Instead, we'll check if detections are close in time (within a tolerance)\n        # Build a time window approach: for each frame where ANY member is detected,\n        # check if ALL members have detections within a small window around it\n        \n        frame_tolerance = 5  # Allow 5 frames of tolerance for detection timing\n        \n        # Get all unique frames where any member is detected\n        all_frames = set()\n        for frames in member_frame_sets:\n            all_frames.update(frames)\n        \n        if not all_frames:\n            return False\n        \n        # Find frames where all members are \"nearby\" (within tolerance)\n        cooccurring_frames = []\n        \n        for target_frame in sorted(all_frames):\n            all_members_present = True\n            \n            for member_frames in member_frame_sets:\n                # Check if this member has a detection within tolerance of target_frame\n                has_nearby_detection = any(\n                    abs(f - target_frame) <= frame_tolerance \n                    for f in member_frames\n                )\n                \n                if not has_nearby_detection:\n                    all_members_present = False\n                    break\n            \n            if all_members_present:\n                cooccurring_frames.append(target_frame)\n        \n        if not cooccurring_frames:\n            return False\n        \n        # Get FPS from first detection (assuming all have same FPS)\n        fps = clips_data[group[0]][0].get('fps', 30)\n        \n        # Check if we have enough co-occurring frames (at least 1 second worth)\n        min_required_cooccur_count = int(fps * 0.5)  # At least 0.5 seconds of actual co-occurrences\n        \n        if len(cooccurring_frames) < min_required_cooccur_count:\n            return False\n        \n        # Also check the time span to ensure detections cover at least 1 second\n        min_cooccur_frame = min(cooccurring_frames)\n        max_cooccur_frame = max(cooccurring_frames)\n        cooccur_span_frames = max_cooccur_frame - min_cooccur_frame + 1\n        \n        min_required_span = fps * 1.0\n        \n        return cooccur_span_frames >= min_required_span\n    \n    def _calculate_velocity_vector(self, detections: List[Dict]) -> Tuple[float, float, float]:\n        \"\"\"\n        Calculate average velocity vector for a sequence of detections\n        \n        Args:\n            detections (list): List of detection dictionaries for a single object\n            \n        Returns:\n            tuple: (vx, vy, speed) or None if cannot calculate\n        \"\"\"\n        if len(detections) < 2:\n            return None\n        \n        # Sort by frame number\n        detections = sorted(detections, key=lambda x: x['frame_number'])\n        \n        # Calculate velocity vectors between consecutive frames\n        velocities = []\n        \n        for i in range(1, len(detections)):\n            dx = detections[i]['centroid_x'] - detections[i-1]['centroid_x']\n            dy = detections[i]['centroid_y'] - detections[i-1]['centroid_y']\n            dt = detections[i]['frame_number'] - detections[i-1]['frame_number']\n            \n            if dt > 0:\n                vx = dx / dt\n                vy = dy / dt\n                velocities.append((vx, vy))\n        \n        if not velocities:\n            return None\n        \n        # Calculate average velocity\n        avg_vx = np.mean([v[0] for v in velocities])\n        avg_vy = np.mean([v[1] for v in velocities])\n        avg_speed = math.sqrt(avg_vx**2 + avg_vy**2)\n        \n        # Filter out very slow motion (but keep threshold low for stars)\n        if avg_speed < self.min_speed:\n            return None\n        \n        return (avg_vx, avg_vy, avg_speed)\n    \n    def _are_velocities_similar(self, vel1: Tuple[float, float, float], \n                                vel2: Tuple[float, float, float]) -> bool:\n        \"\"\"\n        Check if two velocity vectors are similar (same speed and direction)\n        \n        Args:\n            vel1 (tuple): First velocity vector (vx, vy, speed)\n            vel2 (tuple): Second velocity vector (vx, vy, speed)\n            \n        Returns:\n            bool: True if velocities are similar\n        \"\"\"\n        vx1, vy1, speed1 = vel1\n        vx2, vy2, speed2 = vel2\n        \n        # Check if speeds are similar (within tolerance)\n        avg_speed = (speed1 + speed2) / 2\n        if avg_speed == 0:\n            return False\n        \n        speed_diff = abs(speed1 - speed2) / avg_speed\n        if speed_diff > self.velocity_tolerance:\n            return False\n        \n        # Check if directions are similar\n        angle1 = math.atan2(vy1, vx1) * 180 / math.pi\n        angle2 = math.atan2(vy2, vx2) * 180 / math.pi\n        \n        # Calculate angular difference (handle wrap-around at 360 degrees)\n        angle_diff = abs(angle1 - angle2)\n        if angle_diff > 180:\n            angle_diff = 360 - angle_diff\n        \n        if angle_diff > self.direction_tolerance:\n            return False\n        \n        return True\n    \n    def get_star_field_info(self, metadata: List[Dict], star_clip_ids: Set[int]) -> Dict:\n        \"\"\"\n        Get information about the detected star field\n        \n        Args:\n            metadata (list): List of detection metadata dictionaries\n            star_clip_ids (set): Set of clip IDs classified as stars\n            \n        Returns:\n            dict: Information about the star field including average velocity and count\n        \"\"\"\n        if not star_clip_ids:\n            return {\n                'star_count': 0,\n                'avg_velocity_x': 0,\n                'avg_velocity_y': 0,\n                'avg_speed': 0,\n                'direction_deg': 0\n            }\n        \n        # Group metadata by clip_id\n        clips_data = defaultdict(list)\n        for item in metadata:\n            clips_data[item['clip_id']].append(item)\n        \n        # Calculate velocities for all stars\n        velocities = []\n        for clip_id in star_clip_ids:\n            if clip_id in clips_data:\n                vel = self._calculate_velocity_vector(clips_data[clip_id])\n                if vel is not None:\n                    velocities.append(vel)\n        \n        if not velocities:\n            return {\n                'star_count': len(star_clip_ids),\n                'avg_velocity_x': 0,\n                'avg_velocity_y': 0,\n                'avg_speed': 0,\n                'direction_deg': 0\n            }\n        \n        # Calculate average star field motion\n        avg_vx = np.mean([v[0] for v in velocities])\n        avg_vy = np.mean([v[1] for v in velocities])\n        avg_speed = np.mean([v[2] for v in velocities])\n        direction_deg = math.atan2(avg_vy, avg_vx) * 180 / math.pi\n        \n        return {\n            'star_count': len(star_clip_ids),\n            'avg_velocity_x': float(avg_vx),\n            'avg_velocity_y': float(avg_vy),\n            'avg_speed': float(avg_speed),\n            'direction_deg': float(direction_deg)\n        }\n","size_bytes":15723},"object_tracker.py":{"content":"import numpy as np\nfrom scipy.spatial import distance as dist\nfrom collections import defaultdict\n\nclass ObjectTracker:\n    def __init__(self, max_disappeared=15, max_distance=50):\n        \"\"\"\n        Simple centroid-based object tracker\n        \n        Args:\n            max_disappeared (int): Maximum frames an object can be missing before deregistration\n            max_distance (float): Maximum distance to consider same object\n        \"\"\"\n        self.next_object_id = 1\n        self.objects = {}  # object_id -> centroid\n        self.disappeared = defaultdict(int)\n        self.max_disappeared = max_disappeared\n        self.max_distance = max_distance\n    \n    def register(self, centroid):\n        \"\"\"Register a new object with next available ID\"\"\"\n        self.objects[self.next_object_id] = centroid\n        self.disappeared[self.next_object_id] = 0\n        self.next_object_id += 1\n    \n    def deregister(self, object_id):\n        \"\"\"Remove an object from tracking\"\"\"\n        del self.objects[object_id]\n        del self.disappeared[object_id]\n    \n    def update(self, detections):\n        \"\"\"\n        Update tracker with new detections\n        \n        Args:\n            detections (list): List of (centroid_x, centroid_y) tuples\n            \n        Returns:\n            dict: Mapping of object_id to centroid for current frame\n        \"\"\"\n        # If no detections, mark all existing objects as disappeared\n        if len(detections) == 0:\n            for object_id in list(self.disappeared.keys()):\n                self.disappeared[object_id] += 1\n                \n                if self.disappeared[object_id] > self.max_disappeared:\n                    self.deregister(object_id)\n            \n            return {}\n        \n        # If no existing objects, register all detections\n        if len(self.objects) == 0:\n            for centroid in detections:\n                self.register(centroid)\n        else:\n            # Get current object IDs and centroids\n            object_ids = list(self.objects.keys())\n            object_centroids = list(self.objects.values())\n            \n            # Compute distance between each pair of object/detection centroids\n            D = dist.cdist(np.array(object_centroids), np.array(detections))\n            \n            # Find the smallest distance for each object\n            rows = D.min(axis=1).argsort()\n            cols = D.argmin(axis=1)[rows]\n            \n            used_rows = set()\n            used_cols = set()\n            \n            # Match objects to detections\n            for (row, col) in zip(rows, cols):\n                if row in used_rows or col in used_cols:\n                    continue\n                \n                # Check if distance is within threshold\n                if D[row, col] > self.max_distance:\n                    continue\n                \n                # Update object position\n                object_id = object_ids[row]\n                self.objects[object_id] = detections[col]\n                self.disappeared[object_id] = 0\n                \n                used_rows.add(row)\n                used_cols.add(col)\n            \n            # Handle disappeared objects\n            unused_rows = set(range(D.shape[0])) - used_rows\n            for row in unused_rows:\n                object_id = object_ids[row]\n                self.disappeared[object_id] += 1\n                \n                if self.disappeared[object_id] > self.max_disappeared:\n                    self.deregister(object_id)\n            \n            # Register new objects\n            unused_cols = set(range(D.shape[1])) - used_cols\n            for col in unused_cols:\n                self.register(detections[col])\n        \n        # Return current tracked objects\n        return self.objects.copy()\n","size_bytes":3784},"attached_assets/skyseer_pipeline_1760133847200.py":{"content":"# =======================================================================\r\n# SKYSEER AI PIPELINE: LOCAL PC EXECUTION\r\n# Executes all stages: Video Harvester, Feature Pre-processing, and K-Means Clustering.\r\n# Optimized for local CPU (like i7-7700) performance.\r\n# =======================================================================\r\n\r\nimport cv2\r\nimport os\r\nimport shutil\r\nimport numpy as np\r\nimport pandas as pd\r\nimport csv\r\nfrom collections import deque\r\nfrom sklearn.cluster import KMeans\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.ensemble import IsolationForest # Added for Stage 5\r\n\r\n# -----------------------------------------------------------------------\r\n# --- CONFIGURATION (EDIT THESE LINES) ----------------------------------\r\n# -----------------------------------------------------------------------\r\n\r\n# üö® REQUIRED: Set the exact path/name of your video file here.\r\n# NOTE: Using the long video name from your error message for now.\r\nVIDEO_PATH = \"long_night_sky.mp4\" \r\n\r\n# Set the number of groups the AI should find (3 groups: Meteor, Satellite, Outlier)\r\nNUMBER_OF_CLUSTERS = 3\r\n\r\n# --- GLOBAL VARIABLES --------------------------------------------------\r\n\r\nCLIPS_FOLDER = \"motion_clips\"\r\nFRAMES_FOLDER = \"motion_frames\"\r\nLOG_FILENAME = \"motion_metadata.csv\"\r\nSUMMARY_DATA_FILE = \"ai_summary_features.csv\"\r\nFINAL_OUTPUT_FILE = \"ai_classified_clips.csv\"\r\nDATA_FILES = [LOG_FILENAME, SUMMARY_DATA_FILE, FINAL_OUTPUT_FILE]\r\n\r\n# -----------------------------------------------------------------------\r\n# --- STAGE 1: FOLDER RESET & SETUP -------------------------------------\r\n# -----------------------------------------------------------------------\r\n\r\ndef reset_project_folders():\r\n    \"\"\"Clears old clips, folders, and data files for a clean run.\"\"\"\r\n    print(\"--- üóëÔ∏è STAGE 1: Starting Project Reset ---\")\r\n\r\n    # Delete existing folders\r\n    for folder in [CLIPS_FOLDER, FRAMES_FOLDER]:\r\n        if os.path.exists(folder):\r\n            shutil.rmtree(folder)\r\n            print(f\"‚úÖ Deleted existing folder: '{folder}'\")\r\n\r\n    # Delete existing data files\r\n    for filename in DATA_FILES:\r\n        if os.path.exists(filename):\r\n            os.remove(filename)\r\n            print(f\"‚úÖ Deleted old data file: '{filename}'\")\r\n    \r\n    # Recreate empty clips folder\r\n    os.makedirs(CLIPS_FOLDER, exist_ok=True)\r\n    print(f\"‚ûï Created new empty folder: '{CLIPS_FOLDER}'\")\r\n\r\n    print(\"--- Project Reset Complete. ---\")\r\n\r\n# -----------------------------------------------------------------------\r\n# --- STAGE 2: CLIP HARVESTER & RAW DATA LOGGER -------------------------\r\n# -----------------------------------------------------------------------\r\n\r\ndef run_harvester():\r\n    \"\"\"Processes video, saves clips, and logs raw frame-by-frame data.\"\"\"\r\n    print(\"\\n--- üé¨ STAGE 2: Running Harvester & Data Logger ---\")\r\n\r\n    # üö® FIX 1: Video Capture MUST be inside the function.\r\n    # üö® FIX 2: Use r\"\" and cv2.CAP_FFMPEG for robust reading on Windows/Anaconda.\r\n    cap = cv2.VideoCapture(r\"{}\".format(VIDEO_PATH), cv2.CAP_FFMPEG)\r\n\r\n    if not cap.isOpened():\r\n        raise ValueError(f\"ERROR: Video file '{VIDEO_PATH}' could not be opened. Check path or try a different video codec.\")\r\n\r\n    # Get video properties\r\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\r\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\r\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\r\n    \r\n    if fps == 0:\r\n        print(\"ERROR: Could not read video FPS. Check file integrity.\")\r\n        return\r\n\r\n    # MOG2 parameters for timelapse/noise\r\n    fgbg = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=64, detectShadows=False)\r\n\r\n    # --- Initialization ---\r\n    frame_count = 0\r\n    clip_index = 0\r\n    \r\n    FRAME_SKIP_RATE = 5      # Process 1 frame out of every 5 (80% speed boost)\r\n    min_frames_for_clip = int(fps * 1.0) # Minimum 1.0s clip length\r\n    pre_buffer_frames = fps\r\n    post_buffer_frames = fps\r\n    STAR_NOISE_FRAMES = 5    # Motion must be persistent for 5 frames to be logged\r\n    \r\n    pre_motion_buffer = deque(maxlen=pre_buffer_frames)\r\n    post_motion_buffer = deque()\r\n    \r\n    clip_frames_count = 0\r\n    motion_active = False\r\n    clip_writer = None\r\n    false_motion_count = 0\r\n\r\n    # AI Logging Setup\r\n    metadata_file = open(LOG_FILENAME, 'w', newline='')\r\n    csv_writer = csv.writer(metadata_file)\r\n    csv_writer.writerow([\"CLIP_ID\", \"FRAME_NUM\", \"CLIP_FRAME_COUNT\", \"MAX_AREA\", \r\n                         \"CENTROID_X\", \"CENTROID_Y\", \"ASPECT_RATIO\"])\r\n    current_clip_frames_data = [] \r\n\r\n    # Define Contour Filters\r\n    MIN_CONTOUR_AREA = 10\r\n    FRAME_AREA = frame_width * frame_height\r\n    MAX_CONTOUR_AREA = int(FRAME_AREA * 0.005) if FRAME_AREA > 0 else 10000 \r\n\r\n    # --- Main Loop ---\r\n    while True:\r\n        ret, frame = cap.read()\r\n        if not ret:\r\n            # Handle final clip cleanup on video end\r\n            if motion_active and clip_writer is not None:\r\n                if clip_frames_count >= min_frames_for_clip:\r\n                    for f in post_motion_buffer: clip_writer.write(f)\r\n                    csv_writer.writerows(current_clip_frames_data)\r\n                \r\n                # Release writer regardless of clip length\r\n                clip_writer.release()\r\n                if clip_frames_count < min_frames_for_clip:\r\n                     os.remove(clip_filename)\r\n            break\r\n\r\n        frame_count += 1\r\n        \r\n        # Frame skipping optimization\r\n        if frame_count % FRAME_SKIP_RATE != 0:\r\n            pre_motion_buffer.append(frame.copy())\r\n            if false_motion_count > 0:\r\n                false_motion_count = max(0, false_motion_count - 1)\r\n            continue\r\n\r\n        # Processing steps (MOG2, Blur, Dilation)\r\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n        gray_blur = cv2.GaussianBlur(gray, (5,5), 0)\r\n        fgmask = fgbg.apply(gray_blur)\r\n        _, thresh = cv2.threshold(fgmask, 127, 255, cv2.THRESH_BINARY)\r\n        kernel = np.ones((1,1),np.uint8) # FIX: Reduced kernel size\r\n        thresh = cv2.dilate(thresh, kernel, iterations=1)\r\n\r\n        # Contour Detection and Filtering\r\n        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\r\n        raw_motion_found = False \r\n        max_area_in_frame, best_centroid_x, best_centroid_y, best_aspect_ratio = 0, -1, -1, 0\r\n        valid_contours = []\r\n        \r\n        for c in contours:\r\n            area = cv2.contourArea(c)\r\n            if MIN_CONTOUR_AREA < area < MAX_CONTOUR_AREA:\r\n                raw_motion_found = True\r\n                valid_contours.append(c)\r\n                \r\n                if area > max_area_in_frame:\r\n                    max_area_in_frame = area\r\n                    M = cv2.moments(c)\r\n                    if M[\"m00\"] != 0:\r\n                        best_centroid_x = int(M[\"m10\"] / M[\"m00\"])\r\n                        best_centroid_y = int(M[\"m01\"] / M[\"m00\"])\r\n                    x, y, w, h = cv2.boundingRect(c)\r\n                    best_aspect_ratio = w / h if h > 0 else 0\r\n        \r\n        # Star Filter Logic\r\n        motion_detected = False\r\n        if raw_motion_found:\r\n            false_motion_count += 1\r\n            if false_motion_count >= STAR_NOISE_FRAMES:\r\n                motion_detected = True\r\n        elif not motion_active:\r\n            false_motion_count = 0 \r\n\r\n        # Log Data and Draw Rectangles\r\n        if motion_detected:\r\n            # Draw visuals\r\n            for c in valid_contours:\r\n                x, y, w, h = cv2.boundingRect(c)\r\n                pad = 8 \r\n                cv2.rectangle(frame, (max(0, x-pad), max(0, y-pad)),\r\n                              (min(frame.shape[1]-1, x+w+pad), min(frame.shape[0]-1, y+h+pad)),\r\n                              (0, 0, 255), 2)\r\n            \r\n            # Log raw data for the frame\r\n            current_clip_frames_data.append([\r\n                -1, frame_count, clip_frames_count + 1, max_area_in_frame,\r\n                best_centroid_x, best_centroid_y, best_aspect_ratio\r\n            ])\r\n            clip_frames_count += 1\r\n\r\n        # Clip Writing Logic\r\n        if motion_detected:\r\n            if not motion_active:\r\n                clip_index += 1\r\n                clip_filename = os.path.join(CLIPS_FOLDER, f\"clip_{clip_index:04d}.mp4\")\r\n                \r\n                for row in current_clip_frames_data: row[0] = clip_index \r\n\r\n                clip_writer = cv2.VideoWriter(clip_filename, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame.shape[1], frame.shape[0]))\r\n                motion_active = True\r\n                for f in pre_motion_buffer: clip_writer.write(f)\r\n\r\n            clip_writer.write(frame)\r\n            post_motion_count = 0\r\n            post_motion_buffer.clear()\r\n\r\n        else:\r\n            # Motion ended or not persistent\r\n            if motion_active:\r\n                post_motion_buffer.append(frame.copy())\r\n                post_motion_count += 1\r\n                \r\n                if post_motion_count >= post_buffer_frames:\r\n                    # Clip is ending, log data regardless of length\r\n                    csv_writer.writerows(current_clip_frames_data)\r\n                    \r\n                    if clip_frames_count < min_frames_for_clip:\r\n                        clip_writer.release()\r\n                        os.remove(clip_filename)\r\n                        clip_index -= 1\r\n                    else:\r\n                        for f in post_motion_buffer: clip_writer.write(f)\r\n                        clip_writer.release()\r\n\r\n                    clip_writer = None\r\n                    motion_active = False\r\n                    clip_frames_count = 0\r\n                    post_motion_buffer.clear()\r\n                    current_clip_frames_data.clear()\r\n            else:\r\n                 current_clip_frames_data.clear()\r\n\r\n        pre_motion_buffer.append(frame.copy())\r\n\r\n\r\n    cap.release()\r\n    metadata_file.close() \r\n    print(\"--- Harvester Complete! ---\")\r\n    print(f\"Total clips generated: {clip_index}\")\r\n\r\n\r\n# -----------------------------------------------------------------------\r\n# --- STAGE 3: AI FEATURE PRE-PROCESSING --------------------------------\r\n# -----------------------------------------------------------------------\r\n\r\ndef run_pre_processor():\r\n    \"\"\"Calculates Avg Speed, Consistency, and Duration per clip from raw data.\"\"\"\r\n    print(\"\\n--- üß† STAGE 3: Running AI Feature Pre-processing ---\")\r\n\r\n    if not os.path.exists(LOG_FILENAME):\r\n        print(f\"ERROR: Raw data file '{LOG_FILENAME}' not found. Check Stage 2 output.\")\r\n        return\r\n\r\n    df = pd.read_csv(LOG_FILENAME)\r\n    if df.empty or len(df[df['CLIP_ID'] != -1]) == 0:\r\n         print(\"ERROR: Loaded 0 valid data points. Cannot proceed.\")\r\n         return\r\n         \r\n    df = df[df['CENTROID_X'] != -1].copy()\r\n    \r\n    # Calculate Frame-to-Frame Metrics (Speed)\r\n    df['PREV_X'] = df.groupby('CLIP_ID')['CENTROID_X'].shift(1)\r\n    df['PREV_Y'] = df.groupby('CLIP_ID')['CENTROID_Y'].shift(1)\r\n    df['PREV_X'] = df['PREV_X'].fillna(df['CENTROID_X'])\r\n    df['PREV_Y'] = df['PREV_Y'].fillna(df['CENTROID_Y'])\r\n    \r\n    df['SPEED'] = np.sqrt(\r\n        (df['CENTROID_X'] - df['PREV_X'])**2 + \r\n        (df['CENTROID_Y'] - df['PREV_Y'])**2\r\n    )\r\n    \r\n    # Aggregate Features Per Clip\r\n    summary_df = df.groupby('CLIP_ID').agg(\r\n        Total_Duration_Frames=('CLIP_FRAME_COUNT', 'max'),\r\n        Avg_Speed_Pixel_Per_Frame=('SPEED', 'mean'),\r\n        Speed_Consistency_STD=('SPEED', 'std'),\r\n        Max_Area_Overall=('MAX_AREA', 'max'),\r\n        Avg_Aspect_Ratio=('ASPECT_RATIO', 'mean'), \r\n        Max_Aspect_Ratio=('ASPECT_RATIO', 'max'),\r\n    ).reset_index()\r\n\r\n    summary_df['Speed_Consistency_STD'] = summary_df['Speed_Consistency_STD'].fillna(0)\r\n    \r\n    summary_df.to_csv(SUMMARY_DATA_FILE, index=False)\r\n    \r\n    print(f\"--- Pre-processing Complete! Generated {len(summary_df)} summary rows. ---\")\r\n\r\n\r\n# -----------------------------------------------------------------------\r\n# --- STAGE 4: K-MEANS CLUSTERING ---------------------------------------\r\n# -----------------------------------------------------------------------\r\n\r\ndef run_kmeans_clustering():\r\n    \"\"\"Uses K-Means to automatically group clips into primary motion categories.\"\"\"\r\n    print(\"\\n--- ü§ñ STAGE 4: Running K-Means Clustering ---\")\r\n\r\n    if not os.path.exists(SUMMARY_DATA_FILE):\r\n        print(f\"ERROR: Summary data file '{SUMMARY_DATA_FILE}' not found. Check Stage 3 output.\")\r\n        return\r\n\r\n    df = pd.read_csv(SUMMARY_DATA_FILE)\r\n    if df.empty:\r\n        print(\"ERROR: Summary file is empty. Cannot run clustering.\")\r\n        return\r\n\r\n    # Select Features\r\n    feature_columns = [\r\n        'Avg_Speed_Pixel_Per_Frame',\r\n        'Total_Duration_Frames',\r\n        'Speed_Consistency_STD',\r\n        'Max_Area_Overall',\r\n        'Max_Aspect_Ratio'\r\n    ]\r\n    \r\n    X = df[feature_columns].values \r\n\r\n    # Data Scaling (Crucial)\r\n    scaler = StandardScaler()\r\n    X_scaled = scaler.fit_transform(X)\r\n    \r\n    # K-Means Implementation\r\n    kmeans = KMeans(n_clusters=NUMBER_OF_CLUSTERS, random_state=42, n_init=10)\r\n    df['CLUSTER_ID'] = kmeans.fit_predict(X_scaled)\r\n    \r\n    # Analyze Cluster Centers\r\n    cluster_centers = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=feature_columns)\r\n    cluster_centers['CLUSTER_ID'] = range(NUMBER_OF_CLUSTERS)\r\n    \r\n    avg_speed_global = df['Avg_Speed_Pixel_Per_Frame'].mean()\r\n    avg_duration_global = df['Total_Duration_Frames'].mean()\r\n    \r\n    cluster_map = {}\r\n    for index, row in cluster_centers.iterrows():\r\n        if row['Avg_Speed_Pixel_Per_Frame'] > 1.5 * avg_speed_global and row['Total_Duration_Frames'] < avg_duration_global:\r\n            cluster_map[row['CLUSTER_ID']] = '1_METEOR_EVENT'\r\n        elif row['Total_Duration_Frames'] > 1.5 * avg_duration_global and row['Max_Area_Overall'] > 500: # Area heuristic for planes\r\n            cluster_map[row['CLUSTER_ID']] = '3_PLANE_OR_JUNK'\r\n        else:\r\n             cluster_map[row['CLUSTER_ID']] = '2_SATELLITE_ORBIT'\r\n\r\n    df['CLASSIFICATION'] = df['CLUSTER_ID'].map(cluster_map)\r\n    df.to_csv(FINAL_OUTPUT_FILE, index=False)\r\n    \r\n    print(\"--- K-Means Clustering Complete! ---\")\r\n    print(f\"File saved to: {FINAL_OUTPUT_FILE}\")\r\n    print(\"\\nReview the cluster centers table for initial grouping insights:\")\r\n    print(cluster_centers)\r\n\r\n# -----------------------------------------------------------------------\r\n# --- STAGE 5: ISOLATION FOREST ANOMALY DETECTION -----------------------\r\n# -----------------------------------------------------------------------\r\n\r\ndef run_anomaly_detection():\r\n    \"\"\"\r\n    Loads classified clip features and uses Isolation Forest to identify clips \r\n    that are statistical outliers (potential UAPs or rare events).\r\n    \"\"\"\r\n    print(\"\\n--- ü§ñ STAGE 5: Running Isolation Forest Anomaly Detection ---\")\r\n\r\n    if not os.path.exists(FINAL_OUTPUT_FILE):\r\n        print(f\"ERROR: Classified data file '{FINAL_OUTPUT_FILE}' not found. Check Stage 4 output.\")\r\n        return\r\n\r\n    # 1. Load Classified Data (from K-Means output)\r\n    df = pd.read_csv(FINAL_OUTPUT_FILE)\r\n    if df.empty:\r\n        print(\"ERROR: Classified file is empty. Cannot run anomaly detection.\")\r\n        return\r\n\r\n    # 2. Select Features for Anomaly Detection (same features used for clustering)\r\n    feature_columns = [\r\n        'Avg_Speed_Pixel_Per_Frame', \r\n        'Total_Duration_Frames',     \r\n        'Speed_Consistency_STD',     \r\n        'Max_Area_Overall',          \r\n        'Max_Aspect_Ratio'\r\n    ]\r\n    \r\n    X = df[feature_columns].values \r\n\r\n    # 3. Data Scaling\r\n    scaler = StandardScaler()\r\n    X_scaled = scaler.fit_transform(X)\r\n    \r\n    # 4. Implement Isolation Forest\r\n    # contamination=0.01 expects 1% of the data to be outliers (UAPs/rare events).\r\n    model = IsolationForest(contamination=0.01, random_state=42) \r\n    \r\n    # Fit the model and predict the anomalies\r\n    # -1 = Anomaly (Outlier), 1 = Inlier (Normal/Common)\r\n    df['IS_ANOMALY'] = model.fit_predict(X_scaled)\r\n    \r\n    # 5. Add Final Classification Flag\r\n    # If it's an anomaly, it overrides the K-Means classification for human review.\r\n    df['FINAL_CLASSIFICATION'] = np.where(\r\n        df['IS_ANOMALY'] == -1, '0_ANOMALY_UAP_REVIEW', df['CLASSIFICATION']\r\n    )\r\n\r\n    # 6. Save Final Data\r\n    df.to_csv(FINAL_OUTPUT_FILE, index=False)\r\n    \r\n    anomaly_count = len(df[df['IS_ANOMALY'] == -1])\r\n    \r\n    print(\"--- Isolation Forest Analysis Complete! ---\")\r\n    print(f\"Total ANOMALIES flagged for review: {anomaly_count}\")\r\n    print(f\"Final classified data saved to: {FINAL_OUTPUT_FILE}\")\r\n\r\n# -----------------------------------------------------------------------\r\n# --- STAGE 6: AUTOMATED CLIP ORGANIZATION ------------------------------\r\n# -----------------------------------------------------------------------\r\n\r\ndef organize_clips():\r\n    \"\"\"Moves clips into folders based on the final AI classification.\"\"\"\r\n    print(\"\\n--- üìÇ STAGE 6: Organizing Clips into Folders ---\")\r\n\r\n    if not os.path.exists(FINAL_OUTPUT_FILE):\r\n        print(f\"ERROR: Final data file '{FINAL_OUTPUT_FILE}' not found.\")\r\n        return\r\n\r\n    df = pd.read_csv(FINAL_OUTPUT_FILE)\r\n    base_clip_folder = CLIPS_FOLDER # 'motion_clips'\r\n\r\n    if not os.path.exists(base_clip_folder):\r\n         print(f\"ERROR: Base clips folder '{base_clip_folder}' not found. Did Stage 2 run?\")\r\n         return\r\n\r\n    # Process each clip in the final dataset\r\n    for index, row in df.iterrows():\r\n        clip_id = row['CLIP_ID']\r\n        classification = row['FINAL_CLASSIFICATION']\r\n        \r\n        # 1. Determine the source and destination paths\r\n        source_filename = f\"clip_{clip_id:04d}.mp4\"\r\n        # NOTE: os.path.join handles differences between Windows (\\\\) and Linux (/)\r\n        source_path = os.path.join(base_clip_folder, source_filename)\r\n        \r\n        destination_folder = os.path.join(base_clip_folder, classification)\r\n        destination_path = os.path.join(destination_folder, source_filename)\r\n        \r\n        # 2. Create the destination folder if it doesn't exist\r\n        os.makedirs(destination_folder, exist_ok=True)\r\n\r\n        # 3. Move the file\r\n        if os.path.exists(source_path):\r\n            try:\r\n                shutil.move(source_path, destination_path)\r\n            except Exception as e:\r\n                print(f\"Warning: Could not move {source_filename}. Error: {e}\")\r\n        \r\n    print(\"--- File Organization Complete! ---\")\r\n    print(f\"Clips are now sorted into subfolders under: '{base_clip_folder}'\")\r\n\r\n\r\n# =======================================================================\r\n# --- EXECUTION FLOW ----------------------------------------------------\r\n# =======================================================================\r\n\r\nif __name__ == \"__main__\":\r\n    try:\r\n        # Check if the mandatory video path is set\r\n        if VIDEO_PATH == \"your_video_file_name_here.mp4\":\r\n            print(\"\\nüö® ERROR: Please update the 'VIDEO_PATH' variable at the top of the script.\")\r\n        else:\r\n            reset_project_folders()\r\n            run_harvester()\r\n            run_pre_processor()\r\n            run_kmeans_clustering()\r\n            run_anomaly_detection() \r\n            organize_clips()\r\n            print(\"\\nüéâ AI Pipeline complete! Your classified results are in 'ai_classified_clips.csv'.\")\r\n    except Exception as e:\r\n        print(f\"\\n‚ùå A fatal error occurred during execution: {e}\")\r\n        print(\"Please check the video path, file existence, and ensure all required libraries (OpenCV, pandas, scikit-learn) are installed via Anaconda/pip.\")\r\n","size_bytes":19735},".streamlit/config.toml":{"content":"[server]\nheadless = true\naddress = \"0.0.0.0\"\nport = 5000\nmaxUploadSize = 5000\nmaxMessageSize = 1000\nenableXsrfProtection = false\nenableCORS = false\n# Increase timeout for processing large/long videos\nrunOnSave = false\nfileWatcherType = \"none\"\n\n[browser]\nserverAddress = \"0.0.0.0\"\nserverPort = 5000\ngatherUsageStats = false\n\n[client]\nshowErrorDetails = false\ntoolbarMode = \"minimal\"\n\n[theme]\nbase = \"dark\"\nprimaryColor = \"#1f77b4\"\nbackgroundColor = \"#0e1117\"\nsecondaryBackgroundColor = \"#262730\"\ntextColor = \"#fafafa\"\n","size_bytes":517},"attached_assets/video_processor_1760568288736.py":{"content":"import cv2\nimport os\nimport numpy as np\nfrom collections import deque, defaultdict\nimport csv\nfrom datetime import datetime\nfrom object_tracker import ObjectTracker\n\nclass VideoProcessor:\n    def __init__(self, sensitivity=5, min_duration=1.0, max_duration=None, frame_skip=3):\n        \"\"\"\n        Initialize video processor with configurable parameters\n        \n        Args:\n            sensitivity (int): Motion detection sensitivity (1-10)\n            min_duration (float): Minimum clip duration in seconds\n            max_duration (float): Maximum clip duration in seconds (None = no limit)\n            frame_skip (int): Process every Nth frame for performance\n        \"\"\"\n        self.sensitivity = sensitivity\n        self.min_duration = min_duration\n        self.max_duration = max_duration\n        self.frame_skip = frame_skip\n        \n        # Convert sensitivity to contour area threshold\n        # Higher sensitivity = lower threshold (detects smaller objects)\n        # Adjusted to better detect small satellites that don't cross entire screen\n        self.min_contour_area = max(5, 70 - (sensitivity * 6))\n        \n    def process_video(self, video_path, progress_callback=None):\n        \"\"\"\n        Process video to detect and track individual objects\n        \n        Args:\n            video_path (str): Path to input video file\n            progress_callback (callable): Optional callback function(current, total) for progress updates\n            \n        Returns:\n            tuple: (list of clip paths, metadata list)\n        \"\"\"\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            raise ValueError(f\"Could not open video file: {video_path}\")\n        \n        # Get video properties\n        fps = int(cap.get(cv2.CAP_PROP_FPS)) or 30\n        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0\n        \n        # Setup MOG2 background subtractor for better motion detection\n        # Balanced varThreshold to detect small satellites while minimizing false positives\n        backSub = cv2.createBackgroundSubtractorMOG2(\n            history=500,\n            varThreshold=60,  # Balanced threshold - detects smaller satellites\n            detectShadows=False\n        )\n        \n        # Initialize object tracker\n        # Account for frame skipping when calculating max_disappeared\n        # We want 2 seconds of real time, but we only process every frame_skip frames\n        # So: max_disappeared = (fps / frame_skip) * desired_seconds\n        max_disappeared_frames = int((fps / self.frame_skip) * 2.0)  # 2 seconds of tracked frames\n        tracker = ObjectTracker(max_disappeared=max_disappeared_frames, max_distance=150)\n        \n        # Metadata collection per object\n        object_metadata = defaultdict(list)\n        video_fps = fps\n        \n        # Results storage\n        clips_folder = \"processed_clips\"\n        os.makedirs(clips_folder, exist_ok=True)\n        \n        # Maximum contour area to filter out large objects (clouds, etc.)\n        max_contour_area = int(frame_width * frame_height * 0.005)\n        \n        frame_count = 0\n        motion_active = False\n        clip_writer = None\n        clip_filename = None\n        \n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                # Close video writer if active\n                if clip_writer is not None:\n                    clip_writer.release()\n                break\n            \n            frame_count += 1\n            \n            # Frame skipping for performance\n            if frame_count % self.frame_skip != 0:\n                continue\n            \n            # Send progress updates every 30 PROCESSED frames to keep connection alive\n            # (every 30 * frame_skip actual frames)\n            if progress_callback and frame_count % (30 * self.frame_skip) == 0:\n                if total_frames > 0:\n                    progress_callback(frame_count, total_frames)\n                else:\n                    # If total_frames unknown, estimate growing total for monotonic progress\n                    # Use current_frame * 1.5 so progress gradually increases (66% -> 70% -> 75%...)\n                    estimated_total = int(frame_count * 1.5) + 1000  # +1000 to start slower\n                    progress_callback(frame_count, estimated_total)\n            \n            # Motion detection pipeline\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            gray_blur = cv2.GaussianBlur(gray, (5, 5), 0)\n            \n            # Background subtraction\n            fgMask = backSub.apply(gray_blur)\n            \n            # Morphological operations to clean up the mask\n            kernel = np.ones((3, 3), np.uint8)\n            fgMask = cv2.morphologyEx(fgMask, cv2.MORPH_OPEN, kernel)\n            fgMask = cv2.morphologyEx(fgMask, cv2.MORPH_CLOSE, kernel)\n            \n            # Find contours\n            contours, _ = cv2.findContours(fgMask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            \n            # Detect objects and extract their properties\n            current_frame_objects = []\n            current_frame_centroids = []\n            \n            for contour in contours:\n                area = cv2.contourArea(contour)\n                \n                # Filter by area (sensitivity-based)\n                if self.min_contour_area < area < max_contour_area:\n                    # Extract object properties\n                    x, y, w, h = cv2.boundingRect(contour)\n                    \n                    # Calculate centroid\n                    M = cv2.moments(contour)\n                    if M[\"m00\"] != 0:\n                        centroid_x = int(M[\"m10\"] / M[\"m00\"])\n                        centroid_y = int(M[\"m01\"] / M[\"m00\"])\n                    else:\n                        centroid_x, centroid_y = x + w//2, y + h//2\n                    \n                    # Calculate aspect ratio\n                    aspect_ratio = w / h if h > 0 else 0\n                    \n                    # Calculate brightness (mean intensity in bounding box)\n                    roi = gray[y:y+h, x:x+w]\n                    mean_brightness = np.mean(roi) if roi.size > 0 else 0\n                    max_brightness = np.max(roi) if roi.size > 0 else 0\n                    \n                    # Store object data (without clip_id yet)\n                    object_data = {\n                        'frame_number': frame_count,\n                        'area': area,\n                        'centroid_x': centroid_x,\n                        'centroid_y': centroid_y,\n                        'bbox_x': x,\n                        'bbox_y': y,\n                        'bbox_width': w,\n                        'bbox_height': h,\n                        'aspect_ratio': aspect_ratio,\n                        'mean_brightness': mean_brightness,\n                        'max_brightness': max_brightness,\n                        'fps': video_fps\n                    }\n                    current_frame_objects.append(object_data)\n                    current_frame_centroids.append((centroid_x, centroid_y))\n            \n            # Update tracker with detected centroids\n            tracked_objects = tracker.update(current_frame_centroids)\n            \n            # Assign object_id (as clip_id) to each detection\n            if len(tracked_objects) > 0 and len(current_frame_objects) > 0:\n                # Match detections to tracked objects by centroid\n                for i, (centroid_x, centroid_y) in enumerate(current_frame_centroids):\n                    # Find closest tracked object\n                    min_dist = float('inf')\n                    assigned_id = None\n                    \n                    for obj_id, (tx, ty) in tracked_objects.items():\n                        dist = np.sqrt((centroid_x - tx)**2 + (centroid_y - ty)**2)\n                        if dist < min_dist:\n                            min_dist = dist\n                            assigned_id = obj_id\n                    \n                    if assigned_id is not None:\n                        # Add clip_id and store metadata\n                        current_frame_objects[i]['clip_id'] = assigned_id\n                        object_metadata[assigned_id].append(current_frame_objects[i])\n                        \n                        # Draw bounding rectangle with object ID\n                        obj = current_frame_objects[i]\n                        x, y, w, h = obj['bbox_x'], obj['bbox_y'], obj['bbox_width'], obj['bbox_height']\n                        pad = 8\n                        cv2.rectangle(frame, \n                                    (max(0, x-pad), max(0, y-pad)),\n                                    (min(frame_width-1, x+w+pad), min(frame_height-1, y+h+pad)),\n                                    (0, 0, 255), 2)\n                        cv2.putText(frame, f\"ID:{assigned_id}\", (x, y-10),\n                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n            \n            # Start video writer if we have detections\n            if len(tracked_objects) > 0 and not motion_active:\n                motion_active = True\n                clip_filename = os.path.join(clips_folder, \"clip_0001.mp4\")\n                clip_writer = cv2.VideoWriter(\n                    clip_filename,\n                    cv2.VideoWriter_fourcc(*'mp4v'),\n                    fps,\n                    (frame_width, frame_height)\n                )\n            \n            # Write frame if writer is active\n            if clip_writer is not None:\n                clip_writer.write(frame)\n        \n        cap.release()\n        \n        # Filter objects by duration (min and max)\n        # Account for frame skipping: we only get detections every frame_skip frames\n        min_detections = int((fps / self.frame_skip) * self.min_duration)\n        max_detections = int((fps / self.frame_skip) * self.max_duration) if self.max_duration else float('inf')\n        filtered_metadata = []\n        \n        for obj_id, detections in object_metadata.items():\n            num_detections = len(detections)\n            if min_detections <= num_detections <= max_detections:\n                filtered_metadata.extend(detections)\n        \n        # Prepare motion clips list\n        motion_clips = [clip_filename] if clip_filename and os.path.exists(clip_filename) else []\n        \n        return motion_clips, filtered_metadata\n","size_bytes":10533},"attached_assets/ml_classifier_1760631688049.py":{"content":"import pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nclass MLClassifier:\n    def __init__(self, n_clusters=3, random_state=42):\n        \"\"\"\n        Initialize ML classifier with K-Means clustering (simplified - no anomaly detection)\n        \n        Args:\n            n_clusters (int): Number of clusters for K-Means (default: 3 for Satellite/Meteor/Plane)\n            random_state (int): Random seed for reproducibility\n        \"\"\"\n        self.n_clusters = n_clusters\n        self.random_state = random_state\n        \n        # Initialize models (removed Isolation Forest)\n        self.scaler = StandardScaler()\n        self.kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n        \n        # Feature columns for ML processing (optimized for better accuracy)\n        self.feature_columns = [\n            'avg_speed', 'speed_consistency', 'duration', 'linearity',\n            'direction_changes', 'size_consistency', 'avg_acceleration',\n            'blinking_score',  # Added for better plane detection\n            'satellite_score', 'meteor_score', 'plane_score'\n        ]\n    \n    def classify_objects(self, features_df):\n        \"\"\"\n        Classify detected objects using unsupervised learning\n        \n        Args:\n            features_df (pd.DataFrame): DataFrame with extracted features\n            \n        Returns:\n            pd.DataFrame: DataFrame with classifications and confidence scores\n        \"\"\"\n        if features_df.empty:\n            return pd.DataFrame()\n        \n        # Step 0: Check for star groups first (highest priority)\n        # Stars are identified by group movement analysis\n        if 'is_star_group' in features_df.columns:\n            star_mask = features_df['is_star_group'] == 1\n            if star_mask.any():\n                # Classify stars separately and return\n                results_df = features_df.copy()\n                results_df['classification'] = ''\n                results_df['confidence'] = 0.0\n                results_df['cluster'] = 0\n                \n                # Classify stars\n                results_df.loc[star_mask, 'classification'] = 'Star'\n                results_df.loc[star_mask, 'confidence'] = 0.9\n                \n                # Classify non-stars using normal pipeline\n                non_star_df = features_df[~star_mask].copy()\n                if not non_star_df.empty and len(non_star_df) >= 2:\n                    non_star_results = self._classify_non_stars(non_star_df)\n                    for col in ['classification', 'confidence', 'cluster']:\n                        if col in non_star_results.columns:\n                            results_df.loc[~star_mask, col] = non_star_results[col].values\n                elif not non_star_df.empty:\n                    # Single non-star object, use rule-based\n                    single_result = self._classify_single_object(non_star_df)\n                    for col in ['classification', 'confidence', 'cluster']:\n                        if col in single_result.columns:\n                            results_df.loc[~star_mask, col] = single_result[col].values\n                \n                return results_df\n        \n        # No stars detected, use normal classification pipeline\n        return self._classify_non_stars(features_df)\n    \n    def _classify_non_stars(self, features_df):\n        \"\"\"Classify non-star objects using the normal ML pipeline\"\"\"\n        if features_df.empty:\n            return pd.DataFrame()\n        \n        # Prepare features for ML\n        X = self._prepare_features(features_df)\n        \n        if X.shape[0] < 2:\n            # Not enough data for clustering\n            return self._classify_single_object(features_df)\n        \n        # Step 1: K-Means Clustering for primary classification\n        # Adjust number of clusters based on available samples\n        n_samples = X.shape[0]\n        n_clusters = min(self.n_clusters, n_samples)\n        \n        X_scaled = self.scaler.fit_transform(X)\n        \n        # Only use KMeans if we have enough samples\n        if n_clusters >= 2:\n            kmeans = KMeans(n_clusters=n_clusters, random_state=self.random_state, n_init=10)\n            cluster_labels = kmeans.fit_predict(X_scaled)\n        else:\n            # Fall back to rule-based for single sample\n            return self._classify_single_object(features_df)\n        \n        # Create results dataframe\n        results_df = features_df.copy()\n        results_df['cluster'] = cluster_labels\n        \n        # Step 2: Interpret clusters and assign classifications\n        results_df = self._interpret_clusters(results_df)\n        \n        # Step 3: Calculate final confidence scores\n        results_df = self._calculate_confidence_scores(results_df)\n        \n        return results_df\n    \n    def _prepare_features(self, features_df):\n        \"\"\"Prepare feature matrix for ML algorithms\"\"\"\n        # Select relevant features and handle missing values\n        X = features_df[self.feature_columns].copy()\n        X = X.fillna(0)  # Fill NaN values with 0\n        \n        # Handle infinite values\n        X = X.replace([np.inf, -np.inf], 0)\n        \n        return X.values\n    \n    def _classify_single_object(self, features_df):\n        \"\"\"Handle classification when only one object is detected\"\"\"\n        results_df = features_df.copy()\n        \n        # Use rule-based classification for single objects\n        for idx, row in results_df.iterrows():\n            classification, confidence = self._rule_based_classification(row)\n            results_df.loc[idx, 'classification'] = classification\n            results_df.loc[idx, 'confidence'] = confidence\n            results_df.loc[idx, 'cluster'] = 0\n        \n        return results_df\n    \n    def _interpret_clusters(self, results_df):\n        \"\"\"Interpret K-Means clusters and assign object classifications\"\"\"\n        cluster_interpretations = {}\n        \n        # Get actual number of clusters from the data\n        n_actual_clusters = results_df['cluster'].nunique()\n        \n        for cluster_id in range(n_actual_clusters):\n            cluster_data = results_df[results_df['cluster'] == cluster_id]\n            \n            if cluster_data.empty:\n                cluster_interpretations[cluster_id] = ('Junk', 0.5)\n                continue\n            \n            # Analyze cluster characteristics\n            avg_speed = cluster_data['avg_speed'].mean()\n            avg_consistency = cluster_data['speed_consistency'].mean()\n            avg_linearity = cluster_data['linearity'].mean()\n            avg_duration = cluster_data['duration'].mean()\n            avg_satellite_score = cluster_data['satellite_score'].mean()\n            avg_meteor_score = cluster_data['meteor_score'].mean()\n            avg_plane_score = cluster_data['plane_score'].mean()\n            \n            # Determine most likely classification for this cluster\n            scores = {\n                'Satellite': avg_satellite_score,\n                'Meteor': avg_meteor_score,\n                'Plane': avg_plane_score,\n                'Junk': 0.1  # Base score for junk category\n            }\n            \n            # Apply minimal heuristics - mostly trust the base ML scores\n            # Only boost when there's VERY strong evidence\n            if avg_speed > 30 and avg_linearity > 0.85 and avg_duration < 1.5:\n                # Extremely fast, linear, brief -> Definitely Meteor\n                scores['Meteor'] *= 1.8\n            elif avg_consistency < 0.3 or avg_linearity < 0.3:\n                # Very poor quality trajectory -> Likely Junk\n                scores['Junk'] *= 1.5\n            \n            # Select best classification\n            best_class = max(scores, key=scores.get)\n            confidence = min(scores[best_class], 0.95)  # Cap confidence at 95%\n            \n            cluster_interpretations[cluster_id] = (best_class, confidence)\n        \n        # Assign classifications based on cluster interpretations\n        results_df['classification'] = ''\n        results_df['confidence'] = 0.0\n        \n        for idx, row in results_df.iterrows():\n            cluster_id = row['cluster']\n            classification, confidence = cluster_interpretations[cluster_id]\n            results_df.loc[idx, 'classification'] = classification\n            results_df.loc[idx, 'confidence'] = confidence\n        \n        return results_df\n    \n    def _rule_based_classification(self, row):\n        \"\"\"Rule-based classification for edge cases - trust the feature scores\"\"\"\n        # Use the pre-computed scores from feature_extractor\n        satellite_score = row.get('satellite_score', 0)\n        meteor_score = row.get('meteor_score', 0)\n        plane_score = row.get('plane_score', 0)\n        \n        # Determine best classification based on scores\n        scores = {\n            'Satellite': satellite_score,\n            'Meteor': meteor_score,\n            'Plane': plane_score,\n            'Junk': 0.1\n        }\n        \n        # Apply minimal adjustments for extreme cases\n        if row['avg_speed'] > 30 and row['linearity'] > 0.85 and row['duration'] < 1.5:\n            scores['Meteor'] *= 1.8\n        elif row['speed_consistency'] < 0.3 or row['linearity'] < 0.3:\n            scores['Junk'] *= 1.5\n        \n        # Select best classification\n        best_class = max(scores, key=scores.get)\n        confidence = min(scores[best_class], 0.85)  # Cap at 85% for single objects\n        \n        return best_class, confidence\n    \n    def _calculate_confidence_scores(self, results_df):\n        \"\"\"Calculate and refine confidence scores based on multiple factors\"\"\"\n        for idx, row in results_df.iterrows():\n            base_confidence = row.get('confidence', 0.5)\n            \n            # Adjust confidence based on detection count (with safe fallback)\n            detection_count = row.get('detection_count', 1)\n            if detection_count < 3:\n                base_confidence *= 0.8  # Lower confidence for few detections\n            elif detection_count > 10:\n                base_confidence = min(base_confidence * 1.1, 0.95)  # Higher confidence for many detections\n            \n            # Adjust confidence based on feature quality\n            linearity = row.get('linearity', 0)\n            speed_consistency = row.get('speed_consistency', 0)\n            \n            if linearity > 0.9 and speed_consistency > 0.8:\n                base_confidence = min(base_confidence * 1.2, 0.95)  # High quality trajectory\n            elif linearity < 0.3 or speed_consistency < 0.3:\n                base_confidence *= 0.7  # Poor quality trajectory\n            \n            results_df.loc[idx, 'confidence'] = round(base_confidence, 3)\n        \n        return results_df\n","size_bytes":10951},"attached_assets/feature_extractor_1760631680908.py":{"content":"import pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nimport math\nfrom star_detector import StarDetector\n\nclass FeatureExtractor:\n    def __init__(self):\n        \"\"\"Initialize feature extractor for motion analysis\"\"\"\n        self.star_detector = StarDetector()\n    \n    def extract_features(self, metadata):\n        \"\"\"\n        Extract movement features from motion detection metadata\n        \n        Args:\n            metadata (list): List of detection metadata dictionaries\n            \n        Returns:\n            pd.DataFrame: DataFrame with extracted features for each clip\n        \"\"\"\n        if not metadata:\n            return pd.DataFrame()\n        \n        # Detect star groups first\n        star_clip_ids = self.star_detector.detect_star_groups(metadata)\n        \n        # Group metadata by clip_id\n        clips_data = defaultdict(list)\n        for item in metadata:\n            clips_data[item['clip_id']].append(item)\n        \n        features_list = []\n        \n        for clip_id, clip_detections in clips_data.items():\n            if not clip_detections:\n                continue\n            \n            # Sort detections by frame number\n            clip_detections.sort(key=lambda x: x['frame_number'])\n            \n            # Get FPS from first detection (all should have same FPS)\n            video_fps = clip_detections[0].get('fps', 30) if clip_detections else 30\n            \n            # Extract basic trajectory features\n            features = self._extract_clip_features(clip_id, clip_detections, video_fps)\n            \n            # Add star detection feature\n            features['is_star_group'] = 1 if clip_id in star_clip_ids else 0\n            \n            features_list.append(features)\n        \n        return pd.DataFrame(features_list)\n    \n    def _extract_clip_features(self, clip_id, detections, fps=30):\n        \"\"\"\n        Extract comprehensive features for a single clip\n        \n        Args:\n            clip_id (int): Unique identifier for the clip\n            detections (list): List of detection dictionaries for this clip\n            fps (float): Video frame rate for accurate speed/duration calculations\n            \n        Returns:\n            dict: Dictionary of extracted features\n        \"\"\"\n        if len(detections) < 2:\n            return self._create_minimal_features(clip_id, detections, fps)\n        \n        # Extract position data\n        positions = [(d['centroid_x'], d['centroid_y']) for d in detections]\n        frame_numbers = [d['frame_number'] for d in detections]\n        areas = [d['area'] for d in detections]\n        aspect_ratios = [d['aspect_ratio'] for d in detections]\n        \n        # Extract brightness data\n        mean_brightness_values = [d.get('mean_brightness', 0) for d in detections]\n        max_brightness_values = [d.get('max_brightness', 0) for d in detections]\n        \n        # Calculate speeds between consecutive positions\n        speeds = []\n        for i in range(1, len(positions)):\n            dx = positions[i][0] - positions[i-1][0]\n            dy = positions[i][1] - positions[i-1][1]\n            distance = math.sqrt(dx*dx + dy*dy)\n            frame_diff = frame_numbers[i] - frame_numbers[i-1]\n            speed = distance / max(frame_diff, 1)  # pixels per frame\n            speeds.append(speed)\n        \n        # Calculate trajectory features\n        avg_speed = np.mean(speeds) if speeds else 0\n        speed_std = np.std(speeds) if len(speeds) > 1 else 0\n        max_speed = max(speeds) if speeds else 0\n        min_speed = min(speeds) if speeds else 0\n        \n        # Speed consistency (lower std = more consistent)\n        speed_consistency = 1.0 / (1.0 + speed_std) if speed_std > 0 else 1.0\n        \n        # Calculate total path length and duration\n        total_distance = sum([math.sqrt((positions[i][0] - positions[i-1][0])**2 + \n                                      (positions[i][1] - positions[i-1][1])**2) \n                            for i in range(1, len(positions))])\n        \n        duration = (frame_numbers[-1] - frame_numbers[0]) / float(fps)  # Use actual FPS\n        \n        # Trajectory linearity (straight line vs actual path)\n        if len(positions) >= 2:\n            start_pos = positions[0]\n            end_pos = positions[-1]\n            straight_line_distance = math.sqrt((end_pos[0] - start_pos[0])**2 + \n                                             (end_pos[1] - start_pos[1])**2)\n            linearity = straight_line_distance / max(total_distance, 1)\n        else:\n            linearity = 1.0\n        \n        # Direction changes (indicator of erratic movement)\n        direction_changes = 0\n        if len(positions) >= 3:\n            for i in range(1, len(positions) - 1):\n                # Calculate vectors\n                v1 = (positions[i][0] - positions[i-1][0], positions[i][1] - positions[i-1][1])\n                v2 = (positions[i+1][0] - positions[i][0], positions[i+1][1] - positions[i][1])\n                \n                # Calculate angle between vectors\n                dot_product = v1[0]*v2[0] + v1[1]*v2[1]\n                mag1 = math.sqrt(v1[0]**2 + v1[1]**2)\n                mag2 = math.sqrt(v2[0]**2 + v2[1]**2)\n                \n                if mag1 > 0 and mag2 > 0:\n                    cos_angle = dot_product / (mag1 * mag2)\n                    cos_angle = max(-1, min(1, cos_angle))  # Clamp to valid range\n                    angle = math.acos(cos_angle)\n                    \n                    # Count significant direction changes (> 30 degrees)\n                    if angle > math.pi / 6:\n                        direction_changes += 1\n        \n        # Object size statistics\n        avg_area = np.mean(areas)\n        area_std = np.std(areas) if len(areas) > 1 else 0\n        max_area = max(areas)\n        min_area = min(areas)\n        \n        # Size consistency\n        size_consistency = 1.0 / (1.0 + area_std / max(avg_area, 1))\n        \n        # Aspect ratio statistics\n        avg_aspect_ratio = np.mean(aspect_ratios)\n        aspect_ratio_std = np.std(aspect_ratios) if len(aspect_ratios) > 1 else 0\n        \n        # Acceleration analysis\n        accelerations = []\n        if len(speeds) > 1:\n            for i in range(1, len(speeds)):\n                accel = abs(speeds[i] - speeds[i-1])\n                accelerations.append(accel)\n        \n        avg_acceleration = np.mean(accelerations) if accelerations else 0\n        max_acceleration = max(accelerations) if accelerations else 0\n        \n        # Brightness statistics (for distinguishing planes from satellites)\n        avg_brightness = np.mean(mean_brightness_values) if mean_brightness_values else 0\n        brightness_std = np.std(mean_brightness_values) if len(mean_brightness_values) > 1 else 0\n        max_brightness = max(max_brightness_values) if max_brightness_values else 0\n        \n        # Brightness consistency (low variance = constant like satellite, high variance = blinking like plane)\n        brightness_consistency = 1.0 / (1.0 + brightness_std / max(avg_brightness, 1))\n        \n        # Enhanced blinking pattern detection for planes\n        brightness_variation_coeff = (brightness_std / max(avg_brightness, 1)) if avg_brightness > 0 else 0\n        \n        # Detect periodic blinking (on/off pattern typical of plane lights)\n        blinking_pattern_score = 0.0\n        if len(mean_brightness_values) >= 3 and avg_brightness > 1.0:  # Safeguard against zero brightness\n            # Calculate brightness differences between consecutive frames\n            brightness_diffs = [abs(mean_brightness_values[i] - mean_brightness_values[i-1]) \n                               for i in range(1, len(mean_brightness_values))]\n            \n            # High differences indicate blinking\n            avg_diff = np.mean(brightness_diffs) if brightness_diffs else 0\n            if avg_diff > avg_brightness * 0.15:  # 15% change threshold\n                blinking_pattern_score = min(avg_diff / avg_brightness, 1.0)\n        \n        # Combine blinking indicators\n        blinking_score = max(brightness_variation_coeff, blinking_pattern_score)\n        \n        # Movement pattern classification hints  \n        # Use duration as PRIMARY discriminator between satellites and planes\n        \n        # Satellite: consistent, linear, HIGH score for 3-15s duration (typical satellite pass)\n        # Score peaks at 3-15s, gentle decline for longer durations\n        if duration < 3:\n            duration_satellite_factor = 0.6\n        elif duration < 18:\n            duration_satellite_factor = 1.2  # Boost typical satellite range (extended to 18s)\n        elif duration < 25:\n            duration_satellite_factor = 0.85  # Gentle decline\n        else:\n            duration_satellite_factor = 0.65  # Penalize very long durations\n        \n        satellite_consistency = size_consistency * brightness_consistency\n        satellite_score = speed_consistency * linearity * satellite_consistency * duration_satellite_factor\n        \n        # Meteor: high speed, very linear, brief duration, often bright/flashing\n        # Meteors are fast-moving (high speed), very straight (high linearity), \n        # short duration (<3s), and often have brightness spikes\n        speed_factor = min(avg_speed / 30.0, 3.0)  # Cap at 3x for very fast objects\n        duration_factor = (1.0 / max(duration, 0.1)) if duration < 3 else 0.3  # Heavily penalize >3s\n        brightness_factor = 1.0 + min(max_brightness / 255.0, 0.5)  # Bonus for bright objects (up to 1.5x)\n        \n        meteor_score = speed_factor * linearity * duration_factor * brightness_factor\n        \n        # Plane: consistent movement, HIGH score for 15+ sec duration (planes stay visible longer)\n        # Score increases with duration, peaks at 15+ seconds\n        if duration < 10:\n            duration_plane_factor = 0.4  # Low score for short durations\n        elif duration < 15:\n            duration_plane_factor = 0.8\n        else:\n            duration_plane_factor = 1.1  # Boost long durations\n        \n        # Enhanced plane detection with blinking pattern recognition\n        # Planes often have blinking lights (especially red ones)\n        # Penalize planes that have satellite-like consistency (unless they're blinking)\n        satellite_like_consistency = size_consistency * brightness_consistency\n        if satellite_like_consistency > 0.6 and blinking_score < 0.15:\n            consistency_penalty = 0.65  # Penalize satellite-like characteristics\n        else:\n            consistency_penalty = 1.0\n        \n        # Strong bonus for blinking pattern (planes typically blink)\n        blinking_bonus = 1.0 + min(blinking_score * 0.8, 0.8)  # Up to 1.8x bonus for strong blinking\n        plane_score = speed_consistency * linearity * duration_plane_factor * blinking_bonus * consistency_penalty\n        \n        # Anomaly indicators: erratic movement, inconsistent speed/size\n        anomaly_indicators = (\n            direction_changes / max(len(positions), 1) +  # Direction change rate\n            (1.0 - speed_consistency) +  # Speed inconsistency\n            (1.0 - size_consistency) +   # Size inconsistency\n            max_acceleration / max(avg_speed, 1)  # High acceleration relative to speed\n        )\n        \n        return {\n            'clip_id': clip_id,\n            'duration': duration,\n            'avg_speed': avg_speed,\n            'max_speed': max_speed,\n            'min_speed': min_speed,\n            'speed_consistency': speed_consistency,\n            'total_distance': total_distance,\n            'linearity': linearity,\n            'direction_changes': direction_changes,\n            'avg_area': avg_area,\n            'max_area': max_area,\n            'min_area': min_area,\n            'size_consistency': size_consistency,\n            'avg_aspect_ratio': avg_aspect_ratio,\n            'aspect_ratio_std': aspect_ratio_std,\n            'avg_acceleration': avg_acceleration,\n            'max_acceleration': max_acceleration,\n            'avg_brightness': avg_brightness,\n            'brightness_std': brightness_std,\n            'max_brightness': max_brightness,\n            'brightness_consistency': brightness_consistency,\n            'brightness_variation_coeff': brightness_variation_coeff,\n            'blinking_score': blinking_score,  # Enhanced blinking detection\n            'satellite_score': satellite_score,\n            'meteor_score': meteor_score,\n            'plane_score': plane_score,\n            'anomaly_indicators': anomaly_indicators,\n            'detection_count': len(detections)\n        }\n    \n    def _create_minimal_features(self, clip_id, detections, fps=30):\n        \"\"\"Create minimal feature set for clips with insufficient data\"\"\"\n        if not detections:\n            return {\n                'clip_id': clip_id,\n                'duration': 0,\n                'avg_speed': 0,\n                'max_speed': 0,\n                'min_speed': 0,\n                'speed_consistency': 0,\n                'total_distance': 0,\n                'linearity': 0,\n                'direction_changes': 0,\n                'avg_area': 0,\n                'max_area': 0,\n                'min_area': 0,\n                'size_consistency': 0,\n                'avg_aspect_ratio': 1,\n                'aspect_ratio_std': 0,\n                'avg_acceleration': 0,\n                'max_acceleration': 0,\n                'avg_brightness': 0,\n                'brightness_std': 0,\n                'max_brightness': 0,\n                'brightness_consistency': 0,\n                'brightness_variation_coeff': 0,\n                'blinking_score': 0,  # No blinking for insufficient data\n                'satellite_score': 0,\n                'meteor_score': 0,\n                'plane_score': 0,\n                'anomaly_indicators': 1,  # High anomaly score for insufficient data\n                'detection_count': len(detections)\n            }\n        \n        # Single detection case\n        detection = detections[0]\n        return {\n            'clip_id': clip_id,\n            'duration': 1.0 / float(fps),  # Single frame duration using actual FPS\n            'avg_speed': 0,\n            'max_speed': 0,\n            'min_speed': 0,\n            'speed_consistency': 0,\n            'total_distance': 0,\n            'linearity': 0,\n            'direction_changes': 0,\n            'avg_area': detection['area'],\n            'max_area': detection['area'],\n            'min_area': detection['area'],\n            'size_consistency': 1,\n            'avg_aspect_ratio': detection['aspect_ratio'],\n            'aspect_ratio_std': 0,\n            'avg_acceleration': 0,\n            'max_acceleration': 0,\n            'avg_brightness': detection.get('mean_brightness', 0),\n            'brightness_std': 0,\n            'max_brightness': detection.get('max_brightness', 0),\n            'brightness_consistency': 1,\n            'brightness_variation_coeff': 0,\n            'blinking_score': 0,  # No blinking for single detection\n            'satellite_score': 0,\n            'meteor_score': 0,\n            'plane_score': 0,\n            'anomaly_indicators': 0.5,  # Medium anomaly score for single detection\n            'detection_count': 1\n        }\n","size_bytes":15245},"README.md":{"content":"# üåå SkySeer AI - Sky Object Detection System\n\n**Advanced computer vision and machine learning for automatic satellite and meteor detection in night sky videos.**\n\n---\n\n## üìñ What is SkySeer?\n\nSkySeer AI is an intelligent video analysis tool that automatically detects and classifies moving objects in night sky footage. Perfect for amateur astronomers and space enthusiasts, it transforms hours of raw video into precise, actionable data‚Äîidentifying satellites and meteors while filtering out noise and false positives.\n\n**What you get:**\n- ‚úÖ Sped-up annotated videos (10x speed) with color-coded detections\n- ‚úÖ Detailed CSV reports with speeds, trajectories, and confidence scores\n- ‚úÖ Category-specific downloads (separate Satellite and Meteor packages)\n- ‚úÖ Smart filtering: typically <10 high-quality detections per video\n\n---\n\n## üöÄ Quick Start\n\n### System Requirements\n- **Operating System:** Windows 10/11 (64-bit)\n- **RAM:** 4GB minimum, 8GB recommended\n- **Storage:** 2GB free space for processing\n- **Display:** 1920x1080 or higher recommended\n\n### How to Run\n1. **Double-click** `SkySeer.exe` to launch the application\n2. Your default web browser will open automatically to the SkySeer interface\n3. If the browser doesn't open, manually navigate to: `http://localhost:5000`\n\n### First-Time Setup\n- No installation required - it's a standalone executable\n- No internet connection needed after initial launch\n- All processing happens locally on your computer\n\n---\n\n## üìπ How to Use\n\n### Step 1: Upload Your Video\n1. Click **\"Browse files\"** or drag-and-drop your night sky video\n2. Supported formats: **MP4, AVI, MOV, MKV**\n3. Best results: stable, tripod-mounted footage\n\n### Step 2: Configure Settings (Optional)\nThe system auto-recommends optimal settings, but you can adjust:\n\n**Motion Detection Sensitivity (1-10)**\n- Low (1-3): Fewer detections, reduces noise\n- Medium (4-6): Balanced (recommended for most videos)\n- High (7-10): More sensitive, may increase false positives\n\n**Minimum Clip Duration (0.5-5s)**\n- Default: 1.5s\n- Lower for brief meteors, higher to filter noise\n\n**Maximum Clip Duration (5-30s)**\n- Default: 15s\n- Increase for slow-moving satellites\n\n**Frame Skip Rate (1-6)**\n- Higher = faster processing (recommended for long videos)\n- Default auto-adjusts based on video duration\n\n### Step 3: Process Video\n1. Click **\"üöÄ Start Detection\"**\n2. Watch real-time progress updates\n3. Processing time: ~1-5 minutes per 10-minute video (varies by settings)\n\n### Step 4: Review Results\n- **Main Video:** Sped-up 10x with all detections marked\n- **Results Table:** Detailed metrics for each object\n- **Downloads:** Category-specific packages (see below)\n\n---\n\n## üé® Color Coding\n\n**RED Boxes** üî¥ = **Satellites**\n- Slow, steady movement across the sky\n- Typical duration: 5-20 seconds\n- Examples: ISS, Starlink, orbital debris\n\n**YELLOW Boxes** üü° = **Meteors**\n- Fast, brief streaks\n- Typical duration: <4 seconds\n- Examples: shooting stars, meteor showers\n\n---\n\n## üì¶ Understanding Your Downloads\n\n### 1. CSV Report (`analysis_report.csv`)\nContains detailed metrics for every detection:\n- **clip_id:** Unique identifier (matches video labels)\n- **classification:** Satellite, Meteor, or Junk\n- **confidence:** Classification confidence (0-1)\n- **avg_speed:** Average speed in pixels/frame\n- **speed_consistency:** Movement stability (0-1)\n- **duration:** Detection duration in seconds\n- **linearity:** Path straightness (0-1, higher = straighter)\n- **direction_changes:** Number of trajectory shifts\n- **avg_brightness:** Relative brightness\n- **satellite_score / meteor_score:** AI scoring metrics\n\n### 2. Category Downloads (Satellite / Meteor)\nEach category ZIP includes:\n- **`{category}_detections.mp4`:** Sped-up video showing ONLY that classification\n- **`analysis_report.csv`:** Filtered data for that category only\n- **`SUMMARY.txt`:** Quick overview with detection counts\n\n**Note:** Only **Satellite** and **Meteor** downloads are available. Junk/noise detections are automatically filtered out.\n\n### 3. Video Outputs\n**Main Output Video:**\n- 10x speed (10-minute input ‚Üí 1-minute output)\n- All detections visible with color-coded boxes\n- Labels show: \"ID:{number} {Classification}\"\n\n**Category Videos:**\n- Same 10x speed format\n- Shows ONLY that specific classification\n- Clean filtering: Satellite downloads = only RED boxes, Meteor downloads = only YELLOW boxes\n\n---\n\n## üéØ Best Practices\n\n### For Optimal Results:\n‚úÖ **Use tripod-mounted cameras** - reduces false positives from camera shake\n‚úÖ **Record in dark conditions** - best for satellites and meteors\n‚úÖ **Avoid cloudy footage** - clouds create motion artifacts\n‚úÖ **Use higher sensitivity (5-6)** for clean, dark night sky videos\n‚úÖ **Use lower sensitivity (2-4)** for noisy or brighter videos\n\n### Common Issues:\n‚ùå **Too many false detections?** ‚Üí Lower sensitivity or increase min duration\n‚ùå **Missing obvious objects?** ‚Üí Increase sensitivity or lower min duration\n‚ùå **Processing timeout?** ‚Üí Increase frame skip rate (especially for long videos)\n\n---\n\n## üîß Troubleshooting\n\n### Application Won't Start\n- **Check antivirus:** Some security software may block the .exe\n- **Run as Administrator:** Right-click ‚Üí \"Run as administrator\"\n- **Port conflict:** Ensure port 5000 isn't used by another application\n\n### Browser Doesn't Open\n- Manually open your browser and go to: `http://localhost:5000`\n- Try a different browser (Chrome, Firefox, Edge)\n\n### Processing Errors\n- **Video too large:** Try videos under 2GB for best performance\n- **Unsupported format:** Convert to MP4 using free tools like HandBrake\n- **Out of memory:** Close other applications, restart SkySeer\n\n### Poor Detection Quality\n- **Too sensitive:** Reduce sensitivity setting to 3-4\n- **Not sensitive enough:** Increase sensitivity to 6-7\n- **Video too shaky:** Use tripod footage or stabilize video first\n- **Too much noise:** Use longer minimum duration (2-3s)\n\n---\n\n## üìä Technical Details\n\n### How It Works\nSkySeer uses a **4-stage intelligent pipeline:**\n\n1. **Motion Detection:** Advanced background subtraction (MOG2 algorithm)\n2. **Feature Extraction:** Calculates numerical \"flight signatures\" (speed, trajectory, brightness patterns)\n3. **ML Classification:** Unsupervised K-Means clustering categorizes objects\n4. **Output Generation:** Creates annotated videos and detailed reports\n\n### Technology Stack\n- **Computer Vision:** OpenCV with MOG2 background subtraction\n- **Machine Learning:** scikit-learn K-Means clustering (3 clusters)\n- **Frontend:** Streamlit web interface\n- **Data Processing:** Pandas, NumPy for numerical operations\n- **Visualization:** Plotly for interactive charts\n\n### Privacy & Security\n- ‚úÖ **100% offline processing** - no internet required after launch\n- ‚úÖ **No data uploads** - all videos stay on your computer\n- ‚úÖ **No accounts needed** - completely standalone\n\n---\n\n## üìÑ License & Credits\n\n**SkySeer AI** - Developed as a portfolio project showcasing computer vision and machine learning expertise.\n\n**Created by:** A passionate CS student combining AI and space exploration\n\n**Use Cases:**\n- Amateur astronomy and satellite tracking\n- Meteor shower observation and documentation\n- Time-lapse photography event extraction\n- Citizen science data collection\n\n---\n\n## üÜò Support & Contact\n\nFor questions, bug reports, or feature suggestions:\n- Review this README's troubleshooting section\n- Check your video quality and settings\n- Ensure your system meets minimum requirements\n\n---\n\n## üåü Tips for Great Results\n\n1. **Start with recommended settings** - the auto-configuration is optimized for most videos\n2. **Test different sensitivities** - every camera and location is different\n3. **Use the CSV data** - detailed metrics help understand detection quality\n4. **Compare categories** - review both Satellite and Meteor downloads to see differences\n5. **Save your settings** - note what works best for your specific camera setup\n\n---\n\n**SkySeer: Transforming night sky footage into structured astronomical data.** üååüõ∞Ô∏è‚òÑÔ∏è\n\n*Happy stargazing and detection!*\n","size_bytes":8148},"trajectory_analysis.py":{"content":"import numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndef predict_trajectory(detections_df, object_id, method='linear'):\n    \"\"\"\n    Predict object trajectory and compare with actual path\n    \n    Args:\n        detections_df: DataFrame with detection data\n        object_id: Object ID to analyze\n        method: 'linear' or 'polynomial' prediction\n        \n    Returns:\n        dict with prediction results and metrics\n    \"\"\"\n    obj_data = detections_df[detections_df['object_id'] == object_id].copy()\n    \n    if len(obj_data) < 3:\n        return None\n    \n    obj_data = obj_data.sort_values('frame_number')\n    \n    frames = obj_data['frame_number'].values.reshape(-1, 1)\n    x_positions = obj_data['center_x'].values\n    y_positions = obj_data['center_y'].values\n    \n    if method == 'polynomial':\n        poly = PolynomialFeatures(degree=2)\n        frames_poly = poly.fit_transform(frames)\n        \n        model_x = LinearRegression()\n        model_y = LinearRegression()\n        model_x.fit(frames_poly, x_positions)\n        model_y.fit(frames_poly, y_positions)\n        \n        x_pred = model_x.predict(frames_poly)\n        y_pred = model_y.predict(frames_poly)\n    else:\n        model_x = LinearRegression()\n        model_y = LinearRegression()\n        model_x.fit(frames, x_positions)\n        model_y.fit(frames, y_positions)\n        \n        x_pred = model_x.predict(frames)\n        y_pred = model_y.predict(frames)\n    \n    errors = np.sqrt((x_positions - x_pred)**2 + (y_positions - y_pred)**2)\n    \n    rmse_x = np.sqrt(mean_squared_error(x_positions, x_pred))\n    rmse_y = np.sqrt(mean_squared_error(y_positions, y_pred))\n    rmse_total = np.sqrt(rmse_x**2 + rmse_y**2)\n    \n    r2_x = r2_score(x_positions, x_pred)\n    r2_y = r2_score(y_positions, y_pred)\n    \n    return {\n        'object_id': object_id,\n        'frames': frames.flatten(),\n        'actual_x': x_positions,\n        'actual_y': y_positions,\n        'predicted_x': x_pred,\n        'predicted_y': y_pred,\n        'errors': errors,\n        'mean_error': np.mean(errors),\n        'max_error': np.max(errors),\n        'rmse_x': rmse_x,\n        'rmse_y': rmse_y,\n        'rmse_total': rmse_total,\n        'r2_x': r2_x,\n        'r2_y': r2_y,\n        'method': method\n    }\n\ndef analyze_all_trajectories(detections_df, method='linear'):\n    \"\"\"\n    Analyze trajectories for all objects\n    \n    Args:\n        detections_df: DataFrame with all detection data\n        method: Prediction method ('linear' or 'polynomial')\n        \n    Returns:\n        list of prediction results\n    \"\"\"\n    results = []\n    unique_objects = detections_df['object_id'].unique()\n    \n    for obj_id in unique_objects:\n        prediction = predict_trajectory(detections_df, obj_id, method)\n        if prediction:\n            results.append(prediction)\n    \n    return results\n\ndef create_trajectory_comparison_plot(prediction_result, classification=None):\n    \"\"\"\n    Create interactive plot comparing predicted vs actual trajectory\n    \n    Args:\n        prediction_result: Result from predict_trajectory()\n        classification: Object classification for title\n        \n    Returns:\n        Plotly figure\n    \"\"\"\n    fig = go.Figure()\n    \n    fig.add_trace(go.Scatter(\n        x=prediction_result['actual_x'],\n        y=prediction_result['actual_y'],\n        mode='markers+lines',\n        name='Actual Path',\n        marker=dict(size=8, color='#00ff00'),\n        line=dict(color='#00ff00', width=2)\n    ))\n    \n    fig.add_trace(go.Scatter(\n        x=prediction_result['predicted_x'],\n        y=prediction_result['predicted_y'],\n        mode='lines',\n        name='Predicted Path',\n        line=dict(color='#ff6b6b', width=2, dash='dash')\n    ))\n    \n    errors = prediction_result['errors']\n    for i in range(len(prediction_result['actual_x'])):\n        fig.add_trace(go.Scatter(\n            x=[prediction_result['actual_x'][i], prediction_result['predicted_x'][i]],\n            y=[prediction_result['actual_y'][i], prediction_result['predicted_y'][i]],\n            mode='lines',\n            line=dict(color='rgba(255, 255, 0, 0.3)', width=1),\n            showlegend=False,\n            hovertemplate=f'Error: {errors[i]:.2f} px<extra></extra>'\n        ))\n    \n    title = f\"Trajectory Prediction - Object #{prediction_result['object_id']}\"\n    if classification:\n        title += f\" ({classification})\"\n    \n    fig.update_layout(\n        title=title,\n        xaxis_title=\"X Position (pixels)\",\n        yaxis_title=\"Y Position (pixels)\",\n        hovermode='closest',\n        plot_bgcolor='#0e1117',\n        paper_bgcolor='#0e1117',\n        font=dict(color='white'),\n        height=500,\n        showlegend=True,\n        yaxis=dict(scaleanchor=\"x\", scaleratio=1, autorange=\"reversed\")\n    )\n    \n    return fig\n\ndef create_trajectory_error_plot(trajectory_results):\n    \"\"\"\n    Create plot showing prediction errors across all objects\n    \n    Args:\n        trajectory_results: List of prediction results\n        \n    Returns:\n        Plotly figure\n    \"\"\"\n    if not trajectory_results:\n        return None\n    \n    object_ids = [r['object_id'] for r in trajectory_results]\n    mean_errors = [r['mean_error'] for r in trajectory_results]\n    max_errors = [r['max_error'] for r in trajectory_results]\n    \n    fig = go.Figure()\n    \n    fig.add_trace(go.Bar(\n        x=object_ids,\n        y=mean_errors,\n        name='Mean Error',\n        marker_color='#4ecdc4'\n    ))\n    \n    fig.add_trace(go.Bar(\n        x=object_ids,\n        y=max_errors,\n        name='Max Error',\n        marker_color='#ff6b6b'\n    ))\n    \n    fig.update_layout(\n        title=\"Trajectory Prediction Errors by Object\",\n        xaxis_title=\"Object ID\",\n        yaxis_title=\"Error (pixels)\",\n        barmode='group',\n        plot_bgcolor='#0e1117',\n        paper_bgcolor='#0e1117',\n        font=dict(color='white'),\n        height=400\n    )\n    \n    return fig\n\ndef get_trajectory_summary_stats(trajectory_results):\n    \"\"\"\n    Calculate summary statistics for all trajectory predictions\n    \n    Args:\n        trajectory_results: List of prediction results\n        \n    Returns:\n        dict with summary statistics\n    \"\"\"\n    if not trajectory_results:\n        return None\n    \n    all_mean_errors = [r['mean_error'] for r in trajectory_results]\n    all_r2_x = [r['r2_x'] for r in trajectory_results]\n    all_r2_y = [r['r2_y'] for r in trajectory_results]\n    \n    return {\n        'total_objects': len(trajectory_results),\n        'avg_mean_error': np.mean(all_mean_errors),\n        'median_mean_error': np.median(all_mean_errors),\n        'avg_r2_x': np.mean(all_r2_x),\n        'avg_r2_y': np.mean(all_r2_y),\n        'best_r2_x': np.max(all_r2_x),\n        'worst_r2_x': np.min(all_r2_x),\n        'highly_predictable': sum(1 for r in trajectory_results if r['r2_x'] > 0.95 and r['r2_y'] > 0.95)\n    }\n","size_bytes":7017}},"version":2}